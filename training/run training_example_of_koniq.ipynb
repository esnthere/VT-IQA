{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:-1 [(0%)]\t Loss: 2.7974  Pearson:-0.0092 Spearman:-0.0371\n",
      "Test : Loss:2.8113 \n",
      "pearson： 0.0387774822894656 ALL Pearson: 0.05150586135423535\n",
      "spearman： 0.022900735481372902 ALL Spearman: 0.030905031639616116\n",
      "0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 [(0%)]\t Loss: 2.7973  Pearson:-0.0166 Spearman:0.0016\n",
      "Train ALL Pearson: 0.011998093839134027\n",
      "Train  ALL Spearman: 0.0073025029490439795\n",
      "20.53764247894287\n",
      "Test Epoch:0 [(0%)]\t Loss: 0.8558  Pearson:-0.0994 Spearman:-0.1254\n",
      "Test : Loss:0.7866 \n",
      "pearson： -0.020056796876154064 ALL Pearson: 0.020695820447926914\n",
      "spearman： -0.029909485282825162 ALL Spearman: 0.004953398639284304\n",
      "time: 25.210142850875854\n",
      "0.01\n",
      "Train Epoch:1 [(0%)]\t Loss: 0.6513  Pearson:0.0550 Spearman:0.0363\n",
      "Train ALL Pearson: 0.2490218929763945\n",
      "Train  ALL Spearman: 0.23222820669967653\n",
      "20.430988311767578\n",
      "Test Epoch:1 [(0%)]\t Loss: 0.5909  Pearson:0.4383 Spearman:0.4228\n",
      "Test : Loss:0.5983 \n",
      "pearson： 0.4910518493488148 ALL Pearson: 0.5203185030877705\n",
      "spearman： 0.46037475698549846 ALL Spearman: 0.47921073236724476\n",
      "time: 25.169469118118286\n",
      "0.01\n",
      "Train Epoch:2 [(0%)]\t Loss: 0.4692  Pearson:0.4712 Spearman:0.4521\n",
      "Train ALL Pearson: 0.4522405935351652\n",
      "Train  ALL Spearman: 0.422449409092297\n",
      "20.406585931777954\n",
      "Test Epoch:2 [(0%)]\t Loss: 0.6084  Pearson:0.6096 Spearman:0.5819\n",
      "Test : Loss:0.5839 \n",
      "pearson： 0.6241700203949792 ALL Pearson: 0.6385922473956546\n",
      "spearman： 0.581841986650184 ALL Spearman: 0.5997643027899848\n",
      "time: 24.923226833343506\n",
      "0.01\n",
      "Train Epoch:3 [(0%)]\t Loss: 0.4170  Pearson:0.5325 Spearman:0.5210\n",
      "Train ALL Pearson: 0.5342719629578826\n",
      "Train  ALL Spearman: 0.5076689352462859\n",
      "20.53678321838379\n",
      "Test Epoch:3 [(0%)]\t Loss: 0.5969  Pearson:0.6366 Spearman:0.6066\n",
      "Test : Loss:0.5471 \n",
      "pearson： 0.6648691477999028 ALL Pearson: 0.6803035429902586\n",
      "spearman： 0.6249198159390633 ALL Spearman: 0.6419622262459587\n",
      "time: 25.148303985595703\n",
      "0.01\n",
      "Train Epoch:4 [(0%)]\t Loss: 0.3745  Pearson:0.5687 Spearman:0.5505\n",
      "Train ALL Pearson: 0.572349412995934\n",
      "Train  ALL Spearman: 0.5444423387865838\n",
      "20.563958883285522\n",
      "Test Epoch:4 [(0%)]\t Loss: 0.5115  Pearson:0.7196 Spearman:0.6637\n",
      "Test : Loss:0.5266 \n",
      "pearson： 0.7086655585937328 ALL Pearson: 0.705160153539028\n",
      "spearman： 0.6655277764940493 ALL Spearman: 0.6681949261458237\n",
      "time: 25.28044557571411\n",
      "0.01\n",
      "Train Epoch:5 [(0%)]\t Loss: 0.3871  Pearson:0.5785 Spearman:0.5430\n",
      "Train ALL Pearson: 0.5863075442163291\n",
      "Train  ALL Spearman: 0.5513303207086704\n",
      "20.481366872787476\n",
      "Test Epoch:5 [(0%)]\t Loss: 0.5174  Pearson:0.6929 Spearman:0.6857\n",
      "Test : Loss:0.5237 \n",
      "pearson： 0.7104898477902927 ALL Pearson: 0.72168534844151\n",
      "spearman： 0.6840704991170267 ALL Spearman: 0.686971970683437\n",
      "time: 25.127876043319702\n",
      "0.01\n",
      "Train Epoch:6 [(0%)]\t Loss: 0.3612  Pearson:0.6221 Spearman:0.5649\n",
      "Train ALL Pearson: 0.6185172738473446\n",
      "Train  ALL Spearman: 0.5864900983624662\n",
      "20.760897159576416\n",
      "Test Epoch:6 [(0%)]\t Loss: 0.5207  Pearson:0.6883 Spearman:0.6376\n",
      "Test : Loss:0.5161 \n",
      "pearson： 0.7290294718932603 ALL Pearson: 0.7324587864790627\n",
      "spearman： 0.6910673230798833 ALL Spearman: 0.6980018188901588\n",
      "time: 25.36896252632141\n",
      "0.01\n",
      "Train Epoch:7 [(0%)]\t Loss: 0.3654  Pearson:0.6033 Spearman:0.5560\n",
      "Train ALL Pearson: 0.6279156180122331\n",
      "Train  ALL Spearman: 0.5920353643888139\n",
      "20.60945773124695\n",
      "Test Epoch:7 [(0%)]\t Loss: 0.5489  Pearson:0.7136 Spearman:0.6603\n",
      "Test : Loss:0.5340 \n",
      "pearson： 0.7256037000378064 ALL Pearson: 0.7365844969718434\n",
      "spearman： 0.6835642445273017 ALL Spearman: 0.7021189934321185\n",
      "time: 25.187988996505737\n",
      "0.01\n",
      "Train Epoch:8 [(0%)]\t Loss: 0.3567  Pearson:0.5952 Spearman:0.5203\n",
      "Train ALL Pearson: 0.6296561778017193\n",
      "Train  ALL Spearman: 0.5970886229773145\n",
      "20.437989950180054\n",
      "Test Epoch:8 [(0%)]\t Loss: 0.5202  Pearson:0.7582 Spearman:0.6812\n",
      "Test : Loss:0.5135 \n",
      "pearson： 0.7491881658819495 ALL Pearson: 0.7429556655519023\n",
      "spearman： 0.712590018837794 ALL Spearman: 0.7090662672803126\n",
      "time: 25.21345829963684\n",
      "0.01\n",
      "Train Epoch:9 [(0%)]\t Loss: 0.3269  Pearson:0.7218 Spearman:0.6874\n",
      "Train ALL Pearson: 0.6388835267221101\n",
      "Train  ALL Spearman: 0.6069446572288804\n",
      "20.891607761383057\n",
      "Test Epoch:9 [(0%)]\t Loss: 0.5094  Pearson:0.7890 Spearman:0.7847\n",
      "Test : Loss:0.5190 \n",
      "pearson： 0.7559243601318535 ALL Pearson: 0.7469984548753273\n",
      "spearman： 0.728543438450118 ALL Spearman: 0.713882454295992\n",
      "time: 25.562360286712646\n",
      "0.01\n",
      "Train Epoch:10 [(0%)]\t Loss: 0.3554  Pearson:0.5250 Spearman:0.5171\n",
      "Train ALL Pearson: 0.6526696343622577\n",
      "Train  ALL Spearman: 0.6196714301787859\n",
      "20.788649320602417\n",
      "Test Epoch:10 [(0%)]\t Loss: 0.5393  Pearson:0.6611 Spearman:0.6509\n",
      "Test : Loss:0.5295 \n",
      "pearson： 0.7335912502731823 ALL Pearson: 0.7518378631108849\n",
      "spearman： 0.7047264810275679 ALL Spearman: 0.7211841240063198\n",
      "time: 25.601106643676758\n",
      "0.001\n",
      "Train Epoch:11 [(0%)]\t Loss: 0.3278  Pearson:0.6752 Spearman:0.6271\n",
      "Train ALL Pearson: 0.659559386522799\n",
      "Train  ALL Spearman: 0.6304881625802891\n",
      "32.745705127716064\n",
      "Test Epoch:11 [(0%)]\t Loss: 0.5257  Pearson:0.7993 Spearman:0.7938\n",
      "Test : Loss:0.4991 \n",
      "pearson： 0.7773229103702192 ALL Pearson: 0.7615288981130923\n",
      "spearman： 0.751733238899119 ALL Spearman: 0.7318480831014332\n",
      "time: 37.42720341682434\n",
      "0.001\n",
      "Train Epoch:12 [(0%)]\t Loss: 0.3136  Pearson:0.6971 Spearman:0.6455\n",
      "Train ALL Pearson: 0.6761380375024765\n",
      "Train  ALL Spearman: 0.6481510389649348\n",
      "31.70243740081787\n",
      "Test Epoch:12 [(0%)]\t Loss: 0.4965  Pearson:0.7593 Spearman:0.7329\n",
      "Test : Loss:0.4948 \n",
      "pearson： 0.7721438591859539 ALL Pearson: 0.7691720217168811\n",
      "spearman： 0.7480234866658965 ALL Spearman: 0.7403134733408753\n",
      "time: 36.42792272567749\n",
      "0.001\n",
      "Train Epoch:13 [(0%)]\t Loss: 0.3155  Pearson:0.6431 Spearman:0.6041\n",
      "Train ALL Pearson: 0.682414182062815\n",
      "Train  ALL Spearman: 0.6548503506651832\n",
      "33.30416536331177\n",
      "Test Epoch:13 [(0%)]\t Loss: 0.4815  Pearson:0.7495 Spearman:0.7142\n",
      "Test : Loss:0.5033 \n",
      "pearson： 0.7709525145570434 ALL Pearson: 0.7756752623564297\n",
      "spearman： 0.7438945676942854 ALL Spearman: 0.7478474671422733\n",
      "time: 38.76589775085449\n",
      "0.001\n",
      "Train Epoch:14 [(0%)]\t Loss: 0.3158  Pearson:0.6721 Spearman:0.6480\n",
      "Train ALL Pearson: 0.6956461186044228\n",
      "Train  ALL Spearman: 0.6733889404689781\n",
      "35.196035861968994\n",
      "Test Epoch:14 [(0%)]\t Loss: 0.5300  Pearson:0.7744 Spearman:0.7681\n",
      "Test : Loss:0.4859 \n",
      "pearson： 0.7821641043985715 ALL Pearson: 0.7830121711065524\n",
      "spearman： 0.7590149068800227 ALL Spearman: 0.7559029877404673\n",
      "time: 40.36034536361694\n",
      "0.001\n",
      "Train Epoch:15 [(0%)]\t Loss: 0.3154  Pearson:0.6768 Spearman:0.6602\n",
      "Train ALL Pearson: 0.7013070272237856\n",
      "Train  ALL Spearman: 0.6818680182858384\n",
      "35.01057147979736\n",
      "Test Epoch:15 [(0%)]\t Loss: 0.4726  Pearson:0.8270 Spearman:0.8331\n",
      "Test : Loss:0.4918 \n",
      "pearson： 0.7937011749747884 ALL Pearson: 0.7882765160957368\n",
      "spearman： 0.772734580577599 ALL Spearman: 0.7618800091890711\n",
      "time: 40.397897481918335\n",
      "0.001\n",
      "Train Epoch:16 [(0%)]\t Loss: 0.3247  Pearson:0.7395 Spearman:0.7398\n",
      "Train ALL Pearson: 0.7025286551286497\n",
      "Train  ALL Spearman: 0.6758430897579899\n",
      "35.52646541595459\n",
      "Test Epoch:16 [(0%)]\t Loss: 0.4742  Pearson:0.7783 Spearman:0.7733\n",
      "Test : Loss:0.4831 \n",
      "pearson： 0.7896298151763301 ALL Pearson: 0.7931869184705334\n",
      "spearman： 0.7684766444238559 ALL Spearman: 0.7666587148471592\n",
      "time: 40.648390769958496\n",
      "0.001\n",
      "Train Epoch:17 [(0%)]\t Loss: 0.3082  Pearson:0.7224 Spearman:0.6792\n",
      "Train ALL Pearson: 0.715092530002914\n",
      "Train  ALL Spearman: 0.6893512705932903\n",
      "35.30085325241089\n",
      "Test Epoch:17 [(0%)]\t Loss: 0.5100  Pearson:0.7966 Spearman:0.7625\n",
      "Test : Loss:0.4774 \n",
      "pearson： 0.7975615340045329 ALL Pearson: 0.79778921050842\n",
      "spearman： 0.7696671913330383 ALL Spearman: 0.7719668970926269\n",
      "time: 40.77584767341614\n",
      "0.001\n",
      "Train Epoch:18 [(0%)]\t Loss: 0.3386  Pearson:0.7382 Spearman:0.7141\n",
      "Train ALL Pearson: 0.7202077593916008\n",
      "Train  ALL Spearman: 0.6954411639438055\n",
      "35.11317992210388\n",
      "Test Epoch:18 [(0%)]\t Loss: 0.4635  Pearson:0.8207 Spearman:0.7955\n",
      "Test : Loss:0.4755 \n",
      "pearson： 0.8076468820601168 ALL Pearson: 0.8024010256236151\n",
      "spearman： 0.7834593225629735 ALL Spearman: 0.7771867918450076\n",
      "time: 40.401347160339355\n",
      "0.001\n",
      "Train Epoch:19 [(0%)]\t Loss: 0.3247  Pearson:0.6737 Spearman:0.6743\n",
      "Train ALL Pearson: 0.7307578327060111\n",
      "Train  ALL Spearman: 0.7086500432303103\n",
      "35.37144589424133\n",
      "Test Epoch:19 [(0%)]\t Loss: 0.4682  Pearson:0.7700 Spearman:0.7685\n",
      "Test : Loss:0.4723 \n",
      "pearson： 0.804591880232925 ALL Pearson: 0.8067898405558983\n",
      "spearman： 0.786516413426067 ALL Spearman: 0.7820058823742881\n",
      "time: 40.62735891342163\n",
      "0.001\n",
      "Train Epoch:20 [(0%)]\t Loss: 0.3032  Pearson:0.7276 Spearman:0.6837\n",
      "Train ALL Pearson: 0.7321227306525059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  ALL Spearman: 0.7015178784596643\n",
      "35.20733165740967\n",
      "Test Epoch:20 [(0%)]\t Loss: 0.4682  Pearson:0.8189 Spearman:0.8172\n",
      "Test : Loss:0.4695 \n",
      "pearson： 0.8134221468000394 ALL Pearson: 0.8108436456239486\n",
      "spearman： 0.7879207162628602 ALL Spearman: 0.7865559193466453\n",
      "time: 40.53370642662048\n",
      "0.005\n",
      "Train Epoch:21 [(0%)]\t Loss: 0.3029  Pearson:0.7305 Spearman:0.7146\n",
      "Train ALL Pearson: 0.7445124216930096\n",
      "Train  ALL Spearman: 0.7193495065918947\n",
      "35.30280113220215\n",
      "Test Epoch:21 [(0%)]\t Loss: 0.4609  Pearson:0.8303 Spearman:0.8088\n",
      "Test : Loss:0.4530 \n",
      "pearson： 0.8319775712497882 ALL Pearson: 0.8268857536393965\n",
      "spearman： 0.804962186439283 ALL Spearman: 0.8019701484916972\n",
      "time: 40.42671251296997\n",
      "0.005\n",
      "Train Epoch:22 [(0%)]\t Loss: 0.3010  Pearson:0.7444 Spearman:0.7546\n",
      "Train ALL Pearson: 0.7610293237781678\n",
      "Train  ALL Spearman: 0.7377123570726216\n",
      "34.99612092971802\n",
      "Test Epoch:22 [(0%)]\t Loss: 0.4508  Pearson:0.8443 Spearman:0.8158\n",
      "Test : Loss:0.4810 \n",
      "pearson： 0.8449410831730918 ALL Pearson: 0.8393105691722107\n",
      "spearman： 0.8218635629920908 ALL Spearman: 0.817118454653085\n",
      "time: 40.43880271911621\n",
      "0.005\n",
      "Train Epoch:23 [(0%)]\t Loss: 0.2899  Pearson:0.7606 Spearman:0.7636\n",
      "Train ALL Pearson: 0.7791822850096657\n",
      "Train  ALL Spearman: 0.7564347896247207\n",
      "35.264225244522095\n",
      "Test Epoch:23 [(0%)]\t Loss: 0.4326  Pearson:0.8478 Spearman:0.8148\n",
      "Test : Loss:0.4169 \n",
      "pearson： 0.8496192700558963 ALL Pearson: 0.8515526921762865\n",
      "spearman： 0.8233445504015545 ALL Spearman: 0.8279940311535765\n",
      "time: 40.59846329689026\n",
      "0.005\n",
      "Train Epoch:24 [(0%)]\t Loss: 0.2550  Pearson:0.8048 Spearman:0.7688\n",
      "Train ALL Pearson: 0.7894101976207232\n",
      "Train  ALL Spearman: 0.7691587343765379\n",
      "35.280375957489014\n",
      "Test Epoch:24 [(0%)]\t Loss: 0.3973  Pearson:0.8567 Spearman:0.8263\n",
      "Test : Loss:0.3881 \n",
      "pearson： 0.8612847818763526 ALL Pearson: 0.8605015023028133\n",
      "spearman： 0.8395558118245692 ALL Spearman: 0.836992577632272\n",
      "time: 40.48298692703247\n",
      "0.005\n",
      "Train Epoch:25 [(0%)]\t Loss: 0.2727  Pearson:0.7974 Spearman:0.7714\n",
      "Train ALL Pearson: 0.8037598724215449\n",
      "Train  ALL Spearman: 0.7797080339982861\n",
      "35.144089221954346\n",
      "Test Epoch:25 [(0%)]\t Loss: 0.4132  Pearson:0.8453 Spearman:0.7808\n",
      "Test : Loss:0.4158 \n",
      "pearson： 0.8654787961877694 ALL Pearson: 0.8694261961927748\n",
      "spearman： 0.8349363879485799 ALL Spearman: 0.8482521766800704\n",
      "time: 40.40986704826355\n",
      "0.005\n",
      "Train Epoch:26 [(0%)]\t Loss: 0.2573  Pearson:0.8256 Spearman:0.8064\n",
      "Train ALL Pearson: 0.8127740471127765\n",
      "Train  ALL Spearman: 0.7912190870856702\n",
      "35.449442625045776\n",
      "Test Epoch:26 [(0%)]\t Loss: 0.4140  Pearson:0.8447 Spearman:0.8194\n",
      "Test : Loss:0.4089 \n",
      "pearson： 0.870340697061733 ALL Pearson: 0.8758287419780773\n",
      "spearman： 0.845664076183504 ALL Spearman: 0.8516459308584715\n",
      "time: 40.63095688819885\n",
      "0.005\n",
      "Train Epoch:27 [(0%)]\t Loss: 0.2683  Pearson:0.8043 Spearman:0.7809\n",
      "Train ALL Pearson: 0.822094017536114\n",
      "Train  ALL Spearman: 0.7989067370437712\n",
      "35.49572420120239\n",
      "Test Epoch:27 [(0%)]\t Loss: 0.3982  Pearson:0.8462 Spearman:0.8182\n",
      "Test : Loss:0.3699 \n",
      "pearson： 0.876351939501281 ALL Pearson: 0.8815902444045953\n",
      "spearman： 0.8557540906972755 ALL Spearman: 0.857674936584799\n",
      "time: 40.85197973251343\n",
      "0.005\n",
      "Train Epoch:28 [(0%)]\t Loss: 0.2478  Pearson:0.8405 Spearman:0.8366\n",
      "Train ALL Pearson: 0.826620140859299\n",
      "Train  ALL Spearman: 0.8002701514787595\n",
      "35.12216925621033\n",
      "Test Epoch:28 [(0%)]\t Loss: 0.3733  Pearson:0.8926 Spearman:0.8755\n",
      "Test : Loss:0.3600 \n",
      "pearson： 0.8884984551415855 ALL Pearson: 0.8872304122568354\n",
      "spearman： 0.864946876025738 ALL Spearman: 0.8632186409101863\n",
      "time: 40.45046854019165\n",
      "0.005\n",
      "Train Epoch:29 [(0%)]\t Loss: 0.2266  Pearson:0.8485 Spearman:0.8126\n",
      "Train ALL Pearson: 0.8439636514951144\n",
      "Train  ALL Spearman: 0.8198651732804365\n",
      "35.240949392318726\n",
      "Test Epoch:29 [(0%)]\t Loss: 0.3783  Pearson:0.9014 Spearman:0.8819\n",
      "Test : Loss:0.3713 \n",
      "pearson： 0.8958749136485488 ALL Pearson: 0.8918845125766949\n",
      "spearman： 0.8719130841517074 ALL Spearman: 0.8673372528681556\n",
      "time: 40.48581886291504\n",
      "0.005\n",
      "Train Epoch:30 [(0%)]\t Loss: 0.2223  Pearson:0.8391 Spearman:0.7944\n",
      "Train ALL Pearson: 0.8459193135090791\n",
      "Train  ALL Spearman: 0.820671941588691\n",
      "35.303626537323\n",
      "Test Epoch:30 [(0%)]\t Loss: 0.3371  Pearson:0.9014 Spearman:0.8791\n",
      "Test : Loss:0.3376 \n",
      "pearson： 0.8949450333407225 ALL Pearson: 0.8944673585394758\n",
      "spearman： 0.8685700005318023 ALL Spearman: 0.8687988335528412\n",
      "time: 40.63391637802124\n",
      "0.03\n",
      "Train Epoch:31 [(0%)]\t Loss: 0.2346  Pearson:0.8467 Spearman:0.8175\n",
      "Train ALL Pearson: 0.6525462865985098\n",
      "Train  ALL Spearman: 0.6252561441051276\n",
      "35.07374286651611\n",
      "Test Epoch:31 [(0%)]\t Loss: 0.2574  Pearson:0.8881 Spearman:0.8506\n",
      "Test : Loss:0.2688 \n",
      "pearson： 0.8684138800866511 ALL Pearson: 0.8649688487253223\n",
      "spearman： 0.8349957287957189 ALL Spearman: 0.8276140449713932\n",
      "time: 40.27032017707825\n",
      "0.03\n",
      "Train Epoch:32 [(0%)]\t Loss: 0.2815  Pearson:0.7769 Spearman:0.7759\n",
      "Train ALL Pearson: 0.7537072590649309\n",
      "Train  ALL Spearman: 0.7210100904627209\n",
      "35.506402254104614\n",
      "Test Epoch:32 [(0%)]\t Loss: 0.2460  Pearson:0.8794 Spearman:0.8646\n",
      "Test : Loss:0.2568 \n",
      "pearson： 0.8762399260971381 ALL Pearson: 0.8744541847886789\n",
      "spearman： 0.8472299800321708 ALL Spearman: 0.8404092242415495\n",
      "time: 40.74672174453735\n",
      "0.03\n",
      "Train Epoch:33 [(0%)]\t Loss: 0.3084  Pearson:0.8678 Spearman:0.8335\n",
      "Train ALL Pearson: 0.7696626045950546\n",
      "Train  ALL Spearman: 0.7344236801980588\n",
      "35.11606454849243\n",
      "Test Epoch:33 [(0%)]\t Loss: 0.2418  Pearson:0.8638 Spearman:0.8095\n",
      "Test : Loss:0.2273 \n",
      "pearson： 0.8810626026548957 ALL Pearson: 0.884860541254722\n",
      "spearman： 0.8459919592914278 ALL Spearman: 0.8477639465984105\n",
      "time: 40.56135034561157\n",
      "0.03\n",
      "Train Epoch:34 [(0%)]\t Loss: 0.2696  Pearson:0.8462 Spearman:0.8250\n",
      "Train ALL Pearson: 0.8042563775031234\n",
      "Train  ALL Spearman: 0.7740576876800438\n",
      "35.81668138504028\n",
      "Test Epoch:34 [(0%)]\t Loss: 0.4346  Pearson:0.8787 Spearman:0.8706\n",
      "Test : Loss:0.4277 \n",
      "pearson： 0.8805935644279349 ALL Pearson: 0.8862949008449009\n",
      "spearman： 0.8627631056866826 ALL Spearman: 0.865393757883636\n",
      "time: 41.14239740371704\n",
      "0.03\n",
      "Train Epoch:35 [(0%)]\t Loss: 0.2164  Pearson:0.8919 Spearman:0.8802\n",
      "Train ALL Pearson: 0.8230720458955032\n",
      "Train  ALL Spearman: 0.7901913100203592\n",
      "35.304558515548706\n",
      "Test Epoch:35 [(0%)]\t Loss: 0.2352  Pearson:0.8713 Spearman:0.8419\n",
      "Test : Loss:0.2081 \n",
      "pearson： 0.8869955807617589 ALL Pearson: 0.8942499714744128\n",
      "spearman： 0.8599032737562204 ALL Spearman: 0.8623342560427859\n",
      "time: 40.7854380607605\n",
      "0.03\n",
      "Train Epoch:36 [(0%)]\t Loss: 0.2661  Pearson:0.8830 Spearman:0.8590\n",
      "Train ALL Pearson: 0.8180891779455575\n",
      "Train  ALL Spearman: 0.7887105233333735\n",
      "35.14751362800598\n",
      "Test Epoch:36 [(0%)]\t Loss: 0.2395  Pearson:0.8620 Spearman:0.8161\n",
      "Test : Loss:0.2280 \n",
      "pearson： 0.8893448828697725 ALL Pearson: 0.8954907653121226\n",
      "spearman： 0.8540969517970395 ALL Spearman: 0.8658892692349205\n",
      "time: 40.31962513923645\n",
      "0.03\n",
      "Train Epoch:37 [(0%)]\t Loss: 0.2192  Pearson:0.8704 Spearman:0.8349\n",
      "Train ALL Pearson: 0.8419157654169201\n",
      "Train  ALL Spearman: 0.810761291522357\n",
      "35.27378225326538\n",
      "Test Epoch:37 [(0%)]\t Loss: 0.2186  Pearson:0.9107 Spearman:0.8685\n",
      "Test : Loss:0.2255 \n",
      "pearson： 0.8986751204019368 ALL Pearson: 0.9013831453646344\n",
      "spearman： 0.8671203219359428 ALL Spearman: 0.8763898150801247\n",
      "time: 40.62957406044006\n",
      "0.03\n",
      "Train Epoch:38 [(0%)]\t Loss: 0.2140  Pearson:0.9086 Spearman:0.9012\n",
      "Train ALL Pearson: 0.8475230392342477\n",
      "Train  ALL Spearman: 0.8164967744139912\n",
      "35.79311990737915\n",
      "Test Epoch:38 [(0%)]\t Loss: 0.4705  Pearson:0.8988 Spearman:0.8694\n",
      "Test : Loss:0.4664 \n",
      "pearson： 0.8965827624647879 ALL Pearson: 0.8952582632442951\n",
      "spearman： 0.8827527441042381 ALL Spearman: 0.8825706239833592\n",
      "time: 41.00844645500183\n",
      "0.03\n",
      "Train Epoch:39 [(0%)]\t Loss: 0.2754  Pearson:0.9061 Spearman:0.8826\n",
      "Train ALL Pearson: 0.8587382266008882\n",
      "Train  ALL Spearman: 0.8307969227583258\n",
      "35.573288440704346\n",
      "Test Epoch:39 [(0%)]\t Loss: 0.1875  Pearson:0.9040 Spearman:0.8662\n",
      "Test : Loss:0.1833 \n",
      "pearson： 0.9114352227282206 ALL Pearson: 0.9106814071170701\n",
      "spearman： 0.8836311369691312 ALL Spearman: 0.8848478224282564\n",
      "time: 40.916269063949585\n",
      "0.03\n",
      "Train Epoch:40 [(0%)]\t Loss: 0.2413  Pearson:0.9102 Spearman:0.8867\n",
      "Train ALL Pearson: 0.8587532209195832\n",
      "Train  ALL Spearman: 0.8280689779566495\n",
      "35.179054260253906\n",
      "Test Epoch:40 [(0%)]\t Loss: 0.2816  Pearson:0.9230 Spearman:0.9012\n",
      "Test : Loss:0.2841 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.918992322666799 ALL Pearson: 0.9137287310547346\n",
      "spearman： 0.8978663895515322 ALL Spearman: 0.8947894264421467\n",
      "time: 40.630443811416626\n",
      "0.03\n",
      "Train Epoch:41 [(0%)]\t Loss: 0.1738  Pearson:0.9059 Spearman:0.8688\n",
      "Train ALL Pearson: 0.8617848320104755\n",
      "Train  ALL Spearman: 0.8273792894741625\n",
      "35.370798110961914\n",
      "Test Epoch:41 [(0%)]\t Loss: 0.4190  Pearson:0.9223 Spearman:0.8963\n",
      "Test : Loss:0.4074 \n",
      "pearson： 0.9167457173324195 ALL Pearson: 0.9123821920798456\n",
      "spearman： 0.893622473236201 ALL Spearman: 0.8909574601712535\n",
      "time: 40.79186773300171\n",
      "0.03\n",
      "Train Epoch:42 [(0%)]\t Loss: 0.2748  Pearson:0.8977 Spearman:0.8295\n",
      "Train ALL Pearson: 0.87488879593056\n",
      "Train  ALL Spearman: 0.8406197684951018\n",
      "35.09141802787781\n",
      "Test Epoch:42 [(0%)]\t Loss: 0.2330  Pearson:0.9112 Spearman:0.8777\n",
      "Test : Loss:0.2095 \n",
      "pearson： 0.9176171992832003 ALL Pearson: 0.9184105852456179\n",
      "spearman： 0.8925196996364608 ALL Spearman: 0.8950489900382018\n",
      "time: 40.392274141311646\n",
      "0.03\n",
      "Train Epoch:43 [(0%)]\t Loss: 0.1759  Pearson:0.9200 Spearman:0.8902\n",
      "Train ALL Pearson: 0.8812010812244185\n",
      "Train  ALL Spearman: 0.84788885299766\n",
      "35.62257218360901\n",
      "Test Epoch:43 [(0%)]\t Loss: 0.1867  Pearson:0.9162 Spearman:0.8889\n",
      "Test : Loss:0.1770 \n",
      "pearson： 0.9212783782913192 ALL Pearson: 0.9184155609701998\n",
      "spearman： 0.8967358495098492 ALL Spearman: 0.893750394640919\n",
      "time: 40.79591202735901\n",
      "0.03\n",
      "Train Epoch:44 [(0%)]\t Loss: 0.2304  Pearson:0.9071 Spearman:0.8705\n",
      "Train ALL Pearson: 0.8873183399584061\n",
      "Train  ALL Spearman: 0.8552340045488447\n",
      "35.27079749107361\n",
      "Test Epoch:44 [(0%)]\t Loss: 0.1796  Pearson:0.9251 Spearman:0.8993\n",
      "Test : Loss:0.1752 \n",
      "pearson： 0.9186995199939519 ALL Pearson: 0.9199658561911446\n",
      "spearman： 0.8911255535023739 ALL Spearman: 0.8964001163512945\n",
      "time: 40.475664377212524\n",
      "0.03\n",
      "Train Epoch:45 [(0%)]\t Loss: 0.2068  Pearson:0.9263 Spearman:0.8977\n",
      "Train ALL Pearson: 0.8883645603578502\n",
      "Train  ALL Spearman: 0.8537717504360043\n",
      "35.196890354156494\n",
      "Test Epoch:45 [(0%)]\t Loss: 0.1583  Pearson:0.9284 Spearman:0.8820\n",
      "Test : Loss:0.1745 \n",
      "pearson： 0.9234087863520163 ALL Pearson: 0.9218263765506766\n",
      "spearman： 0.8976571876644912 ALL Spearman: 0.899539802133937\n",
      "time: 40.380905628204346\n",
      "0.03\n",
      "Train Epoch:46 [(0%)]\t Loss: 0.1909  Pearson:0.9022 Spearman:0.8614\n",
      "Train ALL Pearson: 0.8970469443428634\n",
      "Train  ALL Spearman: 0.866360556828899\n",
      "35.33714413642883\n",
      "Test Epoch:46 [(0%)]\t Loss: 0.2666  Pearson:0.9165 Spearman:0.9076\n",
      "Test : Loss:0.2715 \n",
      "pearson： 0.9228324692833417 ALL Pearson: 0.9230953371830661\n",
      "spearman： 0.9024886816268766 ALL Spearman: 0.9016273515409085\n",
      "time: 40.92949080467224\n",
      "0.03\n",
      "Train Epoch:47 [(0%)]\t Loss: 0.1679  Pearson:0.9197 Spearman:0.8748\n",
      "Train ALL Pearson: 0.90027222874833\n",
      "Train  ALL Spearman: 0.8724006079772012\n",
      "35.44517374038696\n",
      "Test Epoch:47 [(0%)]\t Loss: 0.3219  Pearson:0.9189 Spearman:0.8913\n",
      "Test : Loss:0.3023 \n",
      "pearson： 0.9208477770532947 ALL Pearson: 0.9229260429361427\n",
      "spearman： 0.8965860083955467 ALL Spearman: 0.9017799519153843\n",
      "time: 40.760037660598755\n",
      "0.03\n",
      "Train Epoch:48 [(0%)]\t Loss: 0.1913  Pearson:0.9160 Spearman:0.8921\n",
      "Train ALL Pearson: 0.9063374755113469\n",
      "Train  ALL Spearman: 0.8789375432661963\n",
      "35.70727515220642\n",
      "Test Epoch:48 [(0%)]\t Loss: 0.1615  Pearson:0.9155 Spearman:0.9045\n",
      "Test : Loss:0.1641 \n",
      "pearson： 0.9213342328037885 ALL Pearson: 0.9262641642899874\n",
      "spearman： 0.9067734307782128 ALL Spearman: 0.9066798138113955\n",
      "time: 40.94021987915039\n",
      "0.03\n",
      "Train Epoch:49 [(0%)]\t Loss: 0.1907  Pearson:0.9263 Spearman:0.8790\n",
      "Train ALL Pearson: 0.9048636519315727\n",
      "Train  ALL Spearman: 0.87443098633187\n",
      "35.18366551399231\n",
      "Test Epoch:49 [(0%)]\t Loss: 0.1787  Pearson:0.9183 Spearman:0.8942\n",
      "Test : Loss:0.1759 \n",
      "pearson： 0.9262252156636713 ALL Pearson: 0.9267145679952407\n",
      "spearman： 0.9034557422018008 ALL Spearman: 0.9062098411131839\n",
      "time: 40.549741983413696\n",
      "0.03\n",
      "Train Epoch:50 [(0%)]\t Loss: 0.1691  Pearson:0.9354 Spearman:0.9015\n",
      "Train ALL Pearson: 0.9082706949444773\n",
      "Train  ALL Spearman: 0.881811377447705\n",
      "35.471697092056274\n",
      "Test Epoch:50 [(0%)]\t Loss: 0.2674  Pearson:0.9047 Spearman:0.8839\n",
      "Test : Loss:0.2771 \n",
      "pearson： 0.9252610062433037 ALL Pearson: 0.9277439424818261\n",
      "spearman： 0.9047155621839482 ALL Spearman: 0.908965389171351\n",
      "time: 40.63703942298889\n",
      "0.03\n",
      "Train Epoch:51 [(0%)]\t Loss: 0.1800  Pearson:0.9190 Spearman:0.9051\n",
      "Train ALL Pearson: 0.9035349269882863\n",
      "Train  ALL Spearman: 0.8755455064310905\n",
      "35.54629588127136\n",
      "Test Epoch:51 [(0%)]\t Loss: 0.2779  Pearson:0.9191 Spearman:0.8990\n",
      "Test : Loss:0.2734 \n",
      "pearson： 0.9254384943046702 ALL Pearson: 0.9280240577415183\n",
      "spearman： 0.9051874573042251 ALL Spearman: 0.9076126975349841\n",
      "time: 40.92369318008423\n",
      "0.03\n",
      "Train Epoch:52 [(0%)]\t Loss: 0.1801  Pearson:0.9309 Spearman:0.8853\n",
      "Train ALL Pearson: 0.9169471064121218\n",
      "Train  ALL Spearman: 0.892222890510031\n",
      "35.557066440582275\n",
      "Test Epoch:52 [(0%)]\t Loss: 0.2555  Pearson:0.9212 Spearman:0.8974\n",
      "Test : Loss:0.2493 \n",
      "pearson： 0.930723394750677 ALL Pearson: 0.9300079320469875\n",
      "spearman： 0.9084603872029898 ALL Spearman: 0.9099020751470606\n",
      "time: 40.982309103012085\n",
      "0.03\n",
      "Train Epoch:53 [(0%)]\t Loss: 0.1556  Pearson:0.9440 Spearman:0.9199\n",
      "Train ALL Pearson: 0.9124033561498737\n",
      "Train  ALL Spearman: 0.8879374121165217\n",
      "34.96417737007141\n",
      "Test Epoch:53 [(0%)]\t Loss: 0.2460  Pearson:0.9222 Spearman:0.9103\n",
      "Test : Loss:0.2540 \n",
      "pearson： 0.9264896445792078 ALL Pearson: 0.9299608031786404\n",
      "spearman： 0.9056177862795205 ALL Spearman: 0.9106876581983171\n",
      "time: 40.37199878692627\n",
      "0.03\n",
      "Train Epoch:54 [(0%)]\t Loss: 0.1659  Pearson:0.9387 Spearman:0.9157\n",
      "Train ALL Pearson: 0.9189896013254185\n",
      "Train  ALL Spearman: 0.8957115199028612\n",
      "35.53042221069336\n",
      "Test Epoch:54 [(0%)]\t Loss: 0.1634  Pearson:0.9248 Spearman:0.8923\n",
      "Test : Loss:0.1603 \n",
      "pearson： 0.9318221662801682 ALL Pearson: 0.9305076723874042\n",
      "spearman： 0.9053157219278701 ALL Spearman: 0.9105015010910691\n",
      "time: 40.92169404029846\n",
      "0.03\n",
      "Train Epoch:55 [(0%)]\t Loss: 0.1711  Pearson:0.9468 Spearman:0.9351\n",
      "Train ALL Pearson: 0.9163804386956096\n",
      "Train  ALL Spearman: 0.8908986999173947\n",
      "35.33381986618042\n",
      "Test Epoch:55 [(0%)]\t Loss: 0.2428  Pearson:0.9246 Spearman:0.8995\n",
      "Test : Loss:0.2273 \n",
      "pearson： 0.9323027303552219 ALL Pearson: 0.9310102225933928\n",
      "spearman： 0.9153779806546328 ALL Spearman: 0.9131475463510268\n",
      "time: 40.581323862075806\n",
      "0.03\n",
      "Train Epoch:56 [(0%)]\t Loss: 0.1681  Pearson:0.9341 Spearman:0.9285\n",
      "Train ALL Pearson: 0.9170649696814482\n",
      "Train  ALL Spearman: 0.8918031960703063\n",
      "35.183608055114746\n",
      "Test Epoch:56 [(0%)]\t Loss: 0.1973  Pearson:0.9173 Spearman:0.9168\n",
      "Test : Loss:0.2065 \n",
      "pearson： 0.9293629720006674 ALL Pearson: 0.9316681392009265\n",
      "spearman： 0.9148765396961803 ALL Spearman: 0.9138150471438481\n",
      "time: 40.468562841415405\n",
      "0.03\n",
      "Train Epoch:57 [(0%)]\t Loss: 0.1516  Pearson:0.9318 Spearman:0.9255\n",
      "Train ALL Pearson: 0.9223189524872708\n",
      "Train  ALL Spearman: 0.8986410959926897\n",
      "35.27435255050659\n",
      "Test Epoch:57 [(0%)]\t Loss: 0.1826  Pearson:0.9257 Spearman:0.9256\n",
      "Test : Loss:0.1738 \n",
      "pearson： 0.9311071444063306 ALL Pearson: 0.9324210837264939\n",
      "spearman： 0.9171864690407475 ALL Spearman: 0.913520260590155\n",
      "time: 40.5842227935791\n",
      "0.03\n",
      "Train Epoch:58 [(0%)]\t Loss: 0.1358  Pearson:0.9570 Spearman:0.9412\n",
      "Train ALL Pearson: 0.9253461230636046\n",
      "Train  ALL Spearman: 0.904700326180509\n",
      "35.96484613418579\n",
      "Test Epoch:58 [(0%)]\t Loss: 0.2013  Pearson:0.9455 Spearman:0.9253\n",
      "Test : Loss:0.2054 \n",
      "pearson： 0.9346983349869347 ALL Pearson: 0.932611500545402\n",
      "spearman： 0.9125192685293531 ALL Spearman: 0.9133837563371796\n",
      "time: 41.30561304092407\n",
      "0.03\n",
      "Train Epoch:59 [(0%)]\t Loss: 0.1419  Pearson:0.9507 Spearman:0.9172\n",
      "Train ALL Pearson: 0.926095623582466\n",
      "Train  ALL Spearman: 0.9050181580281611\n",
      "34.866867780685425\n",
      "Test Epoch:59 [(0%)]\t Loss: 0.1643  Pearson:0.9359 Spearman:0.9148\n",
      "Test : Loss:0.1739 \n",
      "pearson： 0.9331379123766103 ALL Pearson: 0.9327492658502491\n",
      "spearman： 0.9125434872040598 ALL Spearman: 0.9129688667405306\n",
      "time: 40.16218090057373\n",
      "0.03\n",
      "Train Epoch:60 [(0%)]\t Loss: 0.1533  Pearson:0.9548 Spearman:0.9422\n",
      "Train ALL Pearson: 0.9237245253616759\n",
      "Train  ALL Spearman: 0.9026352011299666\n",
      "35.4181125164032\n",
      "Test Epoch:60 [(0%)]\t Loss: 0.2056  Pearson:0.9302 Spearman:0.9094\n",
      "Test : Loss:0.1929 \n",
      "pearson： 0.931250619367944 ALL Pearson: 0.9338830003587694\n",
      "spearman： 0.9131342277250956 ALL Spearman: 0.915397285749888\n",
      "time: 40.655033349990845\n",
      "0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:61 [(0%)]\t Loss: 0.1446  Pearson:0.9013 Spearman:0.8920\n",
      "Train ALL Pearson: 0.9273364104720935\n",
      "Train  ALL Spearman: 0.9058704032136597\n",
      "34.92467379570007\n",
      "Test Epoch:61 [(0%)]\t Loss: 0.2839  Pearson:0.9427 Spearman:0.9230\n",
      "Test : Loss:0.2747 \n",
      "pearson： 0.9375136457541586 ALL Pearson: 0.9337637300079786\n",
      "spearman： 0.9176834402660766 ALL Spearman: 0.9156489714261602\n",
      "time: 40.23853921890259\n",
      "0.03\n",
      "Train Epoch:62 [(0%)]\t Loss: 0.1799  Pearson:0.9380 Spearman:0.9385\n",
      "Train ALL Pearson: 0.9255663679774391\n",
      "Train  ALL Spearman: 0.905081135271044\n",
      "35.139216899871826\n",
      "Test Epoch:62 [(0%)]\t Loss: 0.1551  Pearson:0.9357 Spearman:0.9316\n",
      "Test : Loss:0.1585 \n",
      "pearson： 0.9349528865906728 ALL Pearson: 0.9332430747184969\n",
      "spearman： 0.9221817132297762 ALL Spearman: 0.9153301107217847\n",
      "time: 40.51706075668335\n",
      "0.03\n",
      "Train Epoch:63 [(0%)]\t Loss: 0.1686  Pearson:0.9440 Spearman:0.9033\n",
      "Train ALL Pearson: 0.9369612018935113\n",
      "Train  ALL Spearman: 0.9194715649232345\n",
      "35.2944552898407\n",
      "Test Epoch:63 [(0%)]\t Loss: 0.2104  Pearson:0.9327 Spearman:0.9331\n",
      "Test : Loss:0.2051 \n",
      "pearson： 0.9312005071234535 ALL Pearson: 0.9338501043003133\n",
      "spearman： 0.9159014474863946 ALL Spearman: 0.9146196605563456\n",
      "time: 40.83833336830139\n",
      "0.03\n",
      "Train Epoch:64 [(0%)]\t Loss: 0.1576  Pearson:0.9497 Spearman:0.9203\n",
      "Train ALL Pearson: 0.9251124291640078\n",
      "Train  ALL Spearman: 0.9040384115072074\n",
      "35.25905728340149\n",
      "Test Epoch:64 [(0%)]\t Loss: 0.1842  Pearson:0.9379 Spearman:0.9259\n",
      "Test : Loss:0.1796 \n",
      "pearson： 0.9315553714529073 ALL Pearson: 0.9346103477641685\n",
      "spearman： 0.9139394613743146 ALL Spearman: 0.9156167685096623\n",
      "time: 40.83603048324585\n",
      "0.03\n",
      "Train Epoch:65 [(0%)]\t Loss: 0.1280  Pearson:0.9541 Spearman:0.9401\n",
      "Train ALL Pearson: 0.930814110501774\n",
      "Train  ALL Spearman: 0.9104029449562628\n",
      "35.30297565460205\n",
      "Test Epoch:65 [(0%)]\t Loss: 0.2251  Pearson:0.9326 Spearman:0.9328\n",
      "Test : Loss:0.2150 \n",
      "pearson： 0.9391994817684984 ALL Pearson: 0.935557781958727\n",
      "spearman： 0.9252872429080701 ALL Spearman: 0.9174401637203732\n",
      "time: 40.68779468536377\n",
      "0.03\n",
      "Train Epoch:66 [(0%)]\t Loss: 0.1543  Pearson:0.9365 Spearman:0.9214\n",
      "Train ALL Pearson: 0.9336574848802984\n",
      "Train  ALL Spearman: 0.9154312743549996\n",
      "35.231701612472534\n",
      "Test Epoch:66 [(0%)]\t Loss: 0.2103  Pearson:0.9283 Spearman:0.9073\n",
      "Test : Loss:0.1883 \n",
      "pearson： 0.9322485278357134 ALL Pearson: 0.935659665632697\n",
      "spearman： 0.9102663133227501 ALL Spearman: 0.9169594602714484\n",
      "time: 40.69156503677368\n",
      "0.03\n",
      "Train Epoch:67 [(0%)]\t Loss: 0.1431  Pearson:0.9422 Spearman:0.9314\n",
      "Train ALL Pearson: 0.9383807937704497\n",
      "Train  ALL Spearman: 0.920177051082056\n",
      "35.224207639694214\n",
      "Test Epoch:67 [(0%)]\t Loss: 0.1537  Pearson:0.9355 Spearman:0.9295\n",
      "Test : Loss:0.1557 \n",
      "pearson： 0.9367198546601987 ALL Pearson: 0.9358891578997236\n",
      "spearman： 0.9199762560209824 ALL Spearman: 0.9178432082326641\n",
      "time: 40.739192724227905\n",
      "0.03\n",
      "Train Epoch:68 [(0%)]\t Loss: 0.1487  Pearson:0.9553 Spearman:0.9335\n",
      "Train ALL Pearson: 0.93021828985912\n",
      "Train  ALL Spearman: 0.9098942444505669\n",
      "35.38570952415466\n",
      "Test Epoch:68 [(0%)]\t Loss: 0.2351  Pearson:0.9303 Spearman:0.9171\n",
      "Test : Loss:0.2260 \n",
      "pearson： 0.9346708228630102 ALL Pearson: 0.9350787303160241\n",
      "spearman： 0.9153186391005858 ALL Spearman: 0.916870260540891\n",
      "time: 40.64561152458191\n",
      "0.03\n",
      "Train Epoch:69 [(0%)]\t Loss: 0.1599  Pearson:0.9470 Spearman:0.9162\n",
      "Train ALL Pearson: 0.926557949791919\n",
      "Train  ALL Spearman: 0.9046197549804883\n",
      "35.386687994003296\n",
      "Test Epoch:69 [(0%)]\t Loss: 0.2327  Pearson:0.9306 Spearman:0.9175\n",
      "Test : Loss:0.2288 \n",
      "pearson： 0.9340813320032036 ALL Pearson: 0.9355035122544253\n",
      "spearman： 0.9189094865221764 ALL Spearman: 0.9181354649457875\n",
      "time: 40.68598794937134\n",
      "0.03\n",
      "Train Epoch:70 [(0%)]\t Loss: 0.1688  Pearson:0.9329 Spearman:0.9261\n",
      "Train ALL Pearson: 0.9355899574784231\n",
      "Train  ALL Spearman: 0.9166139728165907\n",
      "36.054107666015625\n",
      "Test Epoch:70 [(0%)]\t Loss: 0.2475  Pearson:0.9249 Spearman:0.8917\n",
      "Test : Loss:0.2344 \n",
      "pearson： 0.9346377796018847 ALL Pearson: 0.9352867765872683\n",
      "spearman： 0.912561781910842 ALL Spearman: 0.9172495546873346\n",
      "time: 41.41738748550415\n",
      "0.03\n",
      "Train Epoch:71 [(0%)]\t Loss: 0.1532  Pearson:0.9604 Spearman:0.9461\n",
      "Train ALL Pearson: 0.9342437274685514\n",
      "Train  ALL Spearman: 0.9142495892835257\n",
      "35.85156559944153\n",
      "Test Epoch:71 [(0%)]\t Loss: 0.2123  Pearson:0.9345 Spearman:0.9265\n",
      "Test : Loss:0.2205 \n",
      "pearson： 0.9378119116801252 ALL Pearson: 0.935777933069912\n",
      "spearman： 0.9222176726047376 ALL Spearman: 0.9181277109664636\n",
      "time: 41.2633695602417\n",
      "0.03\n",
      "Train Epoch:72 [(0%)]\t Loss: 0.1496  Pearson:0.9514 Spearman:0.9475\n",
      "Train ALL Pearson: 0.9351527114436442\n",
      "Train  ALL Spearman: 0.9146146999162188\n",
      "35.25585174560547\n",
      "Test Epoch:72 [(0%)]\t Loss: 0.2338  Pearson:0.9230 Spearman:0.9031\n",
      "Test : Loss:0.2333 \n",
      "pearson： 0.9359094228735014 ALL Pearson: 0.936149923693373\n",
      "spearman： 0.9171187311894144 ALL Spearman: 0.9179356303550177\n",
      "time: 40.60917901992798\n",
      "0.03\n",
      "Train Epoch:73 [(0%)]\t Loss: 0.1607  Pearson:0.9579 Spearman:0.9287\n",
      "Train ALL Pearson: 0.9376358819183385\n",
      "Train  ALL Spearman: 0.9189578731518483\n",
      "35.933701515197754\n",
      "Test Epoch:73 [(0%)]\t Loss: 0.2131  Pearson:0.9205 Spearman:0.9017\n",
      "Test : Loss:0.1982 \n",
      "pearson： 0.9294747749638419 ALL Pearson: 0.9362601023437305\n",
      "spearman： 0.9121562837660548 ALL Spearman: 0.9174534338139048\n",
      "time: 41.23521566390991\n",
      "0.03\n",
      "Train Epoch:74 [(0%)]\t Loss: 0.1385  Pearson:0.9427 Spearman:0.9337\n",
      "Train ALL Pearson: 0.9372714806619282\n",
      "Train  ALL Spearman: 0.9198917191575837\n",
      "35.57660937309265\n",
      "Test Epoch:74 [(0%)]\t Loss: 0.1451  Pearson:0.9268 Spearman:0.8937\n",
      "Test : Loss:0.1586 \n",
      "pearson： 0.9336381839613576 ALL Pearson: 0.9360665166513922\n",
      "spearman： 0.9095627523293452 ALL Spearman: 0.9183689467451764\n",
      "time: 40.94388699531555\n",
      "0.03\n",
      "Train Epoch:75 [(0%)]\t Loss: 0.1459  Pearson:0.9598 Spearman:0.9445\n",
      "Train ALL Pearson: 0.9351987689541547\n",
      "Train  ALL Spearman: 0.9146554792903162\n",
      "35.172842264175415\n",
      "Test Epoch:75 [(0%)]\t Loss: 0.2211  Pearson:0.9469 Spearman:0.9227\n",
      "Test : Loss:0.2349 \n",
      "pearson： 0.9406790610031988 ALL Pearson: 0.9364973906881828\n",
      "spearman： 0.9212053677788525 ALL Spearman: 0.9183432265923532\n",
      "time: 40.54741072654724\n",
      "0.03\n",
      "Train Epoch:76 [(0%)]\t Loss: 0.1633  Pearson:0.9507 Spearman:0.9334\n",
      "Train ALL Pearson: 0.9419619596648269\n",
      "Train  ALL Spearman: 0.924913724086626\n",
      "35.691940784454346\n",
      "Test Epoch:76 [(0%)]\t Loss: 0.1672  Pearson:0.9417 Spearman:0.9243\n",
      "Test : Loss:0.1539 \n",
      "pearson： 0.9387408262497989 ALL Pearson: 0.9362636003156383\n",
      "spearman： 0.9210022544810148 ALL Spearman: 0.9182413933385379\n",
      "time: 41.061453104019165\n",
      "0.03\n",
      "Train Epoch:77 [(0%)]\t Loss: 0.1755  Pearson:0.9639 Spearman:0.9514\n",
      "Train ALL Pearson: 0.9356316624566469\n",
      "Train  ALL Spearman: 0.9171360440874939\n",
      "35.383121490478516\n",
      "Test Epoch:77 [(0%)]\t Loss: 0.1445  Pearson:0.9438 Spearman:0.9171\n",
      "Test : Loss:0.1561 \n",
      "pearson： 0.9385107493548571 ALL Pearson: 0.935643082879538\n",
      "spearman： 0.9174234949810413 ALL Spearman: 0.9171253362759645\n",
      "time: 40.851826906204224\n",
      "0.03\n",
      "Train Epoch:78 [(0%)]\t Loss: 0.1674  Pearson:0.9607 Spearman:0.9243\n",
      "Train ALL Pearson: 0.938480387062402\n",
      "Train  ALL Spearman: 0.9205433310663168\n",
      "35.67886281013489\n",
      "Test Epoch:78 [(0%)]\t Loss: 0.1619  Pearson:0.9293 Spearman:0.9010\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9345693902095683 ALL Pearson: 0.9359100035159875\n",
      "spearman： 0.9137313345952071 ALL Spearman: 0.9184566827180396\n",
      "time: 41.04583430290222\n",
      "0.03\n",
      "Train Epoch:79 [(0%)]\t Loss: 0.1483  Pearson:0.9465 Spearman:0.9388\n",
      "Train ALL Pearson: 0.9403793709702012\n",
      "Train  ALL Spearman: 0.9242335194679071\n",
      "35.05681920051575\n",
      "Test Epoch:79 [(0%)]\t Loss: 0.1644  Pearson:0.9432 Spearman:0.9371\n",
      "Test : Loss:0.1652 \n",
      "pearson： 0.9352914744408637 ALL Pearson: 0.9358380823282755\n",
      "spearman： 0.9222095567497886 ALL Spearman: 0.9185124251241904\n",
      "time: 40.51207089424133\n",
      "0.03\n",
      "Train Epoch:80 [(0%)]\t Loss: 0.1362  Pearson:0.9468 Spearman:0.9254\n",
      "Train ALL Pearson: 0.9345252865570762\n",
      "Train  ALL Spearman: 0.9152470935049604\n",
      "35.30561327934265\n",
      "Test Epoch:80 [(0%)]\t Loss: 0.1555  Pearson:0.9240 Spearman:0.9075\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.9355468184233108 ALL Pearson: 0.9359062732475364\n",
      "spearman： 0.9196898073750958 ALL Spearman: 0.9175517669728612\n",
      "time: 40.50929093360901\n",
      "0.03\n",
      "Train Epoch:81 [(0%)]\t Loss: 0.1597  Pearson:0.9652 Spearman:0.9466\n",
      "Train ALL Pearson: 0.9482081528962449\n",
      "Train  ALL Spearman: 0.9326059421470607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.31699538230896\n",
      "Test Epoch:81 [(0%)]\t Loss: 0.2366  Pearson:0.9083 Spearman:0.8818\n",
      "Test : Loss:0.2185 \n",
      "pearson： 0.9296636029706505 ALL Pearson: 0.9352142616887303\n",
      "spearman： 0.906290963737653 ALL Spearman: 0.9154499010428425\n",
      "time: 40.75554275512695\n",
      "0.03\n",
      "Train Epoch:82 [(0%)]\t Loss: 0.1533  Pearson:0.9505 Spearman:0.9474\n",
      "Train ALL Pearson: 0.9393141958589302\n",
      "Train  ALL Spearman: 0.92223717877874\n",
      "35.447641372680664\n",
      "Test Epoch:82 [(0%)]\t Loss: 0.1441  Pearson:0.9504 Spearman:0.9307\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.9416600846205833 ALL Pearson: 0.9361657356027356\n",
      "spearman： 0.9212566212781471 ALL Spearman: 0.9171476954286685\n",
      "time: 40.70795512199402\n",
      "0.03\n",
      "Train Epoch:83 [(0%)]\t Loss: 0.1337  Pearson:0.9658 Spearman:0.9623\n",
      "Train ALL Pearson: 0.9394451034719498\n",
      "Train  ALL Spearman: 0.9222447430020145\n",
      "35.60877704620361\n",
      "Test Epoch:83 [(0%)]\t Loss: 0.1537  Pearson:0.9407 Spearman:0.9118\n",
      "Test : Loss:0.1535 \n",
      "pearson： 0.9352253512259712 ALL Pearson: 0.9363885946845852\n",
      "spearman： 0.911178150020172 ALL Spearman: 0.9181882100462274\n",
      "time: 40.978400468826294\n",
      "0.03\n",
      "Train Epoch:84 [(0%)]\t Loss: 0.1445  Pearson:0.9527 Spearman:0.9452\n",
      "Train ALL Pearson: 0.9424679418407748\n",
      "Train  ALL Spearman: 0.925067401291813\n",
      "35.38070344924927\n",
      "Test Epoch:84 [(0%)]\t Loss: 0.1969  Pearson:0.9410 Spearman:0.9038\n",
      "Test : Loss:0.1901 \n",
      "pearson： 0.9371285331244046 ALL Pearson: 0.9370424201699495\n",
      "spearman： 0.9166051551845823 ALL Spearman: 0.9192231386024872\n",
      "time: 40.80896306037903\n",
      "0.03\n",
      "Train Epoch:85 [(0%)]\t Loss: 0.1398  Pearson:0.9415 Spearman:0.9368\n",
      "Train ALL Pearson: 0.9441210350825494\n",
      "Train  ALL Spearman: 0.9284025785669122\n",
      "35.49175691604614\n",
      "Test Epoch:85 [(0%)]\t Loss: 0.1594  Pearson:0.9371 Spearman:0.9249\n",
      "Test : Loss:0.1594 \n",
      "pearson： 0.9352253676515544 ALL Pearson: 0.9362454876072663\n",
      "spearman： 0.9195726433987557 ALL Spearman: 0.9170440445336188\n",
      "time: 40.78971457481384\n",
      "0.03\n",
      "Train Epoch:86 [(0%)]\t Loss: 0.1284  Pearson:0.9533 Spearman:0.9383\n",
      "Train ALL Pearson: 0.9410762308334628\n",
      "Train  ALL Spearman: 0.9242884696552514\n",
      "36.17620825767517\n",
      "Test Epoch:86 [(0%)]\t Loss: 0.2143  Pearson:0.9431 Spearman:0.9184\n",
      "Test : Loss:0.2292 \n",
      "pearson： 0.9377370713410006 ALL Pearson: 0.9361303832673488\n",
      "spearman： 0.9173666209570162 ALL Spearman: 0.9181470926445982\n",
      "time: 41.43165040016174\n",
      "0.03\n",
      "Train Epoch:87 [(0%)]\t Loss: 0.1597  Pearson:0.9687 Spearman:0.9539\n",
      "Train ALL Pearson: 0.9474572180671822\n",
      "Train  ALL Spearman: 0.9314481092317022\n",
      "36.10169768333435\n",
      "Test Epoch:87 [(0%)]\t Loss: 0.1828  Pearson:0.9391 Spearman:0.9271\n",
      "Test : Loss:0.1877 \n",
      "pearson： 0.9357100073838278 ALL Pearson: 0.9366204146882278\n",
      "spearman： 0.9172434633235876 ALL Spearman: 0.9187735962793748\n",
      "time: 41.354942321777344\n",
      "0.03\n",
      "Train Epoch:88 [(0%)]\t Loss: 0.1227  Pearson:0.9637 Spearman:0.9517\n",
      "Train ALL Pearson: 0.9412335017732923\n",
      "Train  ALL Spearman: 0.9236686032197935\n",
      "35.53459429740906\n",
      "Test Epoch:88 [(0%)]\t Loss: 0.1870  Pearson:0.9232 Spearman:0.8786\n",
      "Test : Loss:0.1858 \n",
      "pearson： 0.9347764701392957 ALL Pearson: 0.936787739682742\n",
      "spearman： 0.9100379356251556 ALL Spearman: 0.9187457015486927\n",
      "time: 40.97146511077881\n",
      "0.03\n",
      "Train Epoch:89 [(0%)]\t Loss: 0.1192  Pearson:0.9623 Spearman:0.9487\n",
      "Train ALL Pearson: 0.9466307162227122\n",
      "Train  ALL Spearman: 0.9313130708164739\n",
      "35.66216039657593\n",
      "Test Epoch:89 [(0%)]\t Loss: 0.2067  Pearson:0.9385 Spearman:0.9099\n",
      "Test : Loss:0.2203 \n",
      "pearson： 0.9320780445382617 ALL Pearson: 0.9363009132320558\n",
      "spearman： 0.914126707314857 ALL Spearman: 0.9183946369194128\n",
      "time: 40.87538504600525\n",
      "0.03\n",
      "Train Epoch:90 [(0%)]\t Loss: 0.1422  Pearson:0.9567 Spearman:0.9403\n",
      "Train ALL Pearson: 0.9430357595738301\n",
      "Train  ALL Spearman: 0.9256374067168759\n",
      "35.79341220855713\n",
      "Test Epoch:90 [(0%)]\t Loss: 0.1527  Pearson:0.9478 Spearman:0.9304\n",
      "Test : Loss:0.1556 \n",
      "pearson： 0.9405971934735856 ALL Pearson: 0.9364399369948805\n",
      "spearman： 0.922825068749178 ALL Spearman: 0.9169316484717794\n",
      "time: 41.08045721054077\n",
      "0.03\n",
      "Train Epoch:91 [(0%)]\t Loss: 0.1460  Pearson:0.9662 Spearman:0.9491\n",
      "Train ALL Pearson: 0.9411277674787284\n",
      "Train  ALL Spearman: 0.9221381032571481\n",
      "35.42234778404236\n",
      "Test Epoch:91 [(0%)]\t Loss: 0.1540  Pearson:0.9450 Spearman:0.9037\n",
      "Test : Loss:0.1538 \n",
      "pearson： 0.9378691179649377 ALL Pearson: 0.9365485438592869\n",
      "spearman： 0.9113371058014536 ALL Spearman: 0.917085475116593\n",
      "time: 40.66829180717468\n",
      "0.03\n",
      "Train Epoch:92 [(0%)]\t Loss: 0.1417  Pearson:0.9631 Spearman:0.9516\n",
      "Train ALL Pearson: 0.9444313524553746\n",
      "Train  ALL Spearman: 0.9282278313211055\n",
      "35.269842863082886\n",
      "Test Epoch:92 [(0%)]\t Loss: 0.1679  Pearson:0.9514 Spearman:0.9175\n",
      "Test : Loss:0.1871 \n",
      "pearson： 0.9431411618941827 ALL Pearson: 0.9370884093678992\n",
      "spearman： 0.9228920515983711 ALL Spearman: 0.9180989531424123\n",
      "time: 40.58791923522949\n",
      "0.03\n",
      "Train Epoch:93 [(0%)]\t Loss: 0.1279  Pearson:0.9544 Spearman:0.9388\n",
      "Train ALL Pearson: 0.9407245319481062\n",
      "Train  ALL Spearman: 0.9239579032193248\n",
      "35.71553087234497\n",
      "Test Epoch:93 [(0%)]\t Loss: 0.1702  Pearson:0.9432 Spearman:0.9233\n",
      "Test : Loss:0.1740 \n",
      "pearson： 0.9393543396567474 ALL Pearson: 0.9373703054630554\n",
      "spearman： 0.9181794250635817 ALL Spearman: 0.9191354037302495\n",
      "time: 40.93485713005066\n",
      "0.03\n",
      "Train Epoch:94 [(0%)]\t Loss: 0.1117  Pearson:0.9655 Spearman:0.9531\n",
      "Train ALL Pearson: 0.9466376772325005\n",
      "Train  ALL Spearman: 0.9309142763794476\n",
      "35.912851333618164\n",
      "Test Epoch:94 [(0%)]\t Loss: 0.1740  Pearson:0.9381 Spearman:0.9179\n",
      "Test : Loss:0.1821 \n",
      "pearson： 0.9364454692408599 ALL Pearson: 0.9372527207126702\n",
      "spearman： 0.9144758047530535 ALL Spearman: 0.919149493033262\n",
      "time: 41.3996696472168\n",
      "0.03\n",
      "Train Epoch:95 [(0%)]\t Loss: 0.1262  Pearson:0.9623 Spearman:0.9439\n",
      "Train ALL Pearson: 0.9480101844950279\n",
      "Train  ALL Spearman: 0.9319309727662598\n",
      "35.73574352264404\n",
      "Test Epoch:95 [(0%)]\t Loss: 0.1907  Pearson:0.9530 Spearman:0.9392\n",
      "Test : Loss:0.2094 \n",
      "pearson： 0.9387984848766965 ALL Pearson: 0.9371376774531436\n",
      "spearman： 0.921194471760748 ALL Spearman: 0.9187552930301653\n",
      "time: 41.00590801239014\n",
      "0.03\n",
      "Train Epoch:96 [(0%)]\t Loss: 0.1656  Pearson:0.9567 Spearman:0.9541\n",
      "Train ALL Pearson: 0.9404522840089145\n",
      "Train  ALL Spearman: 0.9233066365064232\n",
      "35.26181364059448\n",
      "Test Epoch:96 [(0%)]\t Loss: 0.1490  Pearson:0.9484 Spearman:0.9326\n",
      "Test : Loss:0.1494 \n",
      "pearson： 0.9408657824607015 ALL Pearson: 0.9378801849322888\n",
      "spearman： 0.9223174209679595 ALL Spearman: 0.9193650027621258\n",
      "time: 40.630091428756714\n",
      "0.03\n",
      "Train Epoch:97 [(0%)]\t Loss: 0.1259  Pearson:0.9664 Spearman:0.9511\n",
      "Train ALL Pearson: 0.945710391642836\n",
      "Train  ALL Spearman: 0.929702404624464\n",
      "34.83880591392517\n",
      "Test Epoch:97 [(0%)]\t Loss: 0.1493  Pearson:0.9389 Spearman:0.8798\n",
      "Test : Loss:0.1528 \n",
      "pearson： 0.9374659817973536 ALL Pearson: 0.9371108499737847\n",
      "spearman： 0.9113160040275101 ALL Spearman: 0.9181388561041881\n",
      "time: 40.180092573165894\n",
      "0.03\n",
      "Train Epoch:98 [(0%)]\t Loss: 0.1220  Pearson:0.9653 Spearman:0.9531\n",
      "Train ALL Pearson: 0.9519209203701929\n",
      "Train  ALL Spearman: 0.9374165238356479\n",
      "35.87890148162842\n",
      "Test Epoch:98 [(0%)]\t Loss: 0.1858  Pearson:0.9442 Spearman:0.9144\n",
      "Test : Loss:0.1831 \n",
      "pearson： 0.9407860417687867 ALL Pearson: 0.9372996703306101\n",
      "spearman： 0.9182041164905228 ALL Spearman: 0.9188899334410298\n",
      "time: 41.364142417907715\n",
      "0.03\n",
      "Train Epoch:99 [(0%)]\t Loss: 0.1323  Pearson:0.9681 Spearman:0.9655\n",
      "Train ALL Pearson: 0.9449117443429409\n",
      "Train  ALL Spearman: 0.9289144940777309\n",
      "35.483052492141724\n",
      "Test Epoch:99 [(0%)]\t Loss: 0.1757  Pearson:0.8983 Spearman:0.8770\n",
      "Test : Loss:0.1670 \n",
      "pearson： 0.9300140919782393 ALL Pearson: 0.9375967491258911\n",
      "spearman： 0.911065418450463 ALL Spearman: 0.9194441303133264\n",
      "time: 40.77292776107788\n",
      "0.03\n",
      "Train Epoch:100 [(0%)]\t Loss: 0.1068  Pearson:0.9637 Spearman:0.9452\n",
      "Train ALL Pearson: 0.952170498155778\n",
      "Train  ALL Spearman: 0.9374284052815977\n",
      "35.49105930328369\n",
      "Test Epoch:100 [(0%)]\t Loss: 0.1458  Pearson:0.9343 Spearman:0.9034\n",
      "Test : Loss:0.1539 \n",
      "pearson： 0.9356068337762274 ALL Pearson: 0.9365873394420569\n",
      "spearman： 0.9126068786099865 ALL Spearman: 0.9192538017661485\n",
      "time: 40.872334003448486\n",
      "0.03\n",
      "Train Epoch:101 [(0%)]\t Loss: 0.1308  Pearson:0.9708 Spearman:0.9589\n",
      "Train ALL Pearson: 0.9438840283572691\n",
      "Train  ALL Spearman: 0.9271842125313785\n",
      "35.21056818962097\n",
      "Test Epoch:101 [(0%)]\t Loss: 0.1607  Pearson:0.9368 Spearman:0.9220\n",
      "Test : Loss:0.1610 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9392470389869021 ALL Pearson: 0.9358242384303631\n",
      "spearman： 0.9233123426790821 ALL Spearman: 0.9179513499225936\n",
      "time: 40.54667782783508\n",
      "0.03\n",
      "Train Epoch:102 [(0%)]\t Loss: 0.1141  Pearson:0.9572 Spearman:0.9438\n",
      "Train ALL Pearson: 0.9516787148842837\n",
      "Train  ALL Spearman: 0.9371694261914403\n",
      "35.852152824401855\n",
      "Test Epoch:102 [(0%)]\t Loss: 0.1584  Pearson:0.9475 Spearman:0.9310\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.9395627926006398 ALL Pearson: 0.9362933121255098\n",
      "spearman： 0.9162584420717749 ALL Spearman: 0.9183217188886224\n",
      "time: 41.23044013977051\n",
      "0.03\n",
      "Train Epoch:103 [(0%)]\t Loss: 0.1205  Pearson:0.9613 Spearman:0.9481\n",
      "Train ALL Pearson: 0.9463595915531888\n",
      "Train  ALL Spearman: 0.9300360831939172\n",
      "34.605229139328\n",
      "Test Epoch:103 [(0%)]\t Loss: 0.1527  Pearson:0.9372 Spearman:0.9238\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9370397491333474 ALL Pearson: 0.9355673681769796\n",
      "spearman： 0.9220466555489909 ALL Spearman: 0.9172169583154597\n",
      "time: 39.34570860862732\n",
      "0.03\n",
      "Train Epoch:104 [(0%)]\t Loss: 0.1297  Pearson:0.9643 Spearman:0.9496\n",
      "Train ALL Pearson: 0.9459913152626219\n",
      "Train  ALL Spearman: 0.9296710493914624\n",
      "32.250515937805176\n",
      "Test Epoch:104 [(0%)]\t Loss: 0.1603  Pearson:0.9201 Spearman:0.9009\n",
      "Test : Loss:0.1552 \n",
      "pearson： 0.9317107928171416 ALL Pearson: 0.9355584512396706\n",
      "spearman： 0.9179266531414396 ALL Spearman: 0.9169142835758601\n",
      "time: 36.8990261554718\n",
      "0.03\n",
      "Train Epoch:105 [(0%)]\t Loss: 0.1395  Pearson:0.9702 Spearman:0.9569\n",
      "Train ALL Pearson: 0.9510239017256898\n",
      "Train  ALL Spearman: 0.9363472019106726\n",
      "32.45094704627991\n",
      "Test Epoch:105 [(0%)]\t Loss: 0.1533  Pearson:0.9312 Spearman:0.9193\n",
      "Test : Loss:0.1535 \n",
      "pearson： 0.9358364249418168 ALL Pearson: 0.9352050623090081\n",
      "spearman： 0.9189710193047129 ALL Spearman: 0.9172353257355909\n",
      "time: 37.1684353351593\n",
      "0.03\n",
      "Train Epoch:106 [(0%)]\t Loss: 0.1375  Pearson:0.9624 Spearman:0.9502\n",
      "Train ALL Pearson: 0.952153026093334\n",
      "Train  ALL Spearman: 0.9377079096212929\n",
      "31.99405550956726\n",
      "Test Epoch:106 [(0%)]\t Loss: 0.1548  Pearson:0.9330 Spearman:0.9211\n",
      "Test : Loss:0.1524 \n",
      "pearson： 0.9338697962590294 ALL Pearson: 0.935862229751305\n",
      "spearman： 0.9174859723298163 ALL Spearman: 0.9181675332869547\n",
      "time: 36.575586795806885\n",
      "0.03\n",
      "Train Epoch:107 [(0%)]\t Loss: 0.1146  Pearson:0.9623 Spearman:0.9481\n",
      "Train ALL Pearson: 0.9485340772131228\n",
      "Train  ALL Spearman: 0.9327853469327243\n",
      "31.667932271957397\n",
      "Test Epoch:107 [(0%)]\t Loss: 0.2105  Pearson:0.9361 Spearman:0.9131\n",
      "Test : Loss:0.1953 \n",
      "pearson： 0.9354441018995409 ALL Pearson: 0.9355860477522676\n",
      "spearman： 0.9173336137549309 ALL Spearman: 0.9172623479958295\n",
      "time: 36.2893283367157\n",
      "0.03\n",
      "Train Epoch:108 [(0%)]\t Loss: 0.1316  Pearson:0.9593 Spearman:0.9386\n",
      "Train ALL Pearson: 0.9496186143665765\n",
      "Train  ALL Spearman: 0.9346251567625614\n",
      "32.441136837005615\n",
      "Test Epoch:108 [(0%)]\t Loss: 0.1993  Pearson:0.9388 Spearman:0.9222\n",
      "Test : Loss:0.1948 \n",
      "pearson： 0.9321180255465289 ALL Pearson: 0.9353547068859736\n",
      "spearman： 0.911678826091113 ALL Spearman: 0.9174257781929518\n",
      "time: 37.149627685546875\n",
      "0.03\n",
      "Train Epoch:109 [(0%)]\t Loss: 0.1291  Pearson:0.9621 Spearman:0.9478\n",
      "Train ALL Pearson: 0.9491051669870009\n",
      "Train  ALL Spearman: 0.9323753564790042\n",
      "31.754196643829346\n",
      "Test Epoch:109 [(0%)]\t Loss: 0.1863  Pearson:0.9250 Spearman:0.8753\n",
      "Test : Loss:0.1852 \n",
      "pearson： 0.932938076755711 ALL Pearson: 0.936707991323465\n",
      "spearman： 0.910024773542239 ALL Spearman: 0.9188216451799173\n",
      "time: 36.50467324256897\n",
      "0.03\n",
      "Train Epoch:110 [(0%)]\t Loss: 0.1184  Pearson:0.9699 Spearman:0.9434\n",
      "Train ALL Pearson: 0.9529328660846149\n",
      "Train  ALL Spearman: 0.9380379938817255\n",
      "31.853930711746216\n",
      "Test Epoch:110 [(0%)]\t Loss: 0.1885  Pearson:0.9190 Spearman:0.8883\n",
      "Test : Loss:0.1842 \n",
      "pearson： 0.9298845966970845 ALL Pearson: 0.9346171825243765\n",
      "spearman： 0.9078371603534863 ALL Spearman: 0.9154886716418339\n",
      "time: 36.42346477508545\n",
      "0.03\n",
      "Train Epoch:111 [(0%)]\t Loss: 0.1190  Pearson:0.9685 Spearman:0.9446\n",
      "Train ALL Pearson: 0.9470957078108603\n",
      "Train  ALL Spearman: 0.9299259536837436\n",
      "32.04699754714966\n",
      "Test Epoch:111 [(0%)]\t Loss: 0.2078  Pearson:0.9055 Spearman:0.8832\n",
      "Test : Loss:0.2139 \n",
      "pearson： 0.930677407250328 ALL Pearson: 0.9362518536903084\n",
      "spearman： 0.9166790491153308 ALL Spearman: 0.9182002909710871\n",
      "time: 36.804471492767334\n",
      "0.03\n",
      "Train Epoch:112 [(0%)]\t Loss: 0.1391  Pearson:0.9762 Spearman:0.9688\n",
      "Train ALL Pearson: 0.9507503140364749\n",
      "Train  ALL Spearman: 0.9355550931109158\n",
      "31.947463035583496\n",
      "Test Epoch:112 [(0%)]\t Loss: 0.2113  Pearson:0.9079 Spearman:0.8902\n",
      "Test : Loss:0.2200 \n",
      "pearson： 0.9288302452292097 ALL Pearson: 0.9354820245616993\n",
      "spearman： 0.909450367403128 ALL Spearman: 0.916243763872952\n",
      "time: 36.56933259963989\n",
      "0.03\n",
      "Train Epoch:113 [(0%)]\t Loss: 0.1560  Pearson:0.9503 Spearman:0.9327\n",
      "Train ALL Pearson: 0.9526567779749273\n",
      "Train  ALL Spearman: 0.9371136313963508\n",
      "32.1913058757782\n",
      "Test Epoch:113 [(0%)]\t Loss: 0.2223  Pearson:0.9454 Spearman:0.9241\n",
      "Test : Loss:0.2052 \n",
      "pearson： 0.9385156078262065 ALL Pearson: 0.9356588697278583\n",
      "spearman： 0.9204732775566945 ALL Spearman: 0.9172006601446944\n",
      "time: 36.93783950805664\n",
      "0.03\n",
      "Train Epoch:114 [(0%)]\t Loss: 0.1236  Pearson:0.9667 Spearman:0.9577\n",
      "Train ALL Pearson: 0.9503985487469169\n",
      "Train  ALL Spearman: 0.9351428679299602\n",
      "31.976236581802368\n",
      "Test Epoch:114 [(0%)]\t Loss: 0.1503  Pearson:0.9321 Spearman:0.9201\n",
      "Test : Loss:0.1605 \n",
      "pearson： 0.9324131532838421 ALL Pearson: 0.9351780508389907\n",
      "spearman： 0.9141420235293505 ALL Spearman: 0.9161779573545833\n",
      "time: 36.69829845428467\n",
      "0.03\n",
      "Train Epoch:115 [(0%)]\t Loss: 0.1372  Pearson:0.9750 Spearman:0.9434\n",
      "Train ALL Pearson: 0.953771522942299\n",
      "Train  ALL Spearman: 0.9405603407054749\n",
      "31.700932502746582\n",
      "Test Epoch:115 [(0%)]\t Loss: 0.1566  Pearson:0.9324 Spearman:0.9258\n",
      "Test : Loss:0.1541 \n",
      "pearson： 0.9348039481567118 ALL Pearson: 0.9365633268831393\n",
      "spearman： 0.9191307413743908 ALL Spearman: 0.9174367307895858\n",
      "time: 36.354788064956665\n",
      "0.03\n",
      "Train Epoch:116 [(0%)]\t Loss: 0.1201  Pearson:0.9520 Spearman:0.9385\n",
      "Train ALL Pearson: 0.9516520408857327\n",
      "Train  ALL Spearman: 0.9383060661224322\n",
      "31.86665964126587\n",
      "Test Epoch:116 [(0%)]\t Loss: 0.1533  Pearson:0.9437 Spearman:0.9248\n",
      "Test : Loss:0.1538 \n",
      "pearson： 0.9357536084745374 ALL Pearson: 0.9351271873671371\n",
      "spearman： 0.9161439730515805 ALL Spearman: 0.9156086218386955\n",
      "time: 36.63613176345825\n",
      "0.03\n",
      "Train Epoch:117 [(0%)]\t Loss: 0.1166  Pearson:0.9694 Spearman:0.9684\n",
      "Train ALL Pearson: 0.952001949622337\n",
      "Train  ALL Spearman: 0.9377698652050329\n",
      "32.074774503707886\n",
      "Test Epoch:117 [(0%)]\t Loss: 0.1479  Pearson:0.9436 Spearman:0.9177\n",
      "Test : Loss:0.1562 \n",
      "pearson： 0.9359470081370357 ALL Pearson: 0.9355361929284794\n",
      "spearman： 0.9166591454260454 ALL Spearman: 0.9170654829029524\n",
      "time: 36.665303230285645\n",
      "0.009\n",
      "Train Epoch:118 [(0%)]\t Loss: 0.1297  Pearson:0.9646 Spearman:0.9445\n",
      "Train ALL Pearson: 0.9615672486781169\n",
      "Train  ALL Spearman: 0.9499957747874839\n",
      "31.914010047912598\n",
      "Test Epoch:118 [(0%)]\t Loss: 0.1766  Pearson:0.9288 Spearman:0.9309\n",
      "Test : Loss:0.1650 \n",
      "pearson： 0.9370234513309604 ALL Pearson: 0.9382978772488287\n",
      "spearman： 0.9245389295844251 ALL Spearman: 0.9209847784605302\n",
      "time: 36.70305037498474\n",
      "0.009\n",
      "Train Epoch:119 [(0%)]\t Loss: 0.1130  Pearson:0.9626 Spearman:0.9473\n",
      "Train ALL Pearson: 0.9636097558771671\n",
      "Train  ALL Spearman: 0.9524049385711479\n",
      "32.28266477584839\n",
      "Test Epoch:119 [(0%)]\t Loss: 0.1622  Pearson:0.9455 Spearman:0.9198\n",
      "Test : Loss:0.1637 \n",
      "pearson： 0.9379662630144295 ALL Pearson: 0.9382754873782347\n",
      "spearman： 0.9172251392886177 ALL Spearman: 0.9207240187290099\n",
      "time: 37.0031521320343\n",
      "0.009\n",
      "Train Epoch:120 [(0%)]\t Loss: 0.1214  Pearson:0.9564 Spearman:0.9409\n",
      "Train ALL Pearson: 0.9631891526447233\n",
      "Train  ALL Spearman: 0.9527171736334534\n",
      "32.1643271446228\n",
      "Test Epoch:120 [(0%)]\t Loss: 0.1565  Pearson:0.9460 Spearman:0.9415\n",
      "Test : Loss:0.1610 \n",
      "pearson： 0.9418470941109983 ALL Pearson: 0.9384546868878663\n",
      "spearman： 0.9272918599253352 ALL Spearman: 0.9214595286759351\n",
      "time: 36.87281799316406\n",
      "0.009\n",
      "Train Epoch:121 [(0%)]\t Loss: 0.1112  Pearson:0.9699 Spearman:0.9594\n",
      "Train ALL Pearson: 0.963757320451363\n",
      "Train  ALL Spearman: 0.953257272018602\n",
      "31.89177632331848\n",
      "Test Epoch:121 [(0%)]\t Loss: 0.1499  Pearson:0.9463 Spearman:0.8856\n",
      "Test : Loss:0.1610 \n",
      "pearson： 0.9354834864297499 ALL Pearson: 0.9382403166371189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9128902579347923 ALL Spearman: 0.9212968216750697\n",
      "time: 36.45631146430969\n",
      "0.009\n",
      "Train Epoch:122 [(0%)]\t Loss: 0.1085  Pearson:0.9687 Spearman:0.9607\n",
      "Train ALL Pearson: 0.9637991955213371\n",
      "Train  ALL Spearman: 0.9522618042229163\n",
      "32.36024355888367\n",
      "Test Epoch:122 [(0%)]\t Loss: 0.1636  Pearson:0.9405 Spearman:0.9153\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.9385572151674629 ALL Pearson: 0.9384295597390552\n",
      "spearman： 0.9190514841061287 ALL Spearman: 0.9215728011486547\n",
      "time: 36.98975920677185\n",
      "0.009\n",
      "Train Epoch:123 [(0%)]\t Loss: 0.1004  Pearson:0.9709 Spearman:0.9616\n",
      "Train ALL Pearson: 0.9650543370509653\n",
      "Train  ALL Spearman: 0.9536095546722149\n",
      "31.63287854194641\n",
      "Test Epoch:123 [(0%)]\t Loss: 0.1697  Pearson:0.9348 Spearman:0.9126\n",
      "Test : Loss:0.1583 \n",
      "pearson： 0.9388101997581478 ALL Pearson: 0.9384827224904513\n",
      "spearman： 0.9210639631897337 ALL Spearman: 0.9218736273846908\n",
      "time: 36.08644986152649\n",
      "0.009\n",
      "Train Epoch:124 [(0%)]\t Loss: 0.1143  Pearson:0.9538 Spearman:0.9307\n",
      "Train ALL Pearson: 0.9651198630889167\n",
      "Train  ALL Spearman: 0.9547611525641597\n",
      "32.17687726020813\n",
      "Test Epoch:124 [(0%)]\t Loss: 0.1543  Pearson:0.9486 Spearman:0.9454\n",
      "Test : Loss:0.1604 \n",
      "pearson： 0.9382278818510749 ALL Pearson: 0.9383907758726667\n",
      "spearman： 0.9248364290221424 ALL Spearman: 0.9217184637980688\n",
      "time: 36.76940441131592\n",
      "0.009\n",
      "Train Epoch:125 [(0%)]\t Loss: 0.1004  Pearson:0.9716 Spearman:0.9599\n",
      "Train ALL Pearson: 0.9662406584608315\n",
      "Train  ALL Spearman: 0.9562330541401973\n",
      "31.83661413192749\n",
      "Test Epoch:125 [(0%)]\t Loss: 0.1532  Pearson:0.9421 Spearman:0.9162\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.9377265645711336 ALL Pearson: 0.9382542343348673\n",
      "spearman： 0.9166552137842389 ALL Spearman: 0.92165605027025\n",
      "time: 36.69305682182312\n",
      "0.009\n",
      "Train Epoch:126 [(0%)]\t Loss: 0.1002  Pearson:0.9650 Spearman:0.9526\n",
      "Train ALL Pearson: 0.9645826509801869\n",
      "Train  ALL Spearman: 0.953930566842224\n",
      "32.02029991149902\n",
      "Test Epoch:126 [(0%)]\t Loss: 0.1572  Pearson:0.9323 Spearman:0.9261\n",
      "Test : Loss:0.1571 \n",
      "pearson： 0.9374417634014081 ALL Pearson: 0.9383562052757785\n",
      "spearman： 0.9219745765459514 ALL Spearman: 0.9217731109365929\n",
      "time: 36.85674834251404\n",
      "0.009\n",
      "Train Epoch:127 [(0%)]\t Loss: 0.0956  Pearson:0.9706 Spearman:0.9568\n",
      "Train ALL Pearson: 0.9659370491671739\n",
      "Train  ALL Spearman: 0.9549666996661674\n",
      "32.08531355857849\n",
      "Test Epoch:127 [(0%)]\t Loss: 0.1440  Pearson:0.9461 Spearman:0.9272\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9396163791122739 ALL Pearson: 0.9385331022079818\n",
      "spearman： 0.9199241249967021 ALL Spearman: 0.9218955336493743\n",
      "time: 36.84078860282898\n",
      "0.009\n",
      "Train Epoch:128 [(0%)]\t Loss: 0.1068  Pearson:0.9589 Spearman:0.9593\n",
      "Train ALL Pearson: 0.965719825429398\n",
      "Train  ALL Spearman: 0.9546670079568744\n",
      "31.703835248947144\n",
      "Test Epoch:128 [(0%)]\t Loss: 0.1659  Pearson:0.9394 Spearman:0.9121\n",
      "Test : Loss:0.1628 \n",
      "pearson： 0.9384975399038269 ALL Pearson: 0.9388424806315787\n",
      "spearman： 0.9189971083133625 ALL Spearman: 0.9225402587040316\n",
      "time: 36.4806649684906\n",
      "0.009\n",
      "Train Epoch:129 [(0%)]\t Loss: 0.0996  Pearson:0.9690 Spearman:0.9505\n",
      "Train ALL Pearson: 0.9663489492574328\n",
      "Train  ALL Spearman: 0.9566891237967748\n",
      "32.09024524688721\n",
      "Test Epoch:129 [(0%)]\t Loss: 0.1492  Pearson:0.9332 Spearman:0.9193\n",
      "Test : Loss:0.1607 \n",
      "pearson： 0.9371417532764162 ALL Pearson: 0.9387610147903834\n",
      "spearman： 0.9216304470139701 ALL Spearman: 0.9227921802173391\n",
      "time: 36.5808048248291\n",
      "0.009\n",
      "Train Epoch:130 [(0%)]\t Loss: 0.1091  Pearson:0.9600 Spearman:0.9510\n",
      "Train ALL Pearson: 0.9663510084742192\n",
      "Train  ALL Spearman: 0.9561868728985634\n",
      "32.476706981658936\n",
      "Test Epoch:130 [(0%)]\t Loss: 0.1690  Pearson:0.9372 Spearman:0.9330\n",
      "Test : Loss:0.1662 \n",
      "pearson： 0.9400255274306164 ALL Pearson: 0.9385663701306936\n",
      "spearman： 0.925829696211579 ALL Spearman: 0.9222590132843678\n",
      "time: 37.351144313812256\n",
      "0.009\n",
      "Train Epoch:131 [(0%)]\t Loss: 0.1061  Pearson:0.9681 Spearman:0.9603\n",
      "Train ALL Pearson: 0.9668333673858028\n",
      "Train  ALL Spearman: 0.9564216527023766\n",
      "31.69937491416931\n",
      "Test Epoch:131 [(0%)]\t Loss: 0.1653  Pearson:0.9392 Spearman:0.9122\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9355657907603825 ALL Pearson: 0.9382168678451733\n",
      "spearman： 0.9191713679256757 ALL Spearman: 0.9221656941835749\n",
      "time: 36.4930522441864\n",
      "0.009\n",
      "Train Epoch:132 [(0%)]\t Loss: 0.1054  Pearson:0.9725 Spearman:0.9549\n",
      "Train ALL Pearson: 0.9668061170281603\n",
      "Train  ALL Spearman: 0.9558347403717863\n",
      "32.42813181877136\n",
      "Test Epoch:132 [(0%)]\t Loss: 0.1525  Pearson:0.9453 Spearman:0.9170\n",
      "Test : Loss:0.1545 \n",
      "pearson： 0.9365520179120804 ALL Pearson: 0.9385163785399392\n",
      "spearman： 0.9178203523594299 ALL Spearman: 0.9228277272669451\n",
      "time: 37.06820249557495\n",
      "0.009\n",
      "Train Epoch:133 [(0%)]\t Loss: 0.1072  Pearson:0.9721 Spearman:0.9504\n",
      "Train ALL Pearson: 0.9678855998778483\n",
      "Train  ALL Spearman: 0.9576059809891359\n",
      "31.63769006729126\n",
      "Test Epoch:133 [(0%)]\t Loss: 0.1754  Pearson:0.9398 Spearman:0.9426\n",
      "Test : Loss:0.1605 \n",
      "pearson： 0.9411993017556757 ALL Pearson: 0.9389207304374517\n",
      "spearman： 0.9300542833875649 ALL Spearman: 0.922705600442499\n",
      "time: 36.380367279052734\n",
      "0.009\n",
      "Train Epoch:134 [(0%)]\t Loss: 0.1013  Pearson:0.9723 Spearman:0.9457\n",
      "Train ALL Pearson: 0.9664303936818704\n",
      "Train  ALL Spearman: 0.9556902790756145\n",
      "31.791678428649902\n",
      "Test Epoch:134 [(0%)]\t Loss: 0.1735  Pearson:0.9253 Spearman:0.9004\n",
      "Test : Loss:0.1564 \n",
      "pearson： 0.9361515557620931 ALL Pearson: 0.9388078764823324\n",
      "spearman： 0.9177806799242222 ALL Spearman: 0.9229981494641274\n",
      "time: 36.47717547416687\n",
      "0.009\n",
      "Train Epoch:135 [(0%)]\t Loss: 0.0983  Pearson:0.9743 Spearman:0.9583\n",
      "Train ALL Pearson: 0.9676104858798663\n",
      "Train  ALL Spearman: 0.9571545446123709\n",
      "32.18999695777893\n",
      "Test Epoch:135 [(0%)]\t Loss: 0.1581  Pearson:0.9463 Spearman:0.9328\n",
      "Test : Loss:0.1562 \n",
      "pearson： 0.9416101961809823 ALL Pearson: 0.9386459140095552\n",
      "spearman： 0.9275920669668264 ALL Spearman: 0.9225250129839434\n",
      "time: 36.961467027664185\n",
      "0.009\n",
      "Train Epoch:136 [(0%)]\t Loss: 0.1024  Pearson:0.9690 Spearman:0.9509\n",
      "Train ALL Pearson: 0.9679884076894136\n",
      "Train  ALL Spearman: 0.9577654628870876\n",
      "32.097259521484375\n",
      "Test Epoch:136 [(0%)]\t Loss: 0.1633  Pearson:0.9398 Spearman:0.9340\n",
      "Test : Loss:0.1630 \n",
      "pearson： 0.9404147999062602 ALL Pearson: 0.9379552122632939\n",
      "spearman： 0.9288611477362029 ALL Spearman: 0.9218162876866681\n",
      "time: 36.86373162269592\n",
      "0.009\n",
      "Train Epoch:137 [(0%)]\t Loss: 0.1067  Pearson:0.9524 Spearman:0.9485\n",
      "Train ALL Pearson: 0.9681691674359959\n",
      "Train  ALL Spearman: 0.9577604560536919\n",
      "32.064045667648315\n",
      "Test Epoch:137 [(0%)]\t Loss: 0.1637  Pearson:0.9406 Spearman:0.9359\n",
      "Test : Loss:0.1600 \n",
      "pearson： 0.9383483841921741 ALL Pearson: 0.9382885672883924\n",
      "spearman： 0.9244622231159689 ALL Spearman: 0.9220986885962634\n",
      "time: 36.714555740356445\n",
      "0.009\n",
      "Train Epoch:138 [(0%)]\t Loss: 0.0951  Pearson:0.9751 Spearman:0.9650\n",
      "Train ALL Pearson: 0.968099671048387\n",
      "Train  ALL Spearman: 0.9579344428352888\n",
      "31.778373956680298\n",
      "Test Epoch:138 [(0%)]\t Loss: 0.1629  Pearson:0.9496 Spearman:0.9181\n",
      "Test : Loss:0.1632 \n",
      "pearson： 0.9413510197235456 ALL Pearson: 0.9383008542596797\n",
      "spearman： 0.9225735994848692 ALL Spearman: 0.9220560596776975\n",
      "time: 36.598084926605225\n",
      "0.009\n",
      "Train Epoch:139 [(0%)]\t Loss: 0.0976  Pearson:0.9755 Spearman:0.9610\n",
      "Train ALL Pearson: 0.9683837720650361\n",
      "Train  ALL Spearman: 0.9588942385989979\n",
      "31.434481620788574\n",
      "Test Epoch:139 [(0%)]\t Loss: 0.1731  Pearson:0.9211 Spearman:0.9012\n",
      "Test : Loss:0.1679 \n",
      "pearson： 0.9372397804650273 ALL Pearson: 0.9387322453247455\n",
      "spearman： 0.9181870672048486 ALL Spearman: 0.9222637061542889\n",
      "time: 35.946035623550415\n",
      "0.009\n",
      "Train Epoch:140 [(0%)]\t Loss: 0.1058  Pearson:0.9722 Spearman:0.9579\n",
      "Train ALL Pearson: 0.9681934738329114\n",
      "Train  ALL Spearman: 0.9584352414313936\n",
      "32.34271001815796\n",
      "Test Epoch:140 [(0%)]\t Loss: 0.1569  Pearson:0.9392 Spearman:0.9217\n",
      "Test : Loss:0.1614 \n",
      "pearson： 0.9410866977540582 ALL Pearson: 0.9384748164192911\n",
      "spearman： 0.9243726821256971 ALL Spearman: 0.9228646509189893\n",
      "time: 37.02220821380615\n",
      "0.009\n",
      "Train Epoch:141 [(0%)]\t Loss: 0.1138  Pearson:0.9651 Spearman:0.9559\n",
      "Train ALL Pearson: 0.9684076739239479\n",
      "Train  ALL Spearman: 0.9587142717492311\n",
      "31.95040011405945\n",
      "Test Epoch:141 [(0%)]\t Loss: 0.1592  Pearson:0.9389 Spearman:0.9211\n",
      "Test : Loss:0.1563 \n",
      "pearson： 0.9343421368818913 ALL Pearson: 0.9384275876610692\n",
      "spearman： 0.9183203720568502 ALL Spearman: 0.9226648460317352\n",
      "time: 36.84183216094971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009\n",
      "Train Epoch:142 [(0%)]\t Loss: 0.0915  Pearson:0.9806 Spearman:0.9655\n",
      "Train ALL Pearson: 0.9682551958295527\n",
      "Train  ALL Spearman: 0.9580779245417982\n",
      "31.57120704650879\n",
      "Test Epoch:142 [(0%)]\t Loss: 0.1639  Pearson:0.9525 Spearman:0.9374\n",
      "Test : Loss:0.1603 \n",
      "pearson： 0.9405789253285731 ALL Pearson: 0.9384471611100821\n",
      "spearman： 0.9280378258677424 ALL Spearman: 0.9224403814210619\n",
      "time: 36.269700050354004\n",
      "0.009\n",
      "Train Epoch:143 [(0%)]\t Loss: 0.1160  Pearson:0.9540 Spearman:0.9470\n",
      "Train ALL Pearson: 0.9684914294180355\n",
      "Train  ALL Spearman: 0.9582165154349128\n",
      "31.661354064941406\n",
      "Test Epoch:143 [(0%)]\t Loss: 0.1812  Pearson:0.9199 Spearman:0.9044\n",
      "Test : Loss:0.1652 \n",
      "pearson： 0.9343651576029152 ALL Pearson: 0.9384973890877606\n",
      "spearman： 0.9163257970703477 ALL Spearman: 0.9229459448614612\n",
      "time: 36.20589590072632\n",
      "0.009\n",
      "Train Epoch:144 [(0%)]\t Loss: 0.0980  Pearson:0.9672 Spearman:0.9605\n",
      "Train ALL Pearson: 0.967662393471545\n",
      "Train  ALL Spearman: 0.9572108318549706\n",
      "32.3352313041687\n",
      "Test Epoch:144 [(0%)]\t Loss: 0.1483  Pearson:0.9445 Spearman:0.9106\n",
      "Test : Loss:0.1634 \n",
      "pearson： 0.9378244054264038 ALL Pearson: 0.9377029230389434\n",
      "spearman： 0.9181280311784021 ALL Spearman: 0.9217919110179221\n",
      "time: 36.94875192642212\n",
      "0.009\n",
      "Train Epoch:145 [(0%)]\t Loss: 0.1041  Pearson:0.9591 Spearman:0.9495\n",
      "Train ALL Pearson: 0.9682831702427738\n",
      "Train  ALL Spearman: 0.9582479787268551\n",
      "32.07938289642334\n",
      "Test Epoch:145 [(0%)]\t Loss: 0.1573  Pearson:0.9430 Spearman:0.9267\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9412126213249294 ALL Pearson: 0.9380102246473186\n",
      "spearman： 0.9232301342944546 ALL Spearman: 0.9218846096830293\n",
      "time: 36.81550860404968\n",
      "0.009\n",
      "Train Epoch:146 [(0%)]\t Loss: 0.0934  Pearson:0.9734 Spearman:0.9709\n",
      "Train ALL Pearson: 0.9698463699633184\n",
      "Train  ALL Spearman: 0.9604437745940996\n",
      "31.725306510925293\n",
      "Test Epoch:146 [(0%)]\t Loss: 0.1675  Pearson:0.9488 Spearman:0.9358\n",
      "Test : Loss:0.1620 \n",
      "pearson： 0.9386045695555267 ALL Pearson: 0.9375309952379961\n",
      "spearman： 0.9233812936952993 ALL Spearman: 0.921604697116557\n",
      "time: 36.32949376106262\n",
      "0.009\n",
      "Train Epoch:147 [(0%)]\t Loss: 0.1058  Pearson:0.9631 Spearman:0.9468\n",
      "Train ALL Pearson: 0.9694611756810814\n",
      "Train  ALL Spearman: 0.960181664261405\n",
      "32.164682149887085\n",
      "Test Epoch:147 [(0%)]\t Loss: 0.1509  Pearson:0.9459 Spearman:0.9300\n",
      "Test : Loss:0.1557 \n",
      "pearson： 0.9408181621898624 ALL Pearson: 0.9379915676756857\n",
      "spearman： 0.924349752257677 ALL Spearman: 0.9216954178679\n",
      "time: 36.81219148635864\n",
      "0.009\n",
      "Train Epoch:148 [(0%)]\t Loss: 0.1001  Pearson:0.9768 Spearman:0.9644\n",
      "Train ALL Pearson: 0.9696286832468999\n",
      "Train  ALL Spearman: 0.9608613377349148\n",
      "31.958895921707153\n",
      "Test Epoch:148 [(0%)]\t Loss: 0.1500  Pearson:0.9486 Spearman:0.9246\n",
      "Test : Loss:0.1603 \n",
      "pearson： 0.9420915572504043 ALL Pearson: 0.9388429865106558\n",
      "spearman： 0.925668675691772 ALL Spearman: 0.9230921857779726\n",
      "time: 36.56242060661316\n",
      "0.009\n",
      "Train Epoch:149 [(0%)]\t Loss: 0.1028  Pearson:0.9680 Spearman:0.9621\n",
      "Train ALL Pearson: 0.9696211116113115\n",
      "Train  ALL Spearman: 0.9599571939938819\n",
      "32.083813428878784\n",
      "Test Epoch:149 [(0%)]\t Loss: 0.1687  Pearson:0.9274 Spearman:0.9030\n",
      "Test : Loss:0.1566 \n",
      "pearson： 0.9348893607209559 ALL Pearson: 0.9384077787233294\n",
      "spearman： 0.9168051268481715 ALL Spearman: 0.9228584131652586\n",
      "time: 36.94825291633606\n",
      "0.009\n",
      "Train Epoch:150 [(0%)]\t Loss: 0.0982  Pearson:0.9687 Spearman:0.9553\n",
      "Train ALL Pearson: 0.9700770173957772\n",
      "Train  ALL Spearman: 0.9605283908940769\n",
      "31.962632179260254\n",
      "Test Epoch:150 [(0%)]\t Loss: 0.1547  Pearson:0.9454 Spearman:0.9176\n",
      "Test : Loss:0.1575 \n",
      "pearson： 0.9397609317856755 ALL Pearson: 0.9380438340034477\n",
      "spearman： 0.9228514436690749 ALL Spearman: 0.9221650539182088\n",
      "time: 36.51979374885559\n",
      "0.009\n",
      "Train Epoch:151 [(0%)]\t Loss: 0.0982  Pearson:0.9778 Spearman:0.9651\n",
      "Train ALL Pearson: 0.9697525350748875\n",
      "Train  ALL Spearman: 0.9602497506784049\n",
      "32.52619767189026\n",
      "Test Epoch:151 [(0%)]\t Loss: 0.1597  Pearson:0.9383 Spearman:0.9177\n",
      "Test : Loss:0.1537 \n",
      "pearson： 0.9376706360901473 ALL Pearson: 0.9380938117286504\n",
      "spearman： 0.9207267421307311 ALL Spearman: 0.9219459807462583\n",
      "time: 37.185720443725586\n",
      "0.009\n",
      "Train Epoch:152 [(0%)]\t Loss: 0.0933  Pearson:0.9697 Spearman:0.9477\n",
      "Train ALL Pearson: 0.9699194036080048\n",
      "Train  ALL Spearman: 0.9598451645480203\n",
      "32.1591432094574\n",
      "Test Epoch:152 [(0%)]\t Loss: 0.1639  Pearson:0.9422 Spearman:0.9139\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.9393053734904178 ALL Pearson: 0.9380824292196988\n",
      "spearman： 0.9184057740313855 ALL Spearman: 0.922380048055702\n",
      "time: 36.834643602371216\n",
      "0.009\n",
      "Train Epoch:153 [(0%)]\t Loss: 0.1068  Pearson:0.9679 Spearman:0.9557\n",
      "Train ALL Pearson: 0.9703630443590255\n",
      "Train  ALL Spearman: 0.9611097317692006\n",
      "32.04131364822388\n",
      "Test Epoch:153 [(0%)]\t Loss: 0.1562  Pearson:0.9309 Spearman:0.9234\n",
      "Test : Loss:0.1567 \n",
      "pearson： 0.9375406653941947 ALL Pearson: 0.9385784965065384\n",
      "spearman： 0.9235474543666982 ALL Spearman: 0.9230909618024393\n",
      "time: 36.71081519126892\n",
      "0.009\n",
      "Train Epoch:154 [(0%)]\t Loss: 0.1071  Pearson:0.9701 Spearman:0.9498\n",
      "Train ALL Pearson: 0.970351361566754\n",
      "Train  ALL Spearman: 0.9599975180631161\n",
      "32.42829203605652\n",
      "Test Epoch:154 [(0%)]\t Loss: 0.1588  Pearson:0.9265 Spearman:0.9153\n",
      "Test : Loss:0.1537 \n",
      "pearson： 0.9370565375603135 ALL Pearson: 0.9386568498496055\n",
      "spearman： 0.923484908983075 ALL Spearman: 0.9228647565250634\n",
      "time: 37.2127583026886\n",
      "0.0026999999999999997\n",
      "Train Epoch:155 [(0%)]\t Loss: 0.1137  Pearson:0.9579 Spearman:0.9489\n",
      "Train ALL Pearson: 0.9679944304409946\n",
      "Train  ALL Spearman: 0.9575870319645138\n",
      "31.85498046875\n",
      "Test Epoch:155 [(0%)]\t Loss: 0.1533  Pearson:0.9512 Spearman:0.9463\n",
      "Test : Loss:0.1604 \n",
      "pearson： 0.9412903898673733 ALL Pearson: 0.9387540803245377\n",
      "spearman： 0.9256997344785638 ALL Spearman: 0.9229189119065996\n",
      "time: 36.620903730392456\n",
      "0.0026999999999999997\n",
      "Train Epoch:156 [(0%)]\t Loss: 0.1013  Pearson:0.9705 Spearman:0.9666\n",
      "Train ALL Pearson: 0.9675602657607\n",
      "Train  ALL Spearman: 0.9569443073117897\n",
      "32.42671346664429\n",
      "Test Epoch:156 [(0%)]\t Loss: 0.1608  Pearson:0.9421 Spearman:0.9281\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.9392822106876519 ALL Pearson: 0.9389154960396199\n",
      "spearman： 0.9239587172385023 ALL Spearman: 0.9230713542746678\n",
      "time: 37.042837619781494\n",
      "0.0026999999999999997\n",
      "Train Epoch:157 [(0%)]\t Loss: 0.1015  Pearson:0.9758 Spearman:0.9657\n",
      "Train ALL Pearson: 0.9682905107338556\n",
      "Train  ALL Spearman: 0.9580773980731089\n",
      "32.059775590896606\n",
      "Test Epoch:157 [(0%)]\t Loss: 0.1425  Pearson:0.9503 Spearman:0.9410\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.9367810872754393 ALL Pearson: 0.9388211245331385\n",
      "spearman： 0.9246156875239392 ALL Spearman: 0.9227950682501176\n",
      "time: 36.71628260612488\n",
      "0.0026999999999999997\n",
      "Train Epoch:158 [(0%)]\t Loss: 0.0974  Pearson:0.9732 Spearman:0.9601\n",
      "Train ALL Pearson: 0.9679208153145817\n",
      "Train  ALL Spearman: 0.957967622746978\n",
      "31.772921323776245\n",
      "Test Epoch:158 [(0%)]\t Loss: 0.1514  Pearson:0.9540 Spearman:0.9322\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.942772393769124 ALL Pearson: 0.93871966556541\n",
      "spearman： 0.927013857181718 ALL Spearman: 0.9229256120253054\n",
      "time: 36.40343761444092\n",
      "0.0026999999999999997\n",
      "Train Epoch:159 [(0%)]\t Loss: 0.1158  Pearson:0.9589 Spearman:0.9404\n",
      "Train ALL Pearson: 0.9680870729856266\n",
      "Train  ALL Spearman: 0.9577866452023643\n",
      "31.718639850616455\n",
      "Test Epoch:159 [(0%)]\t Loss: 0.1536  Pearson:0.9345 Spearman:0.9053\n",
      "Test : Loss:0.1604 \n",
      "pearson： 0.9375971674048312 ALL Pearson: 0.9385309180324366\n",
      "spearman： 0.9207470234496019 ALL Spearman: 0.9227636475095491\n",
      "time: 36.387924671173096\n",
      "0.0026999999999999997\n",
      "Train Epoch:160 [(0%)]\t Loss: 0.1065  Pearson:0.9539 Spearman:0.9482\n",
      "Train ALL Pearson: 0.9681470266565972\n",
      "Train  ALL Spearman: 0.9580303322210397\n",
      "31.94734764099121\n",
      "Test Epoch:160 [(0%)]\t Loss: 0.1674  Pearson:0.9322 Spearman:0.9285\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.9380068909388797 ALL Pearson: 0.9387905904784912\n",
      "spearman： 0.9220007006254126 ALL Spearman: 0.9227812184535151\n",
      "time: 36.676830530166626\n",
      "0.0026999999999999997\n",
      "Train Epoch:161 [(0%)]\t Loss: 0.1065  Pearson:0.9620 Spearman:0.9525\n",
      "Train ALL Pearson: 0.9684108770543391\n",
      "Train  ALL Spearman: 0.9586405776097593\n",
      "31.683826446533203\n",
      "Test Epoch:161 [(0%)]\t Loss: 0.1540  Pearson:0.9463 Spearman:0.9266\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9394516416053363 ALL Pearson: 0.9387774121795781\n",
      "spearman： 0.9233693807211057 ALL Spearman: 0.922632989327781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36.300156354904175\n",
      "0.0026999999999999997\n",
      "Train Epoch:162 [(0%)]\t Loss: 0.1251  Pearson:0.9432 Spearman:0.9396\n",
      "Train ALL Pearson: 0.9680157183794679\n",
      "Train  ALL Spearman: 0.9582926496282596\n",
      "32.29046702384949\n",
      "Test Epoch:162 [(0%)]\t Loss: 0.1652  Pearson:0.9364 Spearman:0.9022\n",
      "Test : Loss:0.1612 \n",
      "pearson： 0.9377716197289903 ALL Pearson: 0.9384685104557253\n",
      "spearman： 0.9182749960350395 ALL Spearman: 0.9223418135516713\n",
      "time: 36.79968214035034\n",
      "0.0026999999999999997\n",
      "Train Epoch:163 [(0%)]\t Loss: 0.1147  Pearson:0.9515 Spearman:0.9375\n",
      "Train ALL Pearson: 0.967840704810757\n",
      "Train  ALL Spearman: 0.9576968185015949\n",
      "32.084877490997314\n",
      "Test Epoch:163 [(0%)]\t Loss: 0.1525  Pearson:0.9507 Spearman:0.9313\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.9426390148901148 ALL Pearson: 0.9388380562719615\n",
      "spearman： 0.9259774053396111 ALL Spearman: 0.9229335559488847\n",
      "time: 36.85216951370239\n",
      "0.0026999999999999997\n",
      "Train Epoch:164 [(0%)]\t Loss: 0.0977  Pearson:0.9773 Spearman:0.9624\n",
      "Train ALL Pearson: 0.9681544574720868\n",
      "Train  ALL Spearman: 0.958809794655418\n",
      "32.28969979286194\n",
      "Test Epoch:164 [(0%)]\t Loss: 0.1487  Pearson:0.9497 Spearman:0.9402\n",
      "Test : Loss:0.1608 \n",
      "pearson： 0.9427539671445466 ALL Pearson: 0.9387532529802652\n",
      "spearman： 0.9291800575940024 ALL Spearman: 0.9227346483749316\n",
      "time: 36.83965349197388\n",
      "0.0026999999999999997\n",
      "Train Epoch:165 [(0%)]\t Loss: 0.1066  Pearson:0.9607 Spearman:0.9522\n",
      "Train ALL Pearson: 0.9682777597233895\n",
      "Train  ALL Spearman: 0.9581305432727067\n",
      "31.989922761917114\n",
      "Test Epoch:165 [(0%)]\t Loss: 0.1589  Pearson:0.9501 Spearman:0.9387\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.9406076564991703 ALL Pearson: 0.9387262097467777\n",
      "spearman： 0.9241858438658815 ALL Spearman: 0.9227906731306559\n",
      "time: 36.62243676185608\n",
      "0.0026999999999999997\n",
      "Train Epoch:166 [(0%)]\t Loss: 0.0957  Pearson:0.9760 Spearman:0.9644\n",
      "Train ALL Pearson: 0.9674508554187665\n",
      "Train  ALL Spearman: 0.9568986033703583\n",
      "31.54952073097229\n",
      "Test Epoch:166 [(0%)]\t Loss: 0.1481  Pearson:0.9271 Spearman:0.9125\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.9336469309273191 ALL Pearson: 0.9386654879683545\n",
      "spearman： 0.9171124124172466 ALL Spearman: 0.9228154289545429\n",
      "time: 36.28400254249573\n",
      "0.0026999999999999997\n",
      "Train Epoch:167 [(0%)]\t Loss: 0.1011  Pearson:0.9718 Spearman:0.9651\n",
      "Train ALL Pearson: 0.9689456503037466\n",
      "Train  ALL Spearman: 0.9592088608428597\n",
      "31.953289270401\n",
      "Test Epoch:167 [(0%)]\t Loss: 0.1504  Pearson:0.9526 Spearman:0.9335\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9452497994029393 ALL Pearson: 0.938934524481976\n",
      "spearman： 0.9290322883122305 ALL Spearman: 0.9230770089383702\n",
      "time: 36.484384536743164\n",
      "0.0026999999999999997\n",
      "Train Epoch:168 [(0%)]\t Loss: 0.0933  Pearson:0.9721 Spearman:0.9546\n",
      "Train ALL Pearson: 0.9683702805341224\n",
      "Train  ALL Spearman: 0.958326352584854\n",
      "31.820253372192383\n",
      "Test Epoch:168 [(0%)]\t Loss: 0.1632  Pearson:0.9382 Spearman:0.9051\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.9411519113300926 ALL Pearson: 0.9389080204391077\n",
      "spearman： 0.9232334181705629 ALL Spearman: 0.9230793495345334\n",
      "time: 36.70468616485596\n",
      "0.0026999999999999997\n",
      "Train Epoch:169 [(0%)]\t Loss: 0.1028  Pearson:0.9636 Spearman:0.9534\n",
      "Train ALL Pearson: 0.9690024629322107\n",
      "Train  ALL Spearman: 0.95939899457848\n",
      "31.613046169281006\n",
      "Test Epoch:169 [(0%)]\t Loss: 0.1609  Pearson:0.9410 Spearman:0.9168\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.9412363999255512 ALL Pearson: 0.9386685617524143\n",
      "spearman： 0.9258297919099232 ALL Spearman: 0.9226940428394121\n",
      "time: 36.36552166938782\n",
      "0.0026999999999999997\n",
      "Train Epoch:170 [(0%)]\t Loss: 0.1084  Pearson:0.9615 Spearman:0.9442\n",
      "Train ALL Pearson: 0.9690143416352653\n",
      "Train  ALL Spearman: 0.9591399655405608\n",
      "31.365478515625\n",
      "Test Epoch:170 [(0%)]\t Loss: 0.1748  Pearson:0.9267 Spearman:0.9172\n",
      "Test : Loss:0.1604 \n",
      "pearson： 0.9359981284095227 ALL Pearson: 0.9386626012167449\n",
      "spearman： 0.9213640279308152 ALL Spearman: 0.9229284915960042\n",
      "time: 35.88103151321411\n",
      "0.0026999999999999997\n",
      "Train Epoch:171 [(0%)]\t Loss: 0.0975  Pearson:0.9757 Spearman:0.9629\n",
      "Train ALL Pearson: 0.9689750930300532\n",
      "Train  ALL Spearman: 0.9592553396906329\n",
      "31.926581621170044\n",
      "Test Epoch:171 [(0%)]\t Loss: 0.1469  Pearson:0.9474 Spearman:0.9379\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.938996448171107 ALL Pearson: 0.9386206899071811\n",
      "spearman： 0.9228000637611932 ALL Spearman: 0.9228068587282737\n",
      "time: 36.691009521484375\n",
      "0.0026999999999999997\n",
      "Train Epoch:172 [(0%)]\t Loss: 0.0954  Pearson:0.9738 Spearman:0.9565\n",
      "Train ALL Pearson: 0.9691754318436643\n",
      "Train  ALL Spearman: 0.9594940260772391\n",
      "31.684651136398315\n",
      "Test Epoch:172 [(0%)]\t Loss: 0.1539  Pearson:0.9493 Spearman:0.9382\n",
      "Test : Loss:0.1599 \n",
      "pearson： 0.9408388006708129 ALL Pearson: 0.938596747929242\n",
      "spearman： 0.9270485996631173 ALL Spearman: 0.9226999739855453\n",
      "time: 36.26218295097351\n",
      "0.0026999999999999997\n",
      "Train Epoch:173 [(0%)]\t Loss: 0.1050  Pearson:0.9671 Spearman:0.9503\n",
      "Train ALL Pearson: 0.9691315880567296\n",
      "Train  ALL Spearman: 0.959743956160419\n",
      "32.10438537597656\n",
      "Test Epoch:173 [(0%)]\t Loss: 0.1534  Pearson:0.9350 Spearman:0.9208\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.938799762225883 ALL Pearson: 0.9384561544313224\n",
      "spearman： 0.9263383489821588 ALL Spearman: 0.9226000064973653\n",
      "time: 36.697911739349365\n",
      "0.0026999999999999997\n",
      "Train Epoch:174 [(0%)]\t Loss: 0.1022  Pearson:0.9639 Spearman:0.9493\n",
      "Train ALL Pearson: 0.9691988650754003\n",
      "Train  ALL Spearman: 0.959486251882742\n",
      "31.79845356941223\n",
      "Test Epoch:174 [(0%)]\t Loss: 0.1624  Pearson:0.9385 Spearman:0.9293\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.9395566695918629 ALL Pearson: 0.9385159068527671\n",
      "spearman： 0.9251909655189028 ALL Spearman: 0.9225045840755952\n",
      "time: 36.47636008262634\n",
      "0.0026999999999999997\n",
      "Train Epoch:175 [(0%)]\t Loss: 0.0960  Pearson:0.9747 Spearman:0.9685\n",
      "Train ALL Pearson: 0.9693881519970887\n",
      "Train  ALL Spearman: 0.9597307897863103\n",
      "31.830421209335327\n",
      "Test Epoch:175 [(0%)]\t Loss: 0.1468  Pearson:0.9508 Spearman:0.9357\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.942122984734376 ALL Pearson: 0.9384508531892789\n",
      "spearman： 0.9263882954979732 ALL Spearman: 0.9227013076572645\n",
      "time: 36.58089733123779\n",
      "0.0026999999999999997\n",
      "Train Epoch:176 [(0%)]\t Loss: 0.1005  Pearson:0.9715 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9694798903989492\n",
      "Train  ALL Spearman: 0.9597725589924627\n",
      "31.891399145126343\n",
      "Test Epoch:176 [(0%)]\t Loss: 0.1526  Pearson:0.9487 Spearman:0.9275\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.941938010329946 ALL Pearson: 0.9384577711305576\n",
      "spearman： 0.9224129203768008 ALL Spearman: 0.9224012639044483\n",
      "time: 36.47392988204956\n",
      "0.0026999999999999997\n",
      "Train Epoch:177 [(0%)]\t Loss: 0.0922  Pearson:0.9758 Spearman:0.9612\n",
      "Train ALL Pearson: 0.9706865352369503\n",
      "Train  ALL Spearman: 0.9617690028352883\n",
      "32.01928186416626\n",
      "Test Epoch:177 [(0%)]\t Loss: 0.1601  Pearson:0.9351 Spearman:0.9250\n",
      "Test : Loss:0.1589 \n",
      "pearson： 0.9384418449696845 ALL Pearson: 0.9384888432499193\n",
      "spearman： 0.9207309782229551 ALL Spearman: 0.9226548717864002\n",
      "time: 36.65679597854614\n",
      "0.0026999999999999997\n",
      "Train Epoch:178 [(0%)]\t Loss: 0.0798  Pearson:0.9818 Spearman:0.9747\n",
      "Train ALL Pearson: 0.9699831782564191\n",
      "Train  ALL Spearman: 0.9604142540329932\n",
      "31.730058908462524\n",
      "Test Epoch:178 [(0%)]\t Loss: 0.1625  Pearson:0.9369 Spearman:0.9116\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9386620110335144 ALL Pearson: 0.9382744477014358\n",
      "spearman： 0.920601307731409 ALL Spearman: 0.9222567816226753\n",
      "time: 36.26060652732849\n",
      "0.0026999999999999997\n",
      "Train Epoch:179 [(0%)]\t Loss: 0.1006  Pearson:0.9709 Spearman:0.9524\n",
      "Train ALL Pearson: 0.9694637213328774\n",
      "Train  ALL Spearman: 0.9598007234722783\n",
      "32.22115635871887\n",
      "Test Epoch:179 [(0%)]\t Loss: 0.1592  Pearson:0.9365 Spearman:0.9296\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9395615060151035 ALL Pearson: 0.9386643020997761\n",
      "spearman： 0.9260532858386438 ALL Spearman: 0.9225680511260465\n",
      "time: 36.8911075592041\n",
      "0.0026999999999999997\n",
      "Train Epoch:180 [(0%)]\t Loss: 0.1074  Pearson:0.9560 Spearman:0.9365\n",
      "Train ALL Pearson: 0.9692895266468448\n",
      "Train  ALL Spearman: 0.9593201297578429\n",
      "31.56913709640503\n",
      "Test Epoch:180 [(0%)]\t Loss: 0.1666  Pearson:0.9355 Spearman:0.9207\n",
      "Test : Loss:0.1564 \n",
      "pearson： 0.9403645491464722 ALL Pearson: 0.9387929510451023\n",
      "spearman： 0.9241924118733201 ALL Spearman: 0.9228723960978041\n",
      "time: 36.24063730239868\n",
      "0.0026999999999999997\n",
      "Train Epoch:181 [(0%)]\t Loss: 0.0899  Pearson:0.9688 Spearman:0.9594\n",
      "Train ALL Pearson: 0.9694929781030903\n",
      "Train  ALL Spearman: 0.9594005338752138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.277605056762695\n",
      "Test Epoch:181 [(0%)]\t Loss: 0.1543  Pearson:0.9467 Spearman:0.9285\n",
      "Test : Loss:0.1564 \n",
      "pearson： 0.9418897486885162 ALL Pearson: 0.9388315259826787\n",
      "spearman： 0.9255782148229261 ALL Spearman: 0.9229123386618578\n",
      "time: 36.06510138511658\n",
      "0.0026999999999999997\n",
      "Train Epoch:182 [(0%)]\t Loss: 0.0958  Pearson:0.9670 Spearman:0.9536\n",
      "Train ALL Pearson: 0.9690664234000974\n",
      "Train  ALL Spearman: 0.9594477814091296\n",
      "32.13429546356201\n",
      "Test Epoch:182 [(0%)]\t Loss: 0.1735  Pearson:0.9327 Spearman:0.9277\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9376192282736704 ALL Pearson: 0.9387134093579939\n",
      "spearman： 0.9219082735723952 ALL Spearman: 0.9228246749580501\n",
      "time: 36.94860219955444\n",
      "0.0026999999999999997\n",
      "Train Epoch:183 [(0%)]\t Loss: 0.1157  Pearson:0.9629 Spearman:0.9447\n",
      "Train ALL Pearson: 0.969619294864645\n",
      "Train  ALL Spearman: 0.9593946209678059\n",
      "31.880306482315063\n",
      "Test Epoch:183 [(0%)]\t Loss: 0.1521  Pearson:0.9435 Spearman:0.9318\n",
      "Test : Loss:0.1572 \n",
      "pearson： 0.9421768821059253 ALL Pearson: 0.9386871441203413\n",
      "spearman： 0.9273960554855855 ALL Spearman: 0.92276874593613\n",
      "time: 36.404290437698364\n",
      "0.0026999999999999997\n",
      "Train Epoch:184 [(0%)]\t Loss: 0.1036  Pearson:0.9589 Spearman:0.9572\n",
      "Train ALL Pearson: 0.9697895296540003\n",
      "Train  ALL Spearman: 0.9603771953700551\n",
      "31.773709297180176\n",
      "Test Epoch:184 [(0%)]\t Loss: 0.1511  Pearson:0.9389 Spearman:0.9324\n",
      "Test : Loss:0.1596 \n",
      "pearson： 0.940521599320983 ALL Pearson: 0.9384878951947757\n",
      "spearman： 0.928204235036663 ALL Spearman: 0.9226075903335669\n",
      "time: 36.415754079818726\n",
      "0.0026999999999999997\n",
      "Train Epoch:185 [(0%)]\t Loss: 0.0964  Pearson:0.9771 Spearman:0.9628\n",
      "Train ALL Pearson: 0.9693607601313882\n",
      "Train  ALL Spearman: 0.9598456686079997\n",
      "31.561180591583252\n",
      "Test Epoch:185 [(0%)]\t Loss: 0.1574  Pearson:0.9444 Spearman:0.9276\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9384962135805633 ALL Pearson: 0.9387115938309172\n",
      "spearman： 0.9252275721490645 ALL Spearman: 0.9230608699383043\n",
      "time: 36.575888872146606\n",
      "0.0026999999999999997\n",
      "Train Epoch:186 [(0%)]\t Loss: 0.0998  Pearson:0.9672 Spearman:0.9495\n",
      "Train ALL Pearson: 0.970102504796121\n",
      "Train  ALL Spearman: 0.95987004527489\n",
      "32.20903134346008\n",
      "Test Epoch:186 [(0%)]\t Loss: 0.1671  Pearson:0.9300 Spearman:0.9113\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.937605547494423 ALL Pearson: 0.9383684647782247\n",
      "spearman： 0.9199268136469246 ALL Spearman: 0.9226579877855215\n",
      "time: 36.90452337265015\n",
      "0.0026999999999999997\n",
      "Train Epoch:187 [(0%)]\t Loss: 0.1059  Pearson:0.9620 Spearman:0.9492\n",
      "Train ALL Pearson: 0.9698989579714787\n",
      "Train  ALL Spearman: 0.9604455462758029\n",
      "31.350636959075928\n",
      "Test Epoch:187 [(0%)]\t Loss: 0.1618  Pearson:0.9359 Spearman:0.9211\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.9376286448760598 ALL Pearson: 0.9384358210349213\n",
      "spearman： 0.9185625004444287 ALL Spearman: 0.9227037340018125\n",
      "time: 36.16900157928467\n",
      "0.0026999999999999997\n",
      "Train Epoch:188 [(0%)]\t Loss: 0.0948  Pearson:0.9757 Spearman:0.9628\n",
      "Train ALL Pearson: 0.9702243737143764\n",
      "Train  ALL Spearman: 0.9605684284011573\n",
      "31.642215251922607\n",
      "Test Epoch:188 [(0%)]\t Loss: 0.1495  Pearson:0.9490 Spearman:0.9429\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.940028319703648 ALL Pearson: 0.9385038125694121\n",
      "spearman： 0.9250237413959088 ALL Spearman: 0.9227956410163949\n",
      "time: 36.40291428565979\n",
      "0.0008099999999999998\n",
      "Train Epoch:189 [(0%)]\t Loss: 0.1071  Pearson:0.9654 Spearman:0.9641\n",
      "Train ALL Pearson: 0.9677606303613218\n",
      "Train  ALL Spearman: 0.9576373668060195\n",
      "32.15531015396118\n",
      "Test Epoch:189 [(0%)]\t Loss: 0.1565  Pearson:0.9538 Spearman:0.9364\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9402682859433295 ALL Pearson: 0.9389578063334411\n",
      "spearman： 0.9205255816733229 ALL Spearman: 0.9231362469507501\n",
      "time: 36.68250560760498\n",
      "0.0008099999999999998\n",
      "Train Epoch:190 [(0%)]\t Loss: 0.1048  Pearson:0.9736 Spearman:0.9597\n",
      "Train ALL Pearson: 0.9692823121013926\n",
      "Train  ALL Spearman: 0.9597971249370008\n",
      "31.86979031562805\n",
      "Test Epoch:190 [(0%)]\t Loss: 0.1550  Pearson:0.9248 Spearman:0.9116\n",
      "Test : Loss:0.1572 \n",
      "pearson： 0.9370391162057851 ALL Pearson: 0.9390283689521908\n",
      "spearman： 0.918709830605754 ALL Spearman: 0.9231706745873007\n",
      "time: 36.529295206069946\n",
      "0.0008099999999999998\n",
      "Train Epoch:191 [(0%)]\t Loss: 0.1049  Pearson:0.9730 Spearman:0.9596\n",
      "Train ALL Pearson: 0.9690841506082664\n",
      "Train  ALL Spearman: 0.9593247596366377\n",
      "31.48539090156555\n",
      "Test Epoch:191 [(0%)]\t Loss: 0.1587  Pearson:0.9452 Spearman:0.9373\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.9406607853928244 ALL Pearson: 0.9389520768201246\n",
      "spearman： 0.9239117097768073 ALL Spearman: 0.9231324887961069\n",
      "time: 36.282151222229004\n",
      "0.0008099999999999998\n",
      "Train Epoch:192 [(0%)]\t Loss: 0.1015  Pearson:0.9642 Spearman:0.9588\n",
      "Train ALL Pearson: 0.9692771112422036\n",
      "Train  ALL Spearman: 0.959675626991335\n",
      "31.817360162734985\n",
      "Test Epoch:192 [(0%)]\t Loss: 0.1525  Pearson:0.9450 Spearman:0.9373\n",
      "Test : Loss:0.1572 \n",
      "pearson： 0.9414477232283391 ALL Pearson: 0.9389111472651692\n",
      "spearman： 0.9239927971719741 ALL Spearman: 0.9231036392418867\n",
      "time: 36.483659982681274\n",
      "0.0008099999999999998\n",
      "Train Epoch:193 [(0%)]\t Loss: 0.0990  Pearson:0.9708 Spearman:0.9612\n",
      "Train ALL Pearson: 0.9693323475611377\n",
      "Train  ALL Spearman: 0.9598784851429971\n",
      "31.827396869659424\n",
      "Test Epoch:193 [(0%)]\t Loss: 0.1473  Pearson:0.9485 Spearman:0.9200\n",
      "Test : Loss:0.1583 \n",
      "pearson： 0.9415331610417773 ALL Pearson: 0.9388064494204511\n",
      "spearman： 0.9232494273594849 ALL Spearman: 0.9229818747049007\n",
      "time: 36.446837186813354\n",
      "0.0008099999999999998\n",
      "Train Epoch:194 [(0%)]\t Loss: 0.0981  Pearson:0.9710 Spearman:0.9531\n",
      "Train ALL Pearson: 0.9694754652240092\n",
      "Train  ALL Spearman: 0.9597424720215416\n",
      "31.83951735496521\n",
      "Test Epoch:194 [(0%)]\t Loss: 0.1569  Pearson:0.9409 Spearman:0.9355\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9408277539864103 ALL Pearson: 0.9388258720039048\n",
      "spearman： 0.9284769765262358 ALL Spearman: 0.923001977684316\n",
      "time: 36.39405632019043\n",
      "0.0008099999999999998\n",
      "Train Epoch:195 [(0%)]\t Loss: 0.1042  Pearson:0.9645 Spearman:0.9460\n",
      "Train ALL Pearson: 0.9691497936370974\n",
      "Train  ALL Spearman: 0.9591841433034024\n",
      "32.25431561470032\n",
      "Test Epoch:195 [(0%)]\t Loss: 0.1588  Pearson:0.9492 Spearman:0.9083\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9393676261275599 ALL Pearson: 0.9387959780534827\n",
      "spearman： 0.9181422152783232 ALL Spearman: 0.9229845782654981\n",
      "time: 37.11675572395325\n",
      "0.0008099999999999998\n",
      "Train Epoch:196 [(0%)]\t Loss: 0.0940  Pearson:0.9716 Spearman:0.9665\n",
      "Train ALL Pearson: 0.9686779122149376\n",
      "Train  ALL Spearman: 0.9586930726932984\n",
      "32.2870409488678\n",
      "Test Epoch:196 [(0%)]\t Loss: 0.1532  Pearson:0.9493 Spearman:0.9344\n",
      "Test : Loss:0.1589 \n",
      "pearson： 0.9424252709063058 ALL Pearson: 0.9387767834289611\n",
      "spearman： 0.9259656749927554 ALL Spearman: 0.9228715750556328\n",
      "time: 36.9915292263031\n",
      "0.0008099999999999998\n",
      "Train Epoch:197 [(0%)]\t Loss: 0.1067  Pearson:0.9663 Spearman:0.9594\n",
      "Train ALL Pearson: 0.9680538797963437\n",
      "Train  ALL Spearman: 0.9576726765789063\n",
      "32.00800585746765\n",
      "Test Epoch:197 [(0%)]\t Loss: 0.1567  Pearson:0.9338 Spearman:0.9299\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9379207947374308 ALL Pearson: 0.9388559826194957\n",
      "spearman： 0.9247224974355013 ALL Spearman: 0.9229908384436174\n",
      "time: 36.68338918685913\n",
      "0.0008099999999999998\n",
      "Train Epoch:198 [(0%)]\t Loss: 0.0956  Pearson:0.9739 Spearman:0.9607\n",
      "Train ALL Pearson: 0.9689577257938344\n",
      "Train  ALL Spearman: 0.9586709934930991\n",
      "32.660977363586426\n",
      "Test Epoch:198 [(0%)]\t Loss: 0.1464  Pearson:0.9600 Spearman:0.9477\n",
      "Test : Loss:0.1586 \n",
      "pearson： 0.943105211213054 ALL Pearson: 0.9387623783935151\n",
      "spearman： 0.9291605459107051 ALL Spearman: 0.922964962021943\n",
      "time: 37.33248019218445\n",
      "0.0008099999999999998\n",
      "Train Epoch:199 [(0%)]\t Loss: 0.0996  Pearson:0.9720 Spearman:0.9571\n",
      "Train ALL Pearson: 0.9687441922817963\n",
      "Train  ALL Spearman: 0.9581921719545428\n",
      "31.688584566116333\n",
      "Test Epoch:199 [(0%)]\t Loss: 0.1562  Pearson:0.9434 Spearman:0.9329\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.9377399787578379 ALL Pearson: 0.9387840299104346\n",
      "spearman： 0.9217749655037154 ALL Spearman: 0.9228666742737514\n",
      "time: 36.492045640945435\n",
      "0.0008099999999999998\n",
      "Train Epoch:200 [(0%)]\t Loss: 0.1060  Pearson:0.9712 Spearman:0.9541\n",
      "Train ALL Pearson: 0.9687742623944525\n",
      "Train  ALL Spearman: 0.9590984913481033\n",
      "32.097792863845825\n",
      "Test Epoch:200 [(0%)]\t Loss: 0.1604  Pearson:0.9362 Spearman:0.9248\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9371653148669711 ALL Pearson: 0.9387960365588548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9219998296048275 ALL Spearman: 0.9228634914522997\n",
      "time: 36.7527015209198\n",
      "0.0008099999999999998\n",
      "Train Epoch:201 [(0%)]\t Loss: 0.0999  Pearson:0.9688 Spearman:0.9571\n",
      "Train ALL Pearson: 0.9689327446886051\n",
      "Train  ALL Spearman: 0.9592149625737891\n",
      "32.142720222473145\n",
      "Test Epoch:201 [(0%)]\t Loss: 0.1666  Pearson:0.9327 Spearman:0.9155\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9363012483533978 ALL Pearson: 0.9388519503818671\n",
      "spearman： 0.9232335267228282 ALL Spearman: 0.9230150354071327\n",
      "time: 36.9731707572937\n",
      "0.0008099999999999998\n",
      "Train Epoch:202 [(0%)]\t Loss: 0.1084  Pearson:0.9620 Spearman:0.9372\n",
      "Train ALL Pearson: 0.9689980459544434\n",
      "Train  ALL Spearman: 0.9586754252541729\n",
      "32.03383183479309\n",
      "Test Epoch:202 [(0%)]\t Loss: 0.1492  Pearson:0.9497 Spearman:0.9398\n",
      "Test : Loss:0.1589 \n",
      "pearson： 0.9405625373801872 ALL Pearson: 0.9387174959919171\n",
      "spearman： 0.9260427777727123 ALL Spearman: 0.9229080256804536\n",
      "time: 36.668055057525635\n",
      "0.0008099999999999998\n",
      "Train Epoch:203 [(0%)]\t Loss: 0.1075  Pearson:0.9675 Spearman:0.9584\n",
      "Train ALL Pearson: 0.9689892859813454\n",
      "Train  ALL Spearman: 0.9589486386424924\n",
      "32.01695251464844\n",
      "Test Epoch:203 [(0%)]\t Loss: 0.1637  Pearson:0.9276 Spearman:0.9173\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9365536879471589 ALL Pearson: 0.9388159611689009\n",
      "spearman： 0.9216864794429703 ALL Spearman: 0.9230294059285803\n",
      "time: 36.717997312545776\n",
      "0.0008099999999999998\n",
      "Train Epoch:204 [(0%)]\t Loss: 0.0893  Pearson:0.9792 Spearman:0.9679\n",
      "Train ALL Pearson: 0.9692280528421507\n",
      "Train  ALL Spearman: 0.9599379438660305\n",
      "32.341100215911865\n",
      "Test Epoch:204 [(0%)]\t Loss: 0.1677  Pearson:0.9216 Spearman:0.9126\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.934767058558338 ALL Pearson: 0.9388169463424685\n",
      "spearman： 0.9186033807050673 ALL Spearman: 0.923042624260645\n",
      "time: 37.206125020980835\n",
      "0.0008099999999999998\n",
      "Train Epoch:205 [(0%)]\t Loss: 0.0914  Pearson:0.9775 Spearman:0.9616\n",
      "Train ALL Pearson: 0.970298820589962\n",
      "Train  ALL Spearman: 0.9609771881077894\n",
      "31.296289920806885\n",
      "Test Epoch:205 [(0%)]\t Loss: 0.1473  Pearson:0.9447 Spearman:0.9208\n",
      "Test : Loss:0.1571 \n",
      "pearson： 0.9384138639806746 ALL Pearson: 0.9388940557376548\n",
      "spearman： 0.9180339961613208 ALL Spearman: 0.9230701075362923\n",
      "time: 36.16897177696228\n",
      "0.0008099999999999998\n",
      "Train Epoch:206 [(0%)]\t Loss: 0.0971  Pearson:0.9732 Spearman:0.9498\n",
      "Train ALL Pearson: 0.9692873333663562\n",
      "Train  ALL Spearman: 0.9587063864855749\n",
      "32.40229415893555\n",
      "Test Epoch:206 [(0%)]\t Loss: 0.1540  Pearson:0.9386 Spearman:0.9165\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9403131381787577 ALL Pearson: 0.9389622434488137\n",
      "spearman： 0.92131679555566 ALL Spearman: 0.9231530310391589\n",
      "time: 37.0388069152832\n",
      "0.0008099999999999998\n",
      "Train Epoch:207 [(0%)]\t Loss: 0.1008  Pearson:0.9653 Spearman:0.9542\n",
      "Train ALL Pearson: 0.9693606650253133\n",
      "Train  ALL Spearman: 0.9592059517957742\n",
      "31.951740503311157\n",
      "Test Epoch:207 [(0%)]\t Loss: 0.1583  Pearson:0.9341 Spearman:0.9367\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9365574622665309 ALL Pearson: 0.9389028162903587\n",
      "spearman： 0.9225801555272728 ALL Spearman: 0.9231053781034309\n",
      "time: 36.6352379322052\n",
      "0.0008099999999999998\n",
      "Train Epoch:208 [(0%)]\t Loss: 0.0936  Pearson:0.9760 Spearman:0.9701\n",
      "Train ALL Pearson: 0.969658454848609\n",
      "Train  ALL Spearman: 0.9597198176436784\n",
      "32.16631007194519\n",
      "Test Epoch:208 [(0%)]\t Loss: 0.1480  Pearson:0.9445 Spearman:0.9195\n",
      "Test : Loss:0.1568 \n",
      "pearson： 0.93769071038483 ALL Pearson: 0.9388597550706739\n",
      "spearman： 0.9211194553623042 ALL Spearman: 0.9230951083076017\n",
      "time: 36.869802474975586\n",
      "0.0008099999999999998\n",
      "Train Epoch:209 [(0%)]\t Loss: 0.1065  Pearson:0.9730 Spearman:0.9669\n",
      "Train ALL Pearson: 0.9692640888040661\n",
      "Train  ALL Spearman: 0.9588686594532275\n",
      "31.484888076782227\n",
      "Test Epoch:209 [(0%)]\t Loss: 0.1592  Pearson:0.9444 Spearman:0.9369\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9427391267736849 ALL Pearson: 0.9388466640628635\n",
      "spearman： 0.9296786566259921 ALL Spearman: 0.9230531845013783\n",
      "time: 36.14739370346069\n",
      "0.0008099999999999998\n",
      "Train Epoch:210 [(0%)]\t Loss: 0.1066  Pearson:0.9663 Spearman:0.9568\n",
      "Train ALL Pearson: 0.9684344588226879\n",
      "Train  ALL Spearman: 0.9592762794830976\n",
      "31.867586135864258\n",
      "Test Epoch:210 [(0%)]\t Loss: 0.1616  Pearson:0.9417 Spearman:0.9050\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.9375016852015977 ALL Pearson: 0.9388342360508857\n",
      "spearman： 0.9196931366096184 ALL Spearman: 0.9230863961449667\n",
      "time: 36.480106830596924\n",
      "0.0008099999999999998\n",
      "Train Epoch:211 [(0%)]\t Loss: 0.1083  Pearson:0.9682 Spearman:0.9592\n",
      "Train ALL Pearson: 0.9685754908711909\n",
      "Train  ALL Spearman: 0.9586105344175271\n",
      "31.831795692443848\n",
      "Test Epoch:211 [(0%)]\t Loss: 0.1654  Pearson:0.9451 Spearman:0.8876\n",
      "Test : Loss:0.1585 \n",
      "pearson： 0.9379932937323576 ALL Pearson: 0.9388550175038627\n",
      "spearman： 0.9164770191816327 ALL Spearman: 0.9230582561879684\n",
      "time: 36.699235677719116\n",
      "0.00024299999999999994\n",
      "Train Epoch:212 [(0%)]\t Loss: 0.1047  Pearson:0.9722 Spearman:0.9592\n",
      "Train ALL Pearson: 0.9680620133556135\n",
      "Train  ALL Spearman: 0.9582962129949688\n",
      "31.846306085586548\n",
      "Test Epoch:212 [(0%)]\t Loss: 0.1496  Pearson:0.9444 Spearman:0.9338\n",
      "Test : Loss:0.1567 \n",
      "pearson： 0.9398326618469238 ALL Pearson: 0.939012128877005\n",
      "spearman： 0.9235398909164757 ALL Spearman: 0.9231476843367852\n",
      "time: 36.721628189086914\n",
      "0.00024299999999999994\n",
      "Train Epoch:213 [(0%)]\t Loss: 0.0989  Pearson:0.9743 Spearman:0.9669\n",
      "Train ALL Pearson: 0.9685528463885267\n",
      "Train  ALL Spearman: 0.9583494073551152\n",
      "32.02537441253662\n",
      "Test Epoch:213 [(0%)]\t Loss: 0.1620  Pearson:0.9443 Spearman:0.9267\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.9414361905954205 ALL Pearson: 0.9389875179395377\n",
      "spearman： 0.9228923292596892 ALL Spearman: 0.9230971580921662\n",
      "time: 36.71487069129944\n",
      "0.00024299999999999994\n",
      "Train Epoch:214 [(0%)]\t Loss: 0.0997  Pearson:0.9655 Spearman:0.9519\n",
      "Train ALL Pearson: 0.9686034967314553\n",
      "Train  ALL Spearman: 0.9588395126822206\n",
      "31.766417026519775\n",
      "Test Epoch:214 [(0%)]\t Loss: 0.1710  Pearson:0.9387 Spearman:0.9143\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9407710202564953 ALL Pearson: 0.9389911140923669\n",
      "spearman： 0.9222006345105929 ALL Spearman: 0.9231286037393142\n",
      "time: 36.34794783592224\n",
      "0.00024299999999999994\n",
      "Train Epoch:215 [(0%)]\t Loss: 0.1055  Pearson:0.9691 Spearman:0.9573\n",
      "Train ALL Pearson: 0.9691656284465975\n",
      "Train  ALL Spearman: 0.9593621987183661\n",
      "32.06402015686035\n",
      "Test Epoch:215 [(0%)]\t Loss: 0.1521  Pearson:0.9424 Spearman:0.9281\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.9414700432161066 ALL Pearson: 0.9389588036816967\n",
      "spearman： 0.9234754450564954 ALL Spearman: 0.9231184890523982\n",
      "time: 36.86620235443115\n",
      "0.00024299999999999994\n",
      "Train Epoch:216 [(0%)]\t Loss: 0.1013  Pearson:0.9598 Spearman:0.9473\n",
      "Train ALL Pearson: 0.9690164084252961\n",
      "Train  ALL Spearman: 0.9590199931714164\n",
      "31.963900327682495\n",
      "Test Epoch:216 [(0%)]\t Loss: 0.1644  Pearson:0.9469 Spearman:0.9171\n",
      "Test : Loss:0.1585 \n",
      "pearson： 0.9376790635631906 ALL Pearson: 0.9389959972766069\n",
      "spearman： 0.9182180076135331 ALL Spearman: 0.9231592727981677\n",
      "time: 36.78988480567932\n",
      "0.00024299999999999994\n",
      "Train Epoch:217 [(0%)]\t Loss: 0.1114  Pearson:0.9623 Spearman:0.9499\n",
      "Train ALL Pearson: 0.9691230121251426\n",
      "Train  ALL Spearman: 0.959539218431429\n",
      "32.084861278533936\n",
      "Test Epoch:217 [(0%)]\t Loss: 0.1683  Pearson:0.9405 Spearman:0.9020\n",
      "Test : Loss:0.1592 \n",
      "pearson： 0.9388087856688112 ALL Pearson: 0.9390010517466555\n",
      "spearman： 0.9174311004628588 ALL Spearman: 0.9231419277338604\n",
      "time: 36.652395725250244\n",
      "0.00024299999999999994\n",
      "Train Epoch:218 [(0%)]\t Loss: 0.1076  Pearson:0.9714 Spearman:0.9669\n",
      "Train ALL Pearson: 0.9695026051439751\n",
      "Train  ALL Spearman: 0.9603020121164219\n",
      "32.135472536087036\n",
      "Test Epoch:218 [(0%)]\t Loss: 0.1676  Pearson:0.9264 Spearman:0.9027\n",
      "Test : Loss:0.1586 \n",
      "pearson： 0.9365850241521976 ALL Pearson: 0.9389627560016357\n",
      "spearman： 0.9179549618932756 ALL Spearman: 0.9230810913013817\n",
      "time: 36.82907319068909\n",
      "0.00024299999999999994\n",
      "Train Epoch:219 [(0%)]\t Loss: 0.0879  Pearson:0.9652 Spearman:0.9425\n",
      "Train ALL Pearson: 0.9686324531186653\n",
      "Train  ALL Spearman: 0.9581233679987035\n",
      "32.15251064300537\n",
      "Test Epoch:219 [(0%)]\t Loss: 0.1604  Pearson:0.9362 Spearman:0.9287\n",
      "Test : Loss:0.1572 \n",
      "pearson： 0.9389635047428464 ALL Pearson: 0.93895322486659\n",
      "spearman： 0.9267602040267089 ALL Spearman: 0.9230824817813582\n",
      "time: 36.87099814414978\n",
      "0.00024299999999999994\n",
      "Train Epoch:220 [(0%)]\t Loss: 0.1055  Pearson:0.9627 Spearman:0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson: 0.969875733383614\n",
      "Train  ALL Spearman: 0.9602342242164533\n",
      "32.01294159889221\n",
      "Test Epoch:220 [(0%)]\t Loss: 0.1554  Pearson:0.9485 Spearman:0.9331\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.9399923719472258 ALL Pearson: 0.9389670077252483\n",
      "spearman： 0.9267061723289632 ALL Spearman: 0.9231283632203333\n",
      "time: 36.662450551986694\n",
      "0.00024299999999999994\n",
      "Train Epoch:221 [(0%)]\t Loss: 0.0944  Pearson:0.9758 Spearman:0.9645\n",
      "Train ALL Pearson: 0.9688368373456454\n",
      "Train  ALL Spearman: 0.9589449542253327\n",
      "31.837140798568726\n",
      "Test Epoch:221 [(0%)]\t Loss: 0.1569  Pearson:0.9427 Spearman:0.9300\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9389397466661332 ALL Pearson: 0.9389497474130137\n",
      "spearman： 0.9248750270955042 ALL Spearman: 0.9231198952717572\n",
      "time: 36.65059757232666\n",
      "0.00024299999999999994\n",
      "Train Epoch:222 [(0%)]\t Loss: 0.1060  Pearson:0.9631 Spearman:0.9500\n",
      "Train ALL Pearson: 0.9690251755734153\n",
      "Train  ALL Spearman: 0.9588362905536306\n",
      "31.859248399734497\n",
      "Test Epoch:222 [(0%)]\t Loss: 0.1563  Pearson:0.9375 Spearman:0.9153\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.9393677255385134 ALL Pearson: 0.9389410611465397\n",
      "spearman： 0.9235628955019769 ALL Spearman: 0.9230973377691675\n",
      "time: 36.596822023391724\n",
      "0.00024299999999999994\n",
      "Train Epoch:223 [(0%)]\t Loss: 0.1025  Pearson:0.9682 Spearman:0.9615\n",
      "Train ALL Pearson: 0.9691001868919791\n",
      "Train  ALL Spearman: 0.9591312163832982\n",
      "32.03920102119446\n",
      "Test Epoch:223 [(0%)]\t Loss: 0.1618  Pearson:0.9381 Spearman:0.9315\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.9366277460643994 ALL Pearson: 0.9389343179869805\n",
      "spearman： 0.9204234279565184 ALL Spearman: 0.9230714605576997\n",
      "time: 36.709102153778076\n",
      "0.00024299999999999994\n",
      "Train Epoch:224 [(0%)]\t Loss: 0.1009  Pearson:0.9739 Spearman:0.9523\n",
      "Train ALL Pearson: 0.9690731324930633\n",
      "Train  ALL Spearman: 0.9591415955685627\n",
      "31.731940507888794\n",
      "Test Epoch:224 [(0%)]\t Loss: 0.1627  Pearson:0.9399 Spearman:0.9272\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.9395950294003456 ALL Pearson: 0.9389314038103512\n",
      "spearman： 0.924967796561753 ALL Spearman: 0.9230478682622675\n",
      "time: 36.37345099449158\n",
      "0.00024299999999999994\n",
      "Train Epoch:225 [(0%)]\t Loss: 0.0953  Pearson:0.9742 Spearman:0.9497\n",
      "Train ALL Pearson: 0.9692099798707667\n",
      "Train  ALL Spearman: 0.9592775921426239\n",
      "32.06989598274231\n",
      "Test Epoch:225 [(0%)]\t Loss: 0.1522  Pearson:0.9296 Spearman:0.9191\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9347331646987498 ALL Pearson: 0.9389229158513056\n",
      "spearman： 0.9237160702755554 ALL Spearman: 0.9230495957565108\n",
      "time: 36.71011519432068\n",
      "0.00024299999999999994\n",
      "Train Epoch:226 [(0%)]\t Loss: 0.0999  Pearson:0.9753 Spearman:0.9593\n",
      "Train ALL Pearson: 0.9696756210715376\n",
      "Train  ALL Spearman: 0.9602116843649767\n",
      "32.187782287597656\n",
      "Test Epoch:226 [(0%)]\t Loss: 0.1611  Pearson:0.9486 Spearman:0.9302\n",
      "Test : Loss:0.1571 \n",
      "pearson： 0.9405296585527207 ALL Pearson: 0.9389074476435497\n",
      "spearman： 0.9242863421529782 ALL Spearman: 0.923029116245252\n",
      "time: 36.828293561935425\n",
      "0.00024299999999999994\n",
      "Train Epoch:227 [(0%)]\t Loss: 0.1051  Pearson:0.9689 Spearman:0.9625\n",
      "Train ALL Pearson: 0.9692102749197901\n",
      "Train  ALL Spearman: 0.9589899660905697\n",
      "32.066131591796875\n",
      "Test Epoch:227 [(0%)]\t Loss: 0.1624  Pearson:0.9340 Spearman:0.9295\n",
      "Test : Loss:0.1589 \n",
      "pearson： 0.9376181803247404 ALL Pearson: 0.9388880679478334\n",
      "spearman： 0.9249927113803651 ALL Spearman: 0.9229901747387762\n",
      "time: 36.6556601524353\n",
      "0.00024299999999999994\n",
      "Train Epoch:228 [(0%)]\t Loss: 0.1025  Pearson:0.9706 Spearman:0.9551\n",
      "Train ALL Pearson: 0.9687433512256902\n",
      "Train  ALL Spearman: 0.9588987545894959\n",
      "31.911816596984863\n",
      "Test Epoch:228 [(0%)]\t Loss: 0.1647  Pearson:0.9473 Spearman:0.9372\n",
      "Test : Loss:0.1568 \n",
      "pearson： 0.9410113360443325 ALL Pearson: 0.9389008291783075\n",
      "spearman： 0.9244254287456932 ALL Spearman: 0.9229737951350897\n",
      "time: 36.65229558944702\n",
      "0.00024299999999999994\n",
      "Train Epoch:229 [(0%)]\t Loss: 0.1001  Pearson:0.9712 Spearman:0.9565\n",
      "Train ALL Pearson: 0.9693037903903047\n",
      "Train  ALL Spearman: 0.9597696788040947\n",
      "31.811747550964355\n",
      "Test Epoch:229 [(0%)]\t Loss: 0.1684  Pearson:0.9361 Spearman:0.9130\n",
      "Test : Loss:0.1567 \n",
      "pearson： 0.9378135060092364 ALL Pearson: 0.9388727430316186\n",
      "spearman： 0.9191221846628289 ALL Spearman: 0.9229801406949788\n",
      "time: 36.49524641036987\n",
      "0.00024299999999999994\n",
      "Train Epoch:230 [(0%)]\t Loss: 0.1026  Pearson:0.9594 Spearman:0.9535\n",
      "Train ALL Pearson: 0.9692782312189435\n",
      "Train  ALL Spearman: 0.9594631719190116\n",
      "32.23119878768921\n",
      "Test Epoch:230 [(0%)]\t Loss: 0.1568  Pearson:0.9487 Spearman:0.9259\n",
      "Test : Loss:0.1586 \n",
      "pearson： 0.9397432667867278 ALL Pearson: 0.9388379157318867\n",
      "spearman： 0.9217516458876613 ALL Spearman: 0.9229520531461265\n",
      "time: 36.919214487075806\n",
      "0.00024299999999999994\n",
      "Train Epoch:231 [(0%)]\t Loss: 0.0994  Pearson:0.9750 Spearman:0.9638\n",
      "Train ALL Pearson: 0.968556301620948\n",
      "Train  ALL Spearman: 0.958760321344006\n",
      "32.4140145778656\n",
      "Test Epoch:231 [(0%)]\t Loss: 0.1538  Pearson:0.9461 Spearman:0.9300\n",
      "Test : Loss:0.1605 \n",
      "pearson： 0.9404918092474528 ALL Pearson: 0.9388437579381524\n",
      "spearman： 0.9225718105301166 ALL Spearman: 0.9229525789763707\n",
      "time: 37.106119871139526\n",
      "0.00024299999999999994\n",
      "Train Epoch:232 [(0%)]\t Loss: 0.0954  Pearson:0.9749 Spearman:0.9676\n",
      "Train ALL Pearson: 0.9691026984197368\n",
      "Train  ALL Spearman: 0.9590301966472166\n",
      "31.983137369155884\n",
      "Test Epoch:232 [(0%)]\t Loss: 0.1673  Pearson:0.9431 Spearman:0.9284\n",
      "Test : Loss:0.1589 \n",
      "pearson： 0.9379005283020713 ALL Pearson: 0.9388321450271504\n",
      "spearman： 0.9210786072380454 ALL Spearman: 0.9229077198628638\n",
      "time: 36.746609687805176\n",
      "7.289999999999998e-05\n",
      "Train Epoch:233 [(0%)]\t Loss: 0.1045  Pearson:0.9679 Spearman:0.9627\n",
      "Train ALL Pearson: 0.968749772874351\n",
      "Train  ALL Spearman: 0.95910460745954\n",
      "31.871203184127808\n",
      "Test Epoch:233 [(0%)]\t Loss: 0.1592  Pearson:0.9406 Spearman:0.9095\n",
      "Test : Loss:0.1569 \n",
      "pearson： 0.9395512421312987 ALL Pearson: 0.9390084502626315\n",
      "spearman： 0.9187379022723803 ALL Spearman: 0.9231491796612804\n",
      "time: 36.72464728355408\n",
      "7.289999999999998e-05\n",
      "Train Epoch:234 [(0%)]\t Loss: 0.0907  Pearson:0.9718 Spearman:0.9673\n",
      "Train ALL Pearson: 0.9695183646802675\n",
      "Train  ALL Spearman: 0.9603111294036322\n",
      "31.85592222213745\n",
      "Test Epoch:234 [(0%)]\t Loss: 0.1430  Pearson:0.9452 Spearman:0.9368\n",
      "Test : Loss:0.1586 \n",
      "pearson： 0.9415325897083558 ALL Pearson: 0.9390041123777679\n",
      "spearman： 0.9252773838857512 ALL Spearman: 0.9231406879343685\n",
      "time: 36.63538956642151\n",
      "7.289999999999998e-05\n",
      "Train Epoch:235 [(0%)]\t Loss: 0.1050  Pearson:0.9654 Spearman:0.9535\n",
      "Train ALL Pearson: 0.9685981707214646\n",
      "Train  ALL Spearman: 0.959009873760402\n",
      "31.800249576568604\n",
      "Test Epoch:235 [(0%)]\t Loss: 0.1611  Pearson:0.9455 Spearman:0.9072\n",
      "Test : Loss:0.1575 \n",
      "pearson： 0.9390268702940947 ALL Pearson: 0.9389996809502404\n",
      "spearman： 0.9191184121084827 ALL Spearman: 0.9231432518151702\n",
      "time: 36.61427927017212\n",
      "7.289999999999998e-05\n",
      "Train Epoch:236 [(0%)]\t Loss: 0.0937  Pearson:0.9725 Spearman:0.9496\n",
      "Train ALL Pearson: 0.968476997834006\n",
      "Train  ALL Spearman: 0.9582818493668149\n",
      "32.08171486854553\n",
      "Test Epoch:236 [(0%)]\t Loss: 0.1721  Pearson:0.9325 Spearman:0.9267\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.9369653818563453 ALL Pearson: 0.9389970801991177\n",
      "spearman： 0.9237128660799456 ALL Spearman: 0.9231445238831837\n",
      "time: 36.886558055877686\n",
      "7.289999999999998e-05\n",
      "Train Epoch:237 [(0%)]\t Loss: 0.0887  Pearson:0.9770 Spearman:0.9673\n",
      "Train ALL Pearson: 0.9681369053607599\n",
      "Train  ALL Spearman: 0.957689968898522\n",
      "31.607853174209595\n",
      "Test Epoch:237 [(0%)]\t Loss: 0.1730  Pearson:0.9373 Spearman:0.9238\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.9388192252492911 ALL Pearson: 0.9389911940277365\n",
      "spearman： 0.9227988594212593 ALL Spearman: 0.9231434410260533\n",
      "time: 36.30334758758545\n",
      "7.289999999999998e-05\n",
      "Train Epoch:238 [(0%)]\t Loss: 0.1071  Pearson:0.9531 Spearman:0.9468\n",
      "Train ALL Pearson: 0.9689285922039896\n",
      "Train  ALL Spearman: 0.9577304464312083\n",
      "31.568891525268555\n",
      "Test Epoch:238 [(0%)]\t Loss: 0.1557  Pearson:0.9339 Spearman:0.9078\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.937720879609855 ALL Pearson: 0.93898890450856\n",
      "spearman： 0.9178507582715139 ALL Spearman: 0.9231346541155017\n",
      "time: 36.2189462184906\n",
      "7.289999999999998e-05\n",
      "Train Epoch:239 [(0%)]\t Loss: 0.0946  Pearson:0.9725 Spearman:0.9631\n",
      "Train ALL Pearson: 0.9699951706017123\n",
      "Train  ALL Spearman: 0.9606317138698504\n",
      "32.46052670478821\n",
      "Test Epoch:239 [(0%)]\t Loss: 0.1542  Pearson:0.9409 Spearman:0.9346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : Loss:0.1567 \n",
      "pearson： 0.9389143660520817 ALL Pearson: 0.9389770882221119\n",
      "spearman： 0.9250429543799105 ALL Spearman: 0.9231258418753123\n",
      "time: 37.277981996536255\n",
      "7.289999999999998e-05\n",
      "Train Epoch:240 [(0%)]\t Loss: 0.1095  Pearson:0.9691 Spearman:0.9658\n",
      "Train ALL Pearson: 0.9692956718150726\n",
      "Train  ALL Spearman: 0.9596476951013101\n",
      "31.50218415260315\n",
      "Test Epoch:240 [(0%)]\t Loss: 0.1424  Pearson:0.9491 Spearman:0.9071\n",
      "Test : Loss:0.1569 \n",
      "pearson： 0.9408411259916755 ALL Pearson: 0.9389733043081179\n",
      "spearman： 0.9198467303257272 ALL Spearman: 0.9231102605791188\n",
      "time: 36.096023082733154\n",
      "7.289999999999998e-05\n",
      "Train Epoch:241 [(0%)]\t Loss: 0.1025  Pearson:0.9691 Spearman:0.9658\n",
      "Train ALL Pearson: 0.9685756554788557\n",
      "Train  ALL Spearman: 0.9583516991852883\n",
      "31.67500877380371\n",
      "Test Epoch:241 [(0%)]\t Loss: 0.1708  Pearson:0.9098 Spearman:0.9088\n",
      "Test : Loss:0.1585 \n",
      "pearson： 0.9334322204956773 ALL Pearson: 0.9389772201390936\n",
      "spearman： 0.9217071228854002 ALL Spearman: 0.9231265623885674\n",
      "time: 36.49645113945007\n",
      "7.289999999999998e-05\n",
      "Train Epoch:242 [(0%)]\t Loss: 0.1040  Pearson:0.9683 Spearman:0.9447\n",
      "Train ALL Pearson: 0.968398943522261\n",
      "Train  ALL Spearman: 0.9585339444124034\n",
      "31.84980297088623\n",
      "Test Epoch:242 [(0%)]\t Loss: 0.1589  Pearson:0.9397 Spearman:0.9290\n",
      "Test : Loss:0.1575 \n",
      "pearson： 0.9359519446702443 ALL Pearson: 0.9389731834585854\n",
      "spearman： 0.9231753049646478 ALL Spearman: 0.9231116121901932\n",
      "time: 36.572288513183594\n",
      "7.289999999999998e-05\n",
      "Train Epoch:243 [(0%)]\t Loss: 0.1057  Pearson:0.9644 Spearman:0.9619\n",
      "Train ALL Pearson: 0.9681084223866631\n",
      "Train  ALL Spearman: 0.9576134783282502\n",
      "31.742265462875366\n",
      "Test Epoch:243 [(0%)]\t Loss: 0.1654  Pearson:0.9268 Spearman:0.9244\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.9353796488595972 ALL Pearson: 0.9389814649138817\n",
      "spearman： 0.9211242516727953 ALL Spearman: 0.9231322435101837\n",
      "time: 36.51873278617859\n",
      "7.289999999999998e-05\n",
      "Train Epoch:244 [(0%)]\t Loss: 0.0978  Pearson:0.9724 Spearman:0.9651\n",
      "Train ALL Pearson: 0.9684392368098601\n",
      "Train  ALL Spearman: 0.9590925375606154\n",
      "31.6894748210907\n",
      "Test Epoch:244 [(0%)]\t Loss: 0.1606  Pearson:0.9510 Spearman:0.9406\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9427243818009047 ALL Pearson: 0.9389749976922647\n",
      "spearman： 0.9264684647693585 ALL Spearman: 0.9231372157398063\n",
      "time: 36.29421329498291\n",
      "7.289999999999998e-05\n",
      "Train Epoch:245 [(0%)]\t Loss: 0.1010  Pearson:0.9697 Spearman:0.9528\n",
      "Train ALL Pearson: 0.9685072187645173\n",
      "Train  ALL Spearman: 0.9590292166151022\n",
      "32.203306913375854\n",
      "Test Epoch:245 [(0%)]\t Loss: 0.1650  Pearson:0.9273 Spearman:0.9132\n",
      "Test : Loss:0.1573 \n",
      "pearson： 0.9363340120061111 ALL Pearson: 0.938970522177641\n",
      "spearman： 0.9226793606911613 ALL Spearman: 0.9231232090572132\n",
      "time: 36.91179609298706\n",
      "7.289999999999998e-05\n",
      "Train Epoch:246 [(0%)]\t Loss: 0.0904  Pearson:0.9818 Spearman:0.9673\n",
      "Train ALL Pearson: 0.9697113446391125\n",
      "Train  ALL Spearman: 0.9598228846657224\n",
      "31.969590187072754\n",
      "Test Epoch:246 [(0%)]\t Loss: 0.1489  Pearson:0.9477 Spearman:0.9330\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.939673009852605 ALL Pearson: 0.9389694572868325\n",
      "spearman： 0.9274431362668416 ALL Spearman: 0.9231097222814908\n",
      "time: 36.650540590286255\n",
      "7.289999999999998e-05\n",
      "Train Epoch:247 [(0%)]\t Loss: 0.1095  Pearson:0.9515 Spearman:0.9406\n",
      "Train ALL Pearson: 0.9690616381013719\n",
      "Train  ALL Spearman: 0.9592330792450773\n",
      "32.36203050613403\n",
      "Test Epoch:247 [(0%)]\t Loss: 0.1633  Pearson:0.9278 Spearman:0.9196\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.9359142828809612 ALL Pearson: 0.9389602784799149\n",
      "spearman： 0.9207568645007541 ALL Spearman: 0.9231074253493776\n",
      "time: 36.95155882835388\n",
      "7.289999999999998e-05\n",
      "Train Epoch:248 [(0%)]\t Loss: 0.1147  Pearson:0.9611 Spearman:0.9449\n",
      "Train ALL Pearson: 0.969162433952942\n",
      "Train  ALL Spearman: 0.9593334542495648\n",
      "32.33236050605774\n",
      "Test Epoch:248 [(0%)]\t Loss: 0.1620  Pearson:0.9414 Spearman:0.9193\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.939940564009655 ALL Pearson: 0.9389539909961273\n",
      "spearman： 0.9196984078451749 ALL Spearman: 0.9230882255783925\n",
      "time: 37.15281438827515\n",
      "7.289999999999998e-05\n",
      "Train Epoch:249 [(0%)]\t Loss: 0.1111  Pearson:0.9529 Spearman:0.9454\n",
      "Train ALL Pearson: 0.9694245016200539\n",
      "Train  ALL Spearman: 0.9594627222084307\n",
      "31.867969036102295\n",
      "Test Epoch:249 [(0%)]\t Loss: 0.1618  Pearson:0.9355 Spearman:0.9217\n",
      "Test : Loss:0.1583 \n",
      "pearson： 0.9392413005586189 ALL Pearson: 0.9389486514787763\n",
      "spearman： 0.9235382976119711 ALL Spearman: 0.9230857365018942\n",
      "time: 36.47749042510986\n",
      "7.289999999999998e-05\n",
      "Train Epoch:250 [(0%)]\t Loss: 0.0949  Pearson:0.9646 Spearman:0.9574\n",
      "Train ALL Pearson: 0.9685893097199127\n",
      "Train  ALL Spearman: 0.9589076802431548\n",
      "31.947184801101685\n",
      "Test Epoch:250 [(0%)]\t Loss: 0.1538  Pearson:0.9445 Spearman:0.9160\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.9397155889734019 ALL Pearson: 0.9389547497383762\n",
      "spearman： 0.9216132819393149 ALL Spearman: 0.9230888012499692\n",
      "time: 36.64767789840698\n",
      "7.289999999999998e-05\n",
      "Train Epoch:251 [(0%)]\t Loss: 0.0989  Pearson:0.9728 Spearman:0.9599\n",
      "Train ALL Pearson: 0.9686224376170973\n",
      "Train  ALL Spearman: 0.9581112751176809\n",
      "31.587732553482056\n",
      "Test Epoch:251 [(0%)]\t Loss: 0.1578  Pearson:0.9510 Spearman:0.9334\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.9390957618481258 ALL Pearson: 0.9389472909960354\n",
      "spearman： 0.9201632141615638 ALL Spearman: 0.9230896438984365\n",
      "time: 36.19025659561157\n",
      "7.289999999999998e-05\n",
      "Train Epoch:252 [(0%)]\t Loss: 0.1051  Pearson:0.9726 Spearman:0.9617\n",
      "Train ALL Pearson: 0.9689105191034049\n",
      "Train  ALL Spearman: 0.9589139667438279\n",
      "32.28903532028198\n",
      "Test Epoch:252 [(0%)]\t Loss: 0.1497  Pearson:0.9486 Spearman:0.9337\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.9412677615245673 ALL Pearson: 0.9389417572506372\n",
      "spearman： 0.9246846242284767 ALL Spearman: 0.9230906141824451\n",
      "time: 37.02675104141235\n",
      "7.289999999999998e-05\n",
      "Train Epoch:253 [(0%)]\t Loss: 0.0941  Pearson:0.9771 Spearman:0.9623\n",
      "Train ALL Pearson: 0.9701233403569435\n",
      "Train  ALL Spearman: 0.9604383761612509\n",
      "32.03357148170471\n",
      "Test Epoch:253 [(0%)]\t Loss: 0.1599  Pearson:0.9318 Spearman:0.9140\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9364877640134864 ALL Pearson: 0.9389286669541569\n",
      "spearman： 0.917403836107797 ALL Spearman: 0.9230688930382296\n",
      "time: 36.77697515487671\n",
      "Test Epoch:-1 [(0%)]\t Loss: 0.1693  Pearson:0.9288 Spearman:0.9021\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.9386932316625912 ALL Pearson: 0.9390283689521908\n",
      "spearman： 0.9205389130933775 ALL Spearman: 0.9231706745873007\n",
      "PLCC: [0.9390283689521908] SRCC: [0.9231706745873007]\n",
      "Split: 0 Median PLCC: 0.9390283689521908 SRCC: 0.9231706745873007\n",
      "Test Epoch:-1 [(0%)]\t Loss: 3.3515  Pearson:-0.0013 Spearman:-0.0113\n",
      "Test : Loss:3.3487 \n",
      "pearson： -0.05249992470749237 ALL Pearson: -0.08341996443446144\n",
      "spearman： -0.046117774691332096 ALL Spearman: -0.0703936185893478\n",
      "0.01\n",
      "Train Epoch:0 [(0%)]\t Loss: 3.2721  Pearson:-0.0575 Spearman:-0.0895\n",
      "Train ALL Pearson: -0.05356732444919821\n",
      "Train  ALL Spearman: -0.035217607483795245\n",
      "20.661213636398315\n",
      "Test Epoch:0 [(0%)]\t Loss: 1.2163  Pearson:-0.2212 Spearman:-0.1983\n",
      "Test : Loss:1.1807 \n",
      "pearson： -0.22595521977046015 ALL Pearson: -0.23751201638055883\n",
      "spearman： -0.22091489384480037 ALL Spearman: -0.2293221705281301\n",
      "time: 25.37618088722229\n",
      "0.01\n",
      "Train Epoch:1 [(0%)]\t Loss: 0.9789  Pearson:-0.2934 Spearman:-0.3138\n",
      "Train ALL Pearson: 0.1084357505141164\n",
      "Train  ALL Spearman: 0.09408866665223102\n",
      "20.729068994522095\n",
      "Test Epoch:1 [(0%)]\t Loss: 0.5784  Pearson:0.4938 Spearman:0.4663\n",
      "Test : Loss:0.6167 \n",
      "pearson： 0.483142240176911 ALL Pearson: 0.4817282226920059\n",
      "spearman： 0.4386850868471963 ALL Spearman: 0.4347672044290357\n",
      "time: 25.328592777252197\n",
      "0.01\n",
      "Train Epoch:2 [(0%)]\t Loss: 0.5289  Pearson:0.4604 Spearman:0.4143\n",
      "Train ALL Pearson: 0.42077328818059695\n",
      "Train  ALL Spearman: 0.39126955112567463\n",
      "20.286420583724976\n",
      "Test Epoch:2 [(0%)]\t Loss: 0.6204  Pearson:0.5503 Spearman:0.5347\n",
      "Test : Loss:0.6234 \n",
      "pearson： 0.6049096062526076 ALL Pearson: 0.6263006653385568\n",
      "spearman： 0.5604097362377204 ALL Spearman: 0.5818011498642414\n",
      "time: 24.72282314300537\n",
      "0.01\n",
      "Train Epoch:3 [(0%)]\t Loss: 0.4835  Pearson:0.3755 Spearman:0.3765\n",
      "Train ALL Pearson: 0.503801351433067\n",
      "Train  ALL Spearman: 0.4742071266919387\n",
      "20.228515148162842\n",
      "Test Epoch:3 [(0%)]\t Loss: 0.5744  Pearson:0.6814 Spearman:0.6035\n",
      "Test : Loss:0.6152 \n",
      "pearson： 0.6771714493369968 ALL Pearson: 0.6740126786374042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.6253682189550404 ALL Spearman: 0.6351097993682906\n",
      "time: 24.85920739173889\n",
      "0.01\n",
      "Train Epoch:4 [(0%)]\t Loss: 0.4499  Pearson:0.5412 Spearman:0.5147\n",
      "Train ALL Pearson: 0.5449525931029225\n",
      "Train  ALL Spearman: 0.5092188504446827\n",
      "20.177107572555542\n",
      "Test Epoch:4 [(0%)]\t Loss: 0.5364  Pearson:0.7258 Spearman:0.6836\n",
      "Test : Loss:0.5738 \n",
      "pearson： 0.7005573417399672 ALL Pearson: 0.7002638304758179\n",
      "spearman： 0.6622549281298893 ALL Spearman: 0.66319208335847\n",
      "time: 24.732648134231567\n",
      "0.01\n",
      "Train Epoch:5 [(0%)]\t Loss: 0.3925  Pearson:0.5470 Spearman:0.5034\n",
      "Train ALL Pearson: 0.5825922211619167\n",
      "Train  ALL Spearman: 0.545486954576037\n",
      "19.96180248260498\n",
      "Test Epoch:5 [(0%)]\t Loss: 0.6066  Pearson:0.6990 Spearman:0.6837\n",
      "Test : Loss:0.6151 \n",
      "pearson： 0.7059327814853799 ALL Pearson: 0.7187168749849586\n",
      "spearman： 0.6743935442366898 ALL Spearman: 0.6816945503163346\n",
      "time: 24.409375190734863\n",
      "0.01\n",
      "Train Epoch:6 [(0%)]\t Loss: 0.3878  Pearson:0.6294 Spearman:0.5773\n",
      "Train ALL Pearson: 0.5972576252236428\n",
      "Train  ALL Spearman: 0.5643369291459259\n",
      "20.069084644317627\n",
      "Test Epoch:6 [(0%)]\t Loss: 0.5815  Pearson:0.7209 Spearman:0.7122\n",
      "Test : Loss:0.6038 \n",
      "pearson： 0.7193079064742355 ALL Pearson: 0.7233723183779135\n",
      "spearman： 0.6891462807074772 ALL Spearman: 0.6890053028638625\n",
      "time: 24.400694847106934\n",
      "0.01\n",
      "Train Epoch:7 [(0%)]\t Loss: 0.4087  Pearson:0.5417 Spearman:0.5230\n",
      "Train ALL Pearson: 0.6133550545384331\n",
      "Train  ALL Spearman: 0.5773280527947812\n",
      "20.14362907409668\n",
      "Test Epoch:7 [(0%)]\t Loss: 0.6318  Pearson:0.6651 Spearman:0.6859\n",
      "Test : Loss:0.6133 \n",
      "pearson： 0.7142429189261752 ALL Pearson: 0.731813720784761\n",
      "spearman： 0.6955849150166532 ALL Spearman: 0.6983446022802308\n",
      "time: 24.851435899734497\n",
      "0.01\n",
      "Train Epoch:8 [(0%)]\t Loss: 0.3871  Pearson:0.5999 Spearman:0.5187\n",
      "Train ALL Pearson: 0.6111026396535015\n",
      "Train  ALL Spearman: 0.5794405782146388\n",
      "19.864420175552368\n",
      "Test Epoch:8 [(0%)]\t Loss: 0.6089  Pearson:0.6838 Spearman:0.6624\n",
      "Test : Loss:0.6042 \n",
      "pearson： 0.7275301445631812 ALL Pearson: 0.7362651980255565\n",
      "spearman： 0.6956200815269115 ALL Spearman: 0.7060398651008987\n",
      "time: 24.24001693725586\n",
      "0.01\n",
      "Train Epoch:9 [(0%)]\t Loss: 0.3484  Pearson:0.6182 Spearman:0.5945\n",
      "Train ALL Pearson: 0.627885709975161\n",
      "Train  ALL Spearman: 0.5930183937165643\n",
      "20.069299697875977\n",
      "Test Epoch:9 [(0%)]\t Loss: 0.6010  Pearson:0.7485 Spearman:0.7141\n",
      "Test : Loss:0.6102 \n",
      "pearson： 0.7441518545298359 ALL Pearson: 0.7405786670945661\n",
      "spearman： 0.7175826673363631 ALL Spearman: 0.7136192035083879\n",
      "time: 24.455893754959106\n",
      "0.01\n",
      "Train Epoch:10 [(0%)]\t Loss: 0.3749  Pearson:0.5727 Spearman:0.5327\n",
      "Train ALL Pearson: 0.6278502622451119\n",
      "Train  ALL Spearman: 0.6002349262004377\n",
      "20.36883282661438\n",
      "Test Epoch:10 [(0%)]\t Loss: 0.5705  Pearson:0.8040 Spearman:0.7916\n",
      "Test : Loss:0.5707 \n",
      "pearson： 0.7691463919836458 ALL Pearson: 0.7425294164882509\n",
      "spearman： 0.743538921089489 ALL Spearman: 0.7142770131514707\n",
      "time: 25.141303300857544\n",
      "0.001\n",
      "Train Epoch:11 [(0%)]\t Loss: 0.3756  Pearson:0.6484 Spearman:0.6226\n",
      "Train ALL Pearson: 0.6456040006767408\n",
      "Train  ALL Spearman: 0.6077934001400479\n",
      "31.979469776153564\n",
      "Test Epoch:11 [(0%)]\t Loss: 0.6079  Pearson:0.7467 Spearman:0.7282\n",
      "Test : Loss:0.5867 \n",
      "pearson： 0.7592458092898287 ALL Pearson: 0.7513643416810543\n",
      "spearman： 0.7385086585473001 ALL Spearman: 0.7254278158435644\n",
      "time: 36.29908561706543\n",
      "0.001\n",
      "Train Epoch:12 [(0%)]\t Loss: 0.3331  Pearson:0.7143 Spearman:0.6981\n",
      "Train ALL Pearson: 0.6577242031610757\n",
      "Train  ALL Spearman: 0.6280150931403695\n",
      "32.29043626785278\n",
      "Test Epoch:12 [(0%)]\t Loss: 0.5879  Pearson:0.7896 Spearman:0.7367\n",
      "Test : Loss:0.5738 \n",
      "pearson： 0.7694238992223827 ALL Pearson: 0.7596357016829096\n",
      "spearman： 0.7382611683099303 ALL Spearman: 0.7354443080225121\n",
      "time: 36.92695140838623\n",
      "0.001\n",
      "Train Epoch:13 [(0%)]\t Loss: 0.3506  Pearson:0.6288 Spearman:0.5851\n",
      "Train ALL Pearson: 0.6724169693309557\n",
      "Train  ALL Spearman: 0.6404372081998364\n",
      "32.226107120513916\n",
      "Test Epoch:13 [(0%)]\t Loss: 0.5752  Pearson:0.7690 Spearman:0.7464\n",
      "Test : Loss:0.5807 \n",
      "pearson： 0.7585095495845039 ALL Pearson: 0.7659232186035867\n",
      "spearman： 0.7330056330912396 ALL Spearman: 0.7424929148395601\n",
      "time: 36.76793456077576\n",
      "0.001\n",
      "Train Epoch:14 [(0%)]\t Loss: 0.3238  Pearson:0.7139 Spearman:0.6858\n",
      "Train ALL Pearson: 0.6757969300069069\n",
      "Train  ALL Spearman: 0.6481972921513128\n",
      "31.37119746208191\n",
      "Test Epoch:14 [(0%)]\t Loss: 0.5762  Pearson:0.7169 Spearman:0.7025\n",
      "Test : Loss:0.5540 \n",
      "pearson： 0.7613792767667958 ALL Pearson: 0.7721648388468503\n",
      "spearman： 0.7458629619173259 ALL Spearman: 0.7494830194577593\n",
      "time: 35.85176062583923\n",
      "0.001\n",
      "Train Epoch:15 [(0%)]\t Loss: 0.3207  Pearson:0.6961 Spearman:0.6760\n",
      "Train ALL Pearson: 0.6914338939934908\n",
      "Train  ALL Spearman: 0.6647615456009943\n",
      "32.04256844520569\n",
      "Test Epoch:15 [(0%)]\t Loss: 0.5445  Pearson:0.7463 Spearman:0.7182\n",
      "Test : Loss:0.5570 \n",
      "pearson： 0.7729214126486782 ALL Pearson: 0.7770952054136091\n",
      "spearman： 0.7410316200200119 ALL Spearman: 0.7553013762076144\n",
      "time: 36.65365386009216\n",
      "0.001\n",
      "Train Epoch:16 [(0%)]\t Loss: 0.3445  Pearson:0.7119 Spearman:0.6565\n",
      "Train ALL Pearson: 0.6928747451189635\n",
      "Train  ALL Spearman: 0.6639361376647073\n",
      "31.73582649230957\n",
      "Test Epoch:16 [(0%)]\t Loss: 0.5555  Pearson:0.7603 Spearman:0.7480\n",
      "Test : Loss:0.5718 \n",
      "pearson： 0.7750697658959759 ALL Pearson: 0.781876729389831\n",
      "spearman： 0.7628946459655144 ALL Spearman: 0.7619342381020938\n",
      "time: 36.10095143318176\n",
      "0.001\n",
      "Train Epoch:17 [(0%)]\t Loss: 0.2993  Pearson:0.7309 Spearman:0.6919\n",
      "Train ALL Pearson: 0.7086360059120089\n",
      "Train  ALL Spearman: 0.6801236722948685\n",
      "31.69633936882019\n",
      "Test Epoch:17 [(0%)]\t Loss: 0.5794  Pearson:0.7306 Spearman:0.7180\n",
      "Test : Loss:0.5625 \n",
      "pearson： 0.7848405762410464 ALL Pearson: 0.7857761366790823\n",
      "spearman： 0.7639390603973382 ALL Spearman: 0.7652314089046188\n",
      "time: 36.32085680961609\n",
      "0.001\n",
      "Train Epoch:18 [(0%)]\t Loss: 0.3605  Pearson:0.6810 Spearman:0.6486\n",
      "Train ALL Pearson: 0.700726970156201\n",
      "Train  ALL Spearman: 0.6690837836903687\n",
      "31.877522706985474\n",
      "Test Epoch:18 [(0%)]\t Loss: 0.5134  Pearson:0.7958 Spearman:0.7765\n",
      "Test : Loss:0.5486 \n",
      "pearson： 0.7975390746986245 ALL Pearson: 0.7906642537856851\n",
      "spearman： 0.7776487231551816 ALL Spearman: 0.7698745122509505\n",
      "time: 36.24375057220459\n",
      "0.001\n",
      "Train Epoch:19 [(0%)]\t Loss: 0.3368  Pearson:0.6804 Spearman:0.6705\n",
      "Train ALL Pearson: 0.7165405545662985\n",
      "Train  ALL Spearman: 0.6874813749731836\n",
      "31.623594760894775\n",
      "Test Epoch:19 [(0%)]\t Loss: 0.5462  Pearson:0.7904 Spearman:0.7599\n",
      "Test : Loss:0.5441 \n",
      "pearson： 0.7947488475176819 ALL Pearson: 0.7947838110630375\n",
      "spearman： 0.778887708753646 ALL Spearman: 0.7746225851080126\n",
      "time: 36.182133197784424\n",
      "0.001\n",
      "Train Epoch:20 [(0%)]\t Loss: 0.3376  Pearson:0.6938 Spearman:0.6498\n",
      "Train ALL Pearson: 0.7127670511340378\n",
      "Train  ALL Spearman: 0.6887355296310322\n",
      "32.3179874420166\n",
      "Test Epoch:20 [(0%)]\t Loss: 0.5651  Pearson:0.7959 Spearman:0.7674\n",
      "Test : Loss:0.5359 \n",
      "pearson： 0.7954613737410186 ALL Pearson: 0.7989211065231487\n",
      "spearman： 0.7802621862552468 ALL Spearman: 0.7792709474904888\n",
      "time: 36.67910194396973\n",
      "0.005\n",
      "Train Epoch:21 [(0%)]\t Loss: 0.2998  Pearson:0.7594 Spearman:0.7411\n",
      "Train ALL Pearson: 0.7327501263697148\n",
      "Train  ALL Spearman: 0.7090539666347931\n",
      "32.13829517364502\n",
      "Test Epoch:21 [(0%)]\t Loss: 0.5425  Pearson:0.7855 Spearman:0.7681\n",
      "Test : Loss:0.5209 \n",
      "pearson： 0.8097986895358088 ALL Pearson: 0.8147930965001606\n",
      "spearman： 0.7938760296373245 ALL Spearman: 0.796914631666966\n",
      "time: 36.67251634597778\n",
      "0.005\n",
      "Train Epoch:22 [(0%)]\t Loss: 0.3066  Pearson:0.6709 Spearman:0.6632\n",
      "Train ALL Pearson: 0.751491311136684\n",
      "Train  ALL Spearman: 0.727204809162082\n",
      "31.92417860031128\n",
      "Test Epoch:22 [(0%)]\t Loss: 0.5047  Pearson:0.7895 Spearman:0.7582\n",
      "Test : Loss:0.5203 \n",
      "pearson： 0.8183337297994524 ALL Pearson: 0.821318384690662\n",
      "spearman： 0.7986035366068059 ALL Spearman: 0.8021624963016556\n",
      "time: 36.60867643356323\n",
      "0.005\n",
      "Train Epoch:23 [(0%)]\t Loss: 0.2782  Pearson:0.7573 Spearman:0.7419\n",
      "Train ALL Pearson: 0.7626087940361032\n",
      "Train  ALL Spearman: 0.7369986384303819\n",
      "32.18537092208862\n",
      "Test Epoch:23 [(0%)]\t Loss: 0.4786  Pearson:0.8268 Spearman:0.8162\n",
      "Test : Loss:0.4712 \n",
      "pearson： 0.8361733058119025 ALL Pearson: 0.8345672648321595\n",
      "spearman： 0.8179046470972128 ALL Spearman: 0.8159035333658681\n",
      "time: 36.77889680862427\n",
      "0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:24 [(0%)]\t Loss: 0.2957  Pearson:0.7288 Spearman:0.6737\n",
      "Train ALL Pearson: 0.7762545841911699\n",
      "Train  ALL Spearman: 0.7449154213899523\n",
      "32.23492455482483\n",
      "Test Epoch:24 [(0%)]\t Loss: 0.5180  Pearson:0.8329 Spearman:0.7975\n",
      "Test : Loss:0.5085 \n",
      "pearson： 0.8405835189498959 ALL Pearson: 0.8419260938256641\n",
      "spearman： 0.8138329032982103 ALL Spearman: 0.8239438369878896\n",
      "time: 36.697492837905884\n",
      "0.005\n",
      "Train Epoch:25 [(0%)]\t Loss: 0.2374  Pearson:0.8117 Spearman:0.7673\n",
      "Train ALL Pearson: 0.7861842530044023\n",
      "Train  ALL Spearman: 0.7593651488407722\n",
      "32.045862436294556\n",
      "Test Epoch:25 [(0%)]\t Loss: 0.4253  Pearson:0.8812 Spearman:0.8685\n",
      "Test : Loss:0.4370 \n",
      "pearson： 0.8576772844379366 ALL Pearson: 0.851775530979883\n",
      "spearman： 0.8436467805805065 ALL Spearman: 0.8318065719848105\n",
      "time: 36.43145513534546\n",
      "0.005\n",
      "Train Epoch:26 [(0%)]\t Loss: 0.2595  Pearson:0.8209 Spearman:0.8032\n",
      "Train ALL Pearson: 0.7989878937117193\n",
      "Train  ALL Spearman: 0.7705436987368268\n",
      "32.01405119895935\n",
      "Test Epoch:26 [(0%)]\t Loss: 0.4633  Pearson:0.8191 Spearman:0.8053\n",
      "Test : Loss:0.4463 \n",
      "pearson： 0.8545302818517534 ALL Pearson: 0.8599693701847545\n",
      "spearman： 0.8332055320332453 ALL Spearman: 0.8430049131767124\n",
      "time: 36.47062158584595\n",
      "0.005\n",
      "Train Epoch:27 [(0%)]\t Loss: 0.2519  Pearson:0.8191 Spearman:0.7884\n",
      "Train ALL Pearson: 0.8136920053095644\n",
      "Train  ALL Spearman: 0.7866804476282304\n",
      "31.773160696029663\n",
      "Test Epoch:27 [(0%)]\t Loss: 0.4537  Pearson:0.8696 Spearman:0.8721\n",
      "Test : Loss:0.4410 \n",
      "pearson： 0.8677250227944729 ALL Pearson: 0.8675188369894125\n",
      "spearman： 0.8563077294548258 ALL Spearman: 0.8502476733210471\n",
      "time: 36.06778287887573\n",
      "0.005\n",
      "Train Epoch:28 [(0%)]\t Loss: 0.2310  Pearson:0.8826 Spearman:0.8719\n",
      "Train ALL Pearson: 0.8235978993921181\n",
      "Train  ALL Spearman: 0.7960848279267695\n",
      "31.92774724960327\n",
      "Test Epoch:28 [(0%)]\t Loss: 0.4062  Pearson:0.8380 Spearman:0.8237\n",
      "Test : Loss:0.3823 \n",
      "pearson： 0.8666458324999833 ALL Pearson: 0.8755367619468593\n",
      "spearman： 0.8474811616775315 ALL Spearman: 0.8561185529639493\n",
      "time: 36.5682590007782\n",
      "0.005\n",
      "Train Epoch:29 [(0%)]\t Loss: 0.2577  Pearson:0.8437 Spearman:0.8090\n",
      "Train ALL Pearson: 0.8282859842125337\n",
      "Train  ALL Spearman: 0.797555662649924\n",
      "32.01626467704773\n",
      "Test Epoch:29 [(0%)]\t Loss: 0.4203  Pearson:0.8894 Spearman:0.8656\n",
      "Test : Loss:0.4028 \n",
      "pearson： 0.8881615145087413 ALL Pearson: 0.8807958273413715\n",
      "spearman： 0.8655406362556185 ALL Spearman: 0.8625261230759409\n",
      "time: 36.680768728256226\n",
      "0.005\n",
      "Train Epoch:30 [(0%)]\t Loss: 0.2348  Pearson:0.8305 Spearman:0.8043\n",
      "Train ALL Pearson: 0.8378155308093524\n",
      "Train  ALL Spearman: 0.8089681732635454\n",
      "31.838279247283936\n",
      "Test Epoch:30 [(0%)]\t Loss: 0.3497  Pearson:0.8934 Spearman:0.8855\n",
      "Test : Loss:0.3312 \n",
      "pearson： 0.888901084559559 ALL Pearson: 0.8862739545572509\n",
      "spearman： 0.8725567611701789 ALL Spearman: 0.8680598098851261\n",
      "time: 36.23386836051941\n",
      "0.03\n",
      "Train Epoch:31 [(0%)]\t Loss: 0.2522  Pearson:0.8429 Spearman:0.8036\n",
      "Train ALL Pearson: 0.5519864239040895\n",
      "Train  ALL Spearman: 0.5612974668797577\n",
      "32.05649971961975\n",
      "Test Epoch:31 [(0%)]\t Loss: 0.3632  Pearson:0.8599 Spearman:0.8215\n",
      "Test : Loss:0.3526 \n",
      "pearson： 0.8544442310299022 ALL Pearson: 0.8457468528570043\n",
      "spearman： 0.8259161576320951 ALL Spearman: 0.8182485881932117\n",
      "time: 36.52706599235535\n",
      "0.03\n",
      "Train Epoch:32 [(0%)]\t Loss: 0.2849  Pearson:0.8129 Spearman:0.7644\n",
      "Train ALL Pearson: 0.721317749664881\n",
      "Train  ALL Spearman: 0.6867764657345689\n",
      "32.12325310707092\n",
      "Test Epoch:32 [(0%)]\t Loss: 0.3124  Pearson:0.8387 Spearman:0.8343\n",
      "Test : Loss:0.2923 \n",
      "pearson： 0.8529678002503303 ALL Pearson: 0.8574706553945349\n",
      "spearman： 0.8296657897250526 ALL Spearman: 0.8259296059964544\n",
      "time: 36.55083227157593\n",
      "0.03\n",
      "Train Epoch:33 [(0%)]\t Loss: 0.3276  Pearson:0.7886 Spearman:0.7345\n",
      "Train ALL Pearson: 0.7419705598831577\n",
      "Train  ALL Spearman: 0.7032631378711894\n",
      "31.57448720932007\n",
      "Test Epoch:33 [(0%)]\t Loss: 0.2803  Pearson:0.8515 Spearman:0.8157\n",
      "Test : Loss:0.2729 \n",
      "pearson： 0.8663091199552405 ALL Pearson: 0.8677061638738585\n",
      "spearman： 0.8373003914980786 ALL Spearman: 0.8401633828302398\n",
      "time: 36.1057767868042\n",
      "0.03\n",
      "Train Epoch:34 [(0%)]\t Loss: 0.2637  Pearson:0.8503 Spearman:0.8099\n",
      "Train ALL Pearson: 0.7752529987112402\n",
      "Train  ALL Spearman: 0.7376980204637058\n",
      "31.69388771057129\n",
      "Test Epoch:34 [(0%)]\t Loss: 0.2858  Pearson:0.8709 Spearman:0.8322\n",
      "Test : Loss:0.2813 \n",
      "pearson： 0.8723501188355133 ALL Pearson: 0.8734457160224134\n",
      "spearman： 0.8434820674162836 ALL Spearman: 0.8470525925723226\n",
      "time: 36.37969946861267\n",
      "0.03\n",
      "Train Epoch:35 [(0%)]\t Loss: 0.2805  Pearson:0.8410 Spearman:0.7980\n",
      "Train ALL Pearson: 0.806726684379349\n",
      "Train  ALL Spearman: 0.7737271971817264\n",
      "31.73871874809265\n",
      "Test Epoch:35 [(0%)]\t Loss: 0.5115  Pearson:0.8995 Spearman:0.8857\n",
      "Test : Loss:0.4986 \n",
      "pearson： 0.8862163821598771 ALL Pearson: 0.8828573116612325\n",
      "spearman： 0.8677160822460807 ALL Spearman: 0.8672049729335531\n",
      "time: 36.107316732406616\n",
      "0.03\n",
      "Train Epoch:36 [(0%)]\t Loss: 0.2851  Pearson:0.8552 Spearman:0.8362\n",
      "Train ALL Pearson: 0.8136708739963657\n",
      "Train  ALL Spearman: 0.7771688212533074\n",
      "32.03651189804077\n",
      "Test Epoch:36 [(0%)]\t Loss: 0.3711  Pearson:0.9154 Spearman:0.8991\n",
      "Test : Loss:0.3974 \n",
      "pearson： 0.8950769969016936 ALL Pearson: 0.8900692039994413\n",
      "spearman： 0.8739179445790526 ALL Spearman: 0.8698470009758297\n",
      "time: 36.49308180809021\n",
      "0.03\n",
      "Train Epoch:37 [(0%)]\t Loss: 0.2256  Pearson:0.8701 Spearman:0.8388\n",
      "Train ALL Pearson: 0.8421886301645354\n",
      "Train  ALL Spearman: 0.8108724678319308\n",
      "32.13346886634827\n",
      "Test Epoch:37 [(0%)]\t Loss: 0.5328  Pearson:0.8794 Spearman:0.8490\n",
      "Test : Loss:0.5016 \n",
      "pearson： 0.8899153293138901 ALL Pearson: 0.8915964104606097\n",
      "spearman： 0.8700485811320461 ALL Spearman: 0.8778140055242343\n",
      "time: 36.571045875549316\n",
      "0.03\n",
      "Train Epoch:38 [(0%)]\t Loss: 0.2946  Pearson:0.8873 Spearman:0.8623\n",
      "Train ALL Pearson: 0.8353963505598618\n",
      "Train  ALL Spearman: 0.8032604764033339\n",
      "32.07315397262573\n",
      "Test Epoch:38 [(0%)]\t Loss: 0.2245  Pearson:0.9031 Spearman:0.8802\n",
      "Test : Loss:0.2180 \n",
      "pearson： 0.8997327299135172 ALL Pearson: 0.8988410169479107\n",
      "spearman： 0.8787109754673686 ALL Spearman: 0.8792685675325314\n",
      "time: 36.47924065589905\n",
      "0.03\n",
      "Train Epoch:39 [(0%)]\t Loss: 0.2270  Pearson:0.8943 Spearman:0.8646\n",
      "Train ALL Pearson: 0.8460353551347034\n",
      "Train  ALL Spearman: 0.816781054582579\n",
      "31.768425464630127\n",
      "Test Epoch:39 [(0%)]\t Loss: 0.2201  Pearson:0.9045 Spearman:0.8888\n",
      "Test : Loss:0.2101 \n",
      "pearson： 0.8983660283761271 ALL Pearson: 0.8998799464284883\n",
      "spearman： 0.8751034529585027 ALL Spearman: 0.8794289513115748\n",
      "time: 36.24601650238037\n",
      "0.03\n",
      "Train Epoch:40 [(0%)]\t Loss: 0.2291  Pearson:0.9035 Spearman:0.8691\n",
      "Train ALL Pearson: 0.8420291297320958\n",
      "Train  ALL Spearman: 0.8091765563361436\n",
      "32.384716510772705\n",
      "Test Epoch:40 [(0%)]\t Loss: 0.2384  Pearson:0.8876 Spearman:0.8724\n",
      "Test : Loss:0.2166 \n",
      "pearson： 0.8974977210216006 ALL Pearson: 0.9028668761941813\n",
      "spearman： 0.8765222775894513 ALL Spearman: 0.8824883059335356\n",
      "time: 36.87627625465393\n",
      "0.03\n",
      "Train Epoch:41 [(0%)]\t Loss: 0.2654  Pearson:0.8834 Spearman:0.8687\n",
      "Train ALL Pearson: 0.8495063086072355\n",
      "Train  ALL Spearman: 0.8169557061705787\n",
      "31.736075162887573\n",
      "Test Epoch:41 [(0%)]\t Loss: 0.2382  Pearson:0.8956 Spearman:0.8852\n",
      "Test : Loss:0.2430 \n",
      "pearson： 0.9010301096957369 ALL Pearson: 0.9038248938553928\n",
      "spearman： 0.8860497155490905 ALL Spearman: 0.8863892500802426\n",
      "time: 36.21563696861267\n",
      "0.03\n",
      "Train Epoch:42 [(0%)]\t Loss: 0.1620  Pearson:0.9065 Spearman:0.8512\n",
      "Train ALL Pearson: 0.8642742657937809\n",
      "Train  ALL Spearman: 0.8345971878772892\n",
      "32.09069895744324\n",
      "Test Epoch:42 [(0%)]\t Loss: 0.2137  Pearson:0.8990 Spearman:0.8734\n",
      "Test : Loss:0.2096 \n",
      "pearson： 0.9025671669601226 ALL Pearson: 0.9028332141690477\n",
      "spearman： 0.8759568416271398 ALL Spearman: 0.8779025752906802\n",
      "time: 36.634241819381714\n",
      "0.03\n",
      "Train Epoch:43 [(0%)]\t Loss: 0.2362  Pearson:0.9080 Spearman:0.8471\n",
      "Train ALL Pearson: 0.8649139786012324\n",
      "Train  ALL Spearman: 0.8328577680349315\n",
      "32.13278269767761\n",
      "Test Epoch:43 [(0%)]\t Loss: 0.1742  Pearson:0.9202 Spearman:0.9133\n",
      "Test : Loss:0.1813 \n",
      "pearson： 0.911898379064726 ALL Pearson: 0.9089128615853618\n",
      "spearman： 0.897777845174605 ALL Spearman: 0.890093363153268\n",
      "time: 36.83914566040039\n",
      "0.03\n",
      "Train Epoch:44 [(0%)]\t Loss: 0.2621  Pearson:0.8941 Spearman:0.8811\n",
      "Train ALL Pearson: 0.8698085635057952\n",
      "Train  ALL Spearman: 0.8377027157982532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.89212727546692\n",
      "Test Epoch:44 [(0%)]\t Loss: 0.2008  Pearson:0.9073 Spearman:0.9018\n",
      "Test : Loss:0.1989 \n",
      "pearson： 0.9092290126718994 ALL Pearson: 0.9120829229168395\n",
      "spearman： 0.8958046732731776 ALL Spearman: 0.892481590766459\n",
      "time: 36.27772068977356\n",
      "0.03\n",
      "Train Epoch:45 [(0%)]\t Loss: 0.1820  Pearson:0.9112 Spearman:0.8984\n",
      "Train ALL Pearson: 0.8805224693155714\n",
      "Train  ALL Spearman: 0.8476747682241272\n",
      "32.111087799072266\n",
      "Test Epoch:45 [(0%)]\t Loss: 0.1939  Pearson:0.9130 Spearman:0.9008\n",
      "Test : Loss:0.1902 \n",
      "pearson： 0.9109219397718669 ALL Pearson: 0.9119431681913782\n",
      "spearman： 0.8938452563284744 ALL Spearman: 0.8933981335130449\n",
      "time: 36.8085823059082\n",
      "0.03\n",
      "Train Epoch:46 [(0%)]\t Loss: 0.2095  Pearson:0.9073 Spearman:0.8825\n",
      "Train ALL Pearson: 0.8832221978161147\n",
      "Train  ALL Spearman: 0.8505241614979369\n",
      "31.823427200317383\n",
      "Test Epoch:46 [(0%)]\t Loss: 0.3047  Pearson:0.8846 Spearman:0.8764\n",
      "Test : Loss:0.3114 \n",
      "pearson： 0.9114493174394318 ALL Pearson: 0.9160880660682239\n",
      "spearman： 0.8988280734151253 ALL Spearman: 0.9013140855046411\n",
      "time: 36.405956506729126\n",
      "0.03\n",
      "Train Epoch:47 [(0%)]\t Loss: 0.1938  Pearson:0.9171 Spearman:0.9150\n",
      "Train ALL Pearson: 0.8898064271994722\n",
      "Train  ALL Spearman: 0.8558919791493218\n",
      "31.983585119247437\n",
      "Test Epoch:47 [(0%)]\t Loss: 0.3717  Pearson:0.8876 Spearman:0.8848\n",
      "Test : Loss:0.3757 \n",
      "pearson： 0.9087657405852365 ALL Pearson: 0.9151900831315313\n",
      "spearman： 0.8962572222154196 ALL Spearman: 0.9015981889992876\n",
      "time: 36.53812503814697\n",
      "0.03\n",
      "Train Epoch:48 [(0%)]\t Loss: 0.2140  Pearson:0.9087 Spearman:0.8749\n",
      "Train ALL Pearson: 0.8934212858125703\n",
      "Train  ALL Spearman: 0.8620856264669357\n",
      "31.962952613830566\n",
      "Test Epoch:48 [(0%)]\t Loss: 0.2609  Pearson:0.9290 Spearman:0.9251\n",
      "Test : Loss:0.2692 \n",
      "pearson： 0.921887876216697 ALL Pearson: 0.9210319028738762\n",
      "spearman： 0.9102940642907739 ALL Spearman: 0.9060052473543294\n",
      "time: 36.393532514572144\n",
      "0.03\n",
      "Train Epoch:49 [(0%)]\t Loss: 0.1765  Pearson:0.9140 Spearman:0.8746\n",
      "Train ALL Pearson: 0.8984895470214069\n",
      "Train  ALL Spearman: 0.8669244137463209\n",
      "31.867244482040405\n",
      "Test Epoch:49 [(0%)]\t Loss: 0.3509  Pearson:0.9288 Spearman:0.9245\n",
      "Test : Loss:0.3527 \n",
      "pearson： 0.9224726423938449 ALL Pearson: 0.9190863118160675\n",
      "spearman： 0.911040477360882 ALL Spearman: 0.9018028807186129\n",
      "time: 36.235843896865845\n",
      "0.03\n",
      "Train Epoch:50 [(0%)]\t Loss: 0.2201  Pearson:0.9109 Spearman:0.9029\n",
      "Train ALL Pearson: 0.8999030744565961\n",
      "Train  ALL Spearman: 0.8666414783022739\n",
      "31.86451768875122\n",
      "Test Epoch:50 [(0%)]\t Loss: 0.3146  Pearson:0.9374 Spearman:0.9414\n",
      "Test : Loss:0.3073 \n",
      "pearson： 0.9227036789542684 ALL Pearson: 0.9215745798228507\n",
      "spearman： 0.9147875713366023 ALL Spearman: 0.9066324550684688\n",
      "time: 36.24573850631714\n",
      "0.03\n",
      "Train Epoch:51 [(0%)]\t Loss: 0.1837  Pearson:0.9180 Spearman:0.9011\n",
      "Train ALL Pearson: 0.910405881316702\n",
      "Train  ALL Spearman: 0.8835887633927062\n",
      "32.193780183792114\n",
      "Test Epoch:51 [(0%)]\t Loss: 0.2770  Pearson:0.9214 Spearman:0.8821\n",
      "Test : Loss:0.2879 \n",
      "pearson： 0.9210871544961671 ALL Pearson: 0.9237919331502383\n",
      "spearman： 0.9004613808244041 ALL Spearman: 0.9087952800020535\n",
      "time: 36.7133309841156\n",
      "0.03\n",
      "Train Epoch:52 [(0%)]\t Loss: 0.1703  Pearson:0.9428 Spearman:0.9277\n",
      "Train ALL Pearson: 0.905031829241119\n",
      "Train  ALL Spearman: 0.8773225958254719\n",
      "31.4504714012146\n",
      "Test Epoch:52 [(0%)]\t Loss: 0.3136  Pearson:0.9248 Spearman:0.8946\n",
      "Test : Loss:0.3090 \n",
      "pearson： 0.9267794522703305 ALL Pearson: 0.9242838678063356\n",
      "spearman： 0.9050436289621611 ALL Spearman: 0.9074756888212219\n",
      "time: 36.028377056121826\n",
      "0.03\n",
      "Train Epoch:53 [(0%)]\t Loss: 0.1913  Pearson:0.9186 Spearman:0.8820\n",
      "Train ALL Pearson: 0.9062335291294392\n",
      "Train  ALL Spearman: 0.8763209865927069\n",
      "32.14928126335144\n",
      "Test Epoch:53 [(0%)]\t Loss: 0.2794  Pearson:0.9290 Spearman:0.8951\n",
      "Test : Loss:0.2810 \n",
      "pearson： 0.9248498258299342 ALL Pearson: 0.9239875258365792\n",
      "spearman： 0.9012842487989883 ALL Spearman: 0.906891378443745\n",
      "time: 36.424909830093384\n",
      "0.03\n",
      "Train Epoch:54 [(0%)]\t Loss: 0.1688  Pearson:0.9287 Spearman:0.9049\n",
      "Train ALL Pearson: 0.9093685230851166\n",
      "Train  ALL Spearman: 0.8809433498740138\n",
      "32.1097457408905\n",
      "Test Epoch:54 [(0%)]\t Loss: 0.2548  Pearson:0.9210 Spearman:0.9092\n",
      "Test : Loss:0.2613 \n",
      "pearson： 0.9243763344107289 ALL Pearson: 0.9251170411198819\n",
      "spearman： 0.9105611819676497 ALL Spearman: 0.9095195617725265\n",
      "time: 36.71826696395874\n",
      "0.03\n",
      "Train Epoch:55 [(0%)]\t Loss: 0.1667  Pearson:0.9442 Spearman:0.9291\n",
      "Train ALL Pearson: 0.9174122410576512\n",
      "Train  ALL Spearman: 0.8944219687314425\n",
      "31.815898895263672\n",
      "Test Epoch:55 [(0%)]\t Loss: 0.2516  Pearson:0.9102 Spearman:0.9011\n",
      "Test : Loss:0.2444 \n",
      "pearson： 0.9245706697547021 ALL Pearson: 0.9277583349364603\n",
      "spearman： 0.9116522601472941 ALL Spearman: 0.9120341848292612\n",
      "time: 36.57757520675659\n",
      "0.03\n",
      "Train Epoch:56 [(0%)]\t Loss: 0.1538  Pearson:0.9384 Spearman:0.9360\n",
      "Train ALL Pearson: 0.9135653761669618\n",
      "Train  ALL Spearman: 0.8877943780830954\n",
      "32.176735162734985\n",
      "Test Epoch:56 [(0%)]\t Loss: 0.2225  Pearson:0.9400 Spearman:0.9274\n",
      "Test : Loss:0.2368 \n",
      "pearson： 0.9295503240817695 ALL Pearson: 0.9271859059444522\n",
      "spearman： 0.9144274782180268 ALL Spearman: 0.9123417368712052\n",
      "time: 36.60387635231018\n",
      "0.03\n",
      "Train Epoch:57 [(0%)]\t Loss: 0.1714  Pearson:0.9223 Spearman:0.9118\n",
      "Train ALL Pearson: 0.9186073009440352\n",
      "Train  ALL Spearman: 0.8937501218255751\n",
      "32.03818964958191\n",
      "Test Epoch:57 [(0%)]\t Loss: 0.1784  Pearson:0.9350 Spearman:0.9173\n",
      "Test : Loss:0.1935 \n",
      "pearson： 0.9279010266872924 ALL Pearson: 0.9283221616260646\n",
      "spearman： 0.910488980347251 ALL Spearman: 0.913635075473984\n",
      "time: 36.574169874191284\n",
      "0.03\n",
      "Train Epoch:58 [(0%)]\t Loss: 0.1452  Pearson:0.9501 Spearman:0.9416\n",
      "Train ALL Pearson: 0.9210640947525129\n",
      "Train  ALL Spearman: 0.8972198258023425\n",
      "31.95603656768799\n",
      "Test Epoch:58 [(0%)]\t Loss: 0.1500  Pearson:0.9283 Spearman:0.9050\n",
      "Test : Loss:0.1643 \n",
      "pearson： 0.934084452361845 ALL Pearson: 0.9287752787577062\n",
      "spearman： 0.9165257836820765 ALL Spearman: 0.914092990742228\n",
      "time: 36.31063985824585\n",
      "0.03\n",
      "Train Epoch:59 [(0%)]\t Loss: 0.1726  Pearson:0.9298 Spearman:0.9106\n",
      "Train ALL Pearson: 0.9224314437561676\n",
      "Train  ALL Spearman: 0.8995025021832321\n",
      "32.01713466644287\n",
      "Test Epoch:59 [(0%)]\t Loss: 0.2894  Pearson:0.9365 Spearman:0.9125\n",
      "Test : Loss:0.2991 \n",
      "pearson： 0.9314854212062328 ALL Pearson: 0.9291662007061915\n",
      "spearman： 0.9176377388099549 ALL Spearman: 0.915520029943908\n",
      "time: 36.524688959121704\n",
      "0.03\n",
      "Train Epoch:60 [(0%)]\t Loss: 0.1922  Pearson:0.9568 Spearman:0.9491\n",
      "Train ALL Pearson: 0.9235854518956705\n",
      "Train  ALL Spearman: 0.9012247571998565\n",
      "31.632347345352173\n",
      "Test Epoch:60 [(0%)]\t Loss: 0.2014  Pearson:0.9345 Spearman:0.9142\n",
      "Test : Loss:0.2230 \n",
      "pearson： 0.9311917341753297 ALL Pearson: 0.9298318147336506\n",
      "spearman： 0.9103040788921473 ALL Spearman: 0.9148398296428056\n",
      "time: 36.20487999916077\n",
      "0.03\n",
      "Train Epoch:61 [(0%)]\t Loss: 0.1446  Pearson:0.9337 Spearman:0.8989\n",
      "Train ALL Pearson: 0.921690048946463\n",
      "Train  ALL Spearman: 0.8959391831009702\n",
      "32.16624093055725\n",
      "Test Epoch:61 [(0%)]\t Loss: 0.1953  Pearson:0.9449 Spearman:0.9309\n",
      "Test : Loss:0.2063 \n",
      "pearson： 0.9314676283909707 ALL Pearson: 0.9308716450465088\n",
      "spearman： 0.9190269976137059 ALL Spearman: 0.9166310129110985\n",
      "time: 36.65534806251526\n",
      "0.03\n",
      "Train Epoch:62 [(0%)]\t Loss: 0.1502  Pearson:0.9367 Spearman:0.9204\n",
      "Train ALL Pearson: 0.9171872939850633\n",
      "Train  ALL Spearman: 0.8922842219390331\n",
      "31.64969515800476\n",
      "Test Epoch:62 [(0%)]\t Loss: 0.2096  Pearson:0.9208 Spearman:0.8919\n",
      "Test : Loss:0.2086 \n",
      "pearson： 0.9272920110685923 ALL Pearson: 0.9293445161087304\n",
      "spearman： 0.9072613628999947 ALL Spearman: 0.9139661211389146\n",
      "time: 36.12736773490906\n",
      "0.03\n",
      "Train Epoch:63 [(0%)]\t Loss: 0.1398  Pearson:0.9543 Spearman:0.9401\n",
      "Train ALL Pearson: 0.9227158274534806\n",
      "Train  ALL Spearman: 0.8993660992656799\n",
      "31.972357988357544\n",
      "Test Epoch:63 [(0%)]\t Loss: 0.2437  Pearson:0.9189 Spearman:0.8881\n",
      "Test : Loss:0.2345 \n",
      "pearson： 0.9246788711558778 ALL Pearson: 0.9299628545195809\n",
      "spearman： 0.9091809334365325 ALL Spearman: 0.915859257607353\n",
      "time: 36.34295630455017\n",
      "0.03\n",
      "Train Epoch:64 [(0%)]\t Loss: 0.1647  Pearson:0.9381 Spearman:0.9134\n",
      "Train ALL Pearson: 0.9264471485838502\n",
      "Train  ALL Spearman: 0.9049367560128787\n",
      "32.4221773147583\n",
      "Test Epoch:64 [(0%)]\t Loss: 0.2096  Pearson:0.9200 Spearman:0.9107\n",
      "Test : Loss:0.2019 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9264530143076801 ALL Pearson: 0.9292897994551969\n",
      "spearman： 0.9156451145456227 ALL Spearman: 0.914624570317034\n",
      "time: 37.10667538642883\n",
      "0.03\n",
      "Train Epoch:65 [(0%)]\t Loss: 0.1444  Pearson:0.9333 Spearman:0.9165\n",
      "Train ALL Pearson: 0.9340621789589505\n",
      "Train  ALL Spearman: 0.9142258859104878\n",
      "32.12724184989929\n",
      "Test Epoch:65 [(0%)]\t Loss: 0.1670  Pearson:0.9274 Spearman:0.9132\n",
      "Test : Loss:0.1599 \n",
      "pearson： 0.929193371892275 ALL Pearson: 0.93108391736382\n",
      "spearman： 0.9143326303206056 ALL Spearman: 0.9159960350828953\n",
      "time: 36.54982352256775\n",
      "0.03\n",
      "Train Epoch:66 [(0%)]\t Loss: 0.1588  Pearson:0.9390 Spearman:0.9197\n",
      "Train ALL Pearson: 0.9323825981709944\n",
      "Train  ALL Spearman: 0.9109173267156241\n",
      "32.073084354400635\n",
      "Test Epoch:66 [(0%)]\t Loss: 0.1452  Pearson:0.9331 Spearman:0.9115\n",
      "Test : Loss:0.1549 \n",
      "pearson： 0.9319890421173518 ALL Pearson: 0.9319807559632433\n",
      "spearman： 0.9150759490133336 ALL Spearman: 0.9162747276350665\n",
      "time: 36.41068482398987\n",
      "0.03\n",
      "Train Epoch:67 [(0%)]\t Loss: 0.1680  Pearson:0.9476 Spearman:0.9379\n",
      "Train ALL Pearson: 0.9233534401750275\n",
      "Train  ALL Spearman: 0.8998108758734024\n",
      "31.955893754959106\n",
      "Test Epoch:67 [(0%)]\t Loss: 0.1398  Pearson:0.9491 Spearman:0.9273\n",
      "Test : Loss:0.1535 \n",
      "pearson： 0.9314093792743077 ALL Pearson: 0.9308149547602279\n",
      "spearman： 0.9160747856394345 ALL Spearman: 0.9155419494703269\n",
      "time: 36.4135947227478\n",
      "0.03\n",
      "Train Epoch:68 [(0%)]\t Loss: 0.1867  Pearson:0.9536 Spearman:0.9329\n",
      "Train ALL Pearson: 0.9308558969043412\n",
      "Train  ALL Spearman: 0.9092283937680536\n",
      "31.838029861450195\n",
      "Test Epoch:68 [(0%)]\t Loss: 0.1574  Pearson:0.9448 Spearman:0.9316\n",
      "Test : Loss:0.1621 \n",
      "pearson： 0.9333939315377641 ALL Pearson: 0.9310078787798919\n",
      "spearman： 0.9187166065339635 ALL Spearman: 0.9155740727866297\n",
      "time: 36.25861120223999\n",
      "0.03\n",
      "Train Epoch:69 [(0%)]\t Loss: 0.1634  Pearson:0.9476 Spearman:0.9051\n",
      "Train ALL Pearson: 0.9290743644752363\n",
      "Train  ALL Spearman: 0.9086103640854216\n",
      "31.774401426315308\n",
      "Test Epoch:69 [(0%)]\t Loss: 0.2063  Pearson:0.9415 Spearman:0.9333\n",
      "Test : Loss:0.2064 \n",
      "pearson： 0.9337547667099803 ALL Pearson: 0.9315149443606512\n",
      "spearman： 0.9212992964350261 ALL Spearman: 0.9172058106741797\n",
      "time: 36.19598364830017\n",
      "0.03\n",
      "Train Epoch:70 [(0%)]\t Loss: 0.1511  Pearson:0.9466 Spearman:0.9273\n",
      "Train ALL Pearson: 0.9327219099813109\n",
      "Train  ALL Spearman: 0.91023123312328\n",
      "32.06527543067932\n",
      "Test Epoch:70 [(0%)]\t Loss: 0.1714  Pearson:0.9328 Spearman:0.8944\n",
      "Test : Loss:0.1875 \n",
      "pearson： 0.9335543647824326 ALL Pearson: 0.9315147355877503\n",
      "spearman： 0.9151459581187681 ALL Spearman: 0.917241198948131\n",
      "time: 36.765562772750854\n",
      "0.03\n",
      "Train Epoch:71 [(0%)]\t Loss: 0.1345  Pearson:0.9504 Spearman:0.9242\n",
      "Train ALL Pearson: 0.939762512439298\n",
      "Train  ALL Spearman: 0.9224830096373986\n",
      "32.01309370994568\n",
      "Test Epoch:71 [(0%)]\t Loss: 0.1655  Pearson:0.9268 Spearman:0.9291\n",
      "Test : Loss:0.1563 \n",
      "pearson： 0.9310056216191701 ALL Pearson: 0.9331703003601994\n",
      "spearman： 0.9198759084091473 ALL Spearman: 0.9192009735541289\n",
      "time: 36.44667148590088\n",
      "0.03\n",
      "Train Epoch:72 [(0%)]\t Loss: 0.1555  Pearson:0.9283 Spearman:0.9007\n",
      "Train ALL Pearson: 0.9363077939521887\n",
      "Train  ALL Spearman: 0.9175150886105826\n",
      "32.295494556427\n",
      "Test Epoch:72 [(0%)]\t Loss: 0.2042  Pearson:0.9519 Spearman:0.9390\n",
      "Test : Loss:0.2079 \n",
      "pearson： 0.9336104101506583 ALL Pearson: 0.9337991153808194\n",
      "spearman： 0.9217193348397344 ALL Spearman: 0.9197449277412608\n",
      "time: 36.959718227386475\n",
      "0.03\n",
      "Train Epoch:73 [(0%)]\t Loss: 0.1352  Pearson:0.9604 Spearman:0.9490\n",
      "Train ALL Pearson: 0.9378700460932626\n",
      "Train  ALL Spearman: 0.9197585934356969\n",
      "32.14981245994568\n",
      "Test Epoch:73 [(0%)]\t Loss: 0.1650  Pearson:0.9146 Spearman:0.8891\n",
      "Test : Loss:0.1620 \n",
      "pearson： 0.929369769444241 ALL Pearson: 0.9328080271752199\n",
      "spearman： 0.9095823382658111 ALL Spearman: 0.9172081303107188\n",
      "time: 36.59937262535095\n",
      "0.03\n",
      "Train Epoch:74 [(0%)]\t Loss: 0.1348  Pearson:0.9428 Spearman:0.9200\n",
      "Train ALL Pearson: 0.9322997418350604\n",
      "Train  ALL Spearman: 0.9116825363195401\n",
      "31.555381298065186\n",
      "Test Epoch:74 [(0%)]\t Loss: 0.1604  Pearson:0.9318 Spearman:0.9017\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.9329464399960524 ALL Pearson: 0.9323952834279893\n",
      "spearman： 0.9130068902203157 ALL Spearman: 0.9174015999475761\n",
      "time: 35.94066286087036\n",
      "0.03\n",
      "Train Epoch:75 [(0%)]\t Loss: 0.1335  Pearson:0.9436 Spearman:0.9309\n",
      "Train ALL Pearson: 0.9356866839835684\n",
      "Train  ALL Spearman: 0.9161941031417173\n",
      "31.66439127922058\n",
      "Test Epoch:75 [(0%)]\t Loss: 0.2166  Pearson:0.9473 Spearman:0.9289\n",
      "Test : Loss:0.2196 \n",
      "pearson： 0.9391136570214145 ALL Pearson: 0.9334303855611877\n",
      "spearman： 0.9239766927648154 ALL Spearman: 0.9205384573029488\n",
      "time: 36.30088543891907\n",
      "0.03\n",
      "Train Epoch:76 [(0%)]\t Loss: 0.1411  Pearson:0.9468 Spearman:0.9209\n",
      "Train ALL Pearson: 0.9344668103211569\n",
      "Train  ALL Spearman: 0.9168076103656828\n",
      "32.28766202926636\n",
      "Test Epoch:76 [(0%)]\t Loss: 0.1929  Pearson:0.9320 Spearman:0.9244\n",
      "Test : Loss:0.2049 \n",
      "pearson： 0.9323429784147229 ALL Pearson: 0.9322494657003091\n",
      "spearman： 0.9195134880884769 ALL Spearman: 0.9192673645768733\n",
      "time: 36.77022385597229\n",
      "0.03\n",
      "Train Epoch:77 [(0%)]\t Loss: 0.1399  Pearson:0.9455 Spearman:0.9405\n",
      "Train ALL Pearson: 0.9402442124151041\n",
      "Train  ALL Spearman: 0.9218202203139698\n",
      "31.778958559036255\n",
      "Test Epoch:77 [(0%)]\t Loss: 0.2334  Pearson:0.9386 Spearman:0.9248\n",
      "Test : Loss:0.2316 \n",
      "pearson： 0.9353023842164115 ALL Pearson: 0.9333436027073647\n",
      "spearman： 0.923726760214421 ALL Spearman: 0.920887326653048\n",
      "time: 36.12856459617615\n",
      "0.03\n",
      "Train Epoch:78 [(0%)]\t Loss: 0.1467  Pearson:0.9644 Spearman:0.9552\n",
      "Train ALL Pearson: 0.9318119379135442\n",
      "Train  ALL Spearman: 0.9107786936256526\n",
      "32.205788135528564\n",
      "Test Epoch:78 [(0%)]\t Loss: 0.2111  Pearson:0.9258 Spearman:0.9223\n",
      "Test : Loss:0.2055 \n",
      "pearson： 0.9318755378588754 ALL Pearson: 0.933079592812754\n",
      "spearman： 0.9184670790195092 ALL Spearman: 0.9180700809081562\n",
      "time: 36.589383602142334\n",
      "0.03\n",
      "Train Epoch:79 [(0%)]\t Loss: 0.1302  Pearson:0.9546 Spearman:0.9463\n",
      "Train ALL Pearson: 0.94006631719142\n",
      "Train  ALL Spearman: 0.9223515286565006\n",
      "32.09446978569031\n",
      "Test Epoch:79 [(0%)]\t Loss: 0.1796  Pearson:0.9245 Spearman:0.9145\n",
      "Test : Loss:0.1700 \n",
      "pearson： 0.9344450313346905 ALL Pearson: 0.9342979992741233\n",
      "spearman： 0.919972080236784 ALL Spearman: 0.9206617337011544\n",
      "time: 36.57003426551819\n",
      "0.03\n",
      "Train Epoch:80 [(0%)]\t Loss: 0.1305  Pearson:0.9505 Spearman:0.9357\n",
      "Train ALL Pearson: 0.9425608795507624\n",
      "Train  ALL Spearman: 0.9245713527917626\n",
      "32.0426287651062\n",
      "Test Epoch:80 [(0%)]\t Loss: 0.1559  Pearson:0.9224 Spearman:0.9225\n",
      "Test : Loss:0.1532 \n",
      "pearson： 0.9324928133677856 ALL Pearson: 0.9343793827266428\n",
      "spearman： 0.9208995885141144 ALL Spearman: 0.9207974294187519\n",
      "time: 36.62116074562073\n",
      "0.03\n",
      "Train Epoch:81 [(0%)]\t Loss: 0.1482  Pearson:0.9486 Spearman:0.9318\n",
      "Train ALL Pearson: 0.9397341050962097\n",
      "Train  ALL Spearman: 0.9225273774458476\n",
      "32.43272376060486\n",
      "Test Epoch:81 [(0%)]\t Loss: 0.1408  Pearson:0.9430 Spearman:0.9042\n",
      "Test : Loss:0.1551 \n",
      "pearson： 0.9365075168056491 ALL Pearson: 0.9342623208320414\n",
      "spearman： 0.9203431812806105 ALL Spearman: 0.9203898479419382\n",
      "time: 36.816317081451416\n",
      "0.03\n",
      "Train Epoch:82 [(0%)]\t Loss: 0.1388  Pearson:0.9491 Spearman:0.9434\n",
      "Train ALL Pearson: 0.9418581819289024\n",
      "Train  ALL Spearman: 0.9238286835196141\n",
      "32.19799017906189\n",
      "Test Epoch:82 [(0%)]\t Loss: 0.2215  Pearson:0.9428 Spearman:0.9295\n",
      "Test : Loss:0.2283 \n",
      "pearson： 0.9329778602988964 ALL Pearson: 0.9335229427620577\n",
      "spearman： 0.9209717390498464 ALL Spearman: 0.9204077643068822\n",
      "time: 36.508606910705566\n",
      "0.03\n",
      "Train Epoch:83 [(0%)]\t Loss: 0.1595  Pearson:0.9650 Spearman:0.9396\n",
      "Train ALL Pearson: 0.9334727999436138\n",
      "Train  ALL Spearman: 0.9124001149393892\n",
      "31.6950044631958\n",
      "Test Epoch:83 [(0%)]\t Loss: 0.2350  Pearson:0.9145 Spearman:0.8733\n",
      "Test : Loss:0.2196 \n",
      "pearson： 0.9328727131705381 ALL Pearson: 0.933790597036267\n",
      "spearman： 0.9129638469631342 ALL Spearman: 0.9206130617944209\n",
      "time: 36.20515751838684\n",
      "0.03\n",
      "Train Epoch:84 [(0%)]\t Loss: 0.1385  Pearson:0.9638 Spearman:0.9565\n",
      "Train ALL Pearson: 0.9389990249375174\n",
      "Train  ALL Spearman: 0.922013494430057\n",
      "32.28534555435181\n",
      "Test Epoch:84 [(0%)]\t Loss: 0.1545  Pearson:0.9295 Spearman:0.9241\n",
      "Test : Loss:0.1539 \n",
      "pearson： 0.9294400798476933 ALL Pearson: 0.9325762057745544\n",
      "spearman： 0.9177932342053174 ALL Spearman: 0.9178455671474104\n",
      "time: 36.81216740608215\n",
      "0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:85 [(0%)]\t Loss: 0.1451  Pearson:0.9597 Spearman:0.9391\n",
      "Train ALL Pearson: 0.9458059883220442\n",
      "Train  ALL Spearman: 0.9302121605487229\n",
      "31.517463207244873\n",
      "Test Epoch:85 [(0%)]\t Loss: 0.2197  Pearson:0.9362 Spearman:0.9293\n",
      "Test : Loss:0.2031 \n",
      "pearson： 0.9354703255103742 ALL Pearson: 0.9333007895584639\n",
      "spearman： 0.9235723519247565 ALL Spearman: 0.9200470638046905\n",
      "time: 36.21795654296875\n",
      "0.03\n",
      "Train Epoch:86 [(0%)]\t Loss: 0.1278  Pearson:0.9579 Spearman:0.9416\n",
      "Train ALL Pearson: 0.939314791770635\n",
      "Train  ALL Spearman: 0.9209675859965728\n",
      "32.030272483825684\n",
      "Test Epoch:86 [(0%)]\t Loss: 0.2303  Pearson:0.9297 Spearman:0.9194\n",
      "Test : Loss:0.2282 \n",
      "pearson： 0.9315703035132151 ALL Pearson: 0.9320049697430574\n",
      "spearman： 0.9161495475060742 ALL Spearman: 0.9195037051185136\n",
      "time: 36.37987756729126\n",
      "0.03\n",
      "Train Epoch:87 [(0%)]\t Loss: 0.1538  Pearson:0.9602 Spearman:0.9502\n",
      "Train ALL Pearson: 0.9416415346073118\n",
      "Train  ALL Spearman: 0.9223634944181237\n",
      "32.16376566886902\n",
      "Test Epoch:87 [(0%)]\t Loss: 0.1612  Pearson:0.9347 Spearman:0.9101\n",
      "Test : Loss:0.1758 \n",
      "pearson： 0.9349326441953913 ALL Pearson: 0.9320107140643831\n",
      "spearman： 0.9209898825719708 ALL Spearman: 0.9185669762490501\n",
      "time: 36.64532661437988\n",
      "0.03\n",
      "Train Epoch:88 [(0%)]\t Loss: 0.1254  Pearson:0.9569 Spearman:0.9501\n",
      "Train ALL Pearson: 0.9463684072618117\n",
      "Train  ALL Spearman: 0.929727877142834\n",
      "32.05942440032959\n",
      "Test Epoch:88 [(0%)]\t Loss: 0.2060  Pearson:0.9269 Spearman:0.9153\n",
      "Test : Loss:0.1901 \n",
      "pearson： 0.9322314053215913 ALL Pearson: 0.9325333079792549\n",
      "spearman： 0.9156174012687793 ALL Spearman: 0.919940854155887\n",
      "time: 36.550983905792236\n",
      "0.03\n",
      "Train Epoch:89 [(0%)]\t Loss: 0.1306  Pearson:0.9502 Spearman:0.9525\n",
      "Train ALL Pearson: 0.9428231003740349\n",
      "Train  ALL Spearman: 0.9246026002015579\n",
      "32.22216439247131\n",
      "Test Epoch:89 [(0%)]\t Loss: 0.1648  Pearson:0.9418 Spearman:0.9227\n",
      "Test : Loss:0.1638 \n",
      "pearson： 0.9349558343833528 ALL Pearson: 0.9335554318768099\n",
      "spearman： 0.9208744014027659 ALL Spearman: 0.9202186795236054\n",
      "time: 37.02062511444092\n",
      "0.03\n",
      "Train Epoch:90 [(0%)]\t Loss: 0.1045  Pearson:0.9714 Spearman:0.9616\n",
      "Train ALL Pearson: 0.9397609863989603\n",
      "Train  ALL Spearman: 0.9211938409866096\n",
      "32.2719931602478\n",
      "Test Epoch:90 [(0%)]\t Loss: 0.1727  Pearson:0.9415 Spearman:0.9303\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.932472710525041 ALL Pearson: 0.9328970857576282\n",
      "spearman： 0.9186706489906652 ALL Spearman: 0.9184744564919406\n",
      "time: 36.733562707901\n",
      "0.03\n",
      "Train Epoch:91 [(0%)]\t Loss: 0.1573  Pearson:0.9592 Spearman:0.9352\n",
      "Train ALL Pearson: 0.9429648717225505\n",
      "Train  ALL Spearman: 0.9254124570586162\n",
      "32.12332057952881\n",
      "Test Epoch:91 [(0%)]\t Loss: 0.1467  Pearson:0.9470 Spearman:0.9433\n",
      "Test : Loss:0.1515 \n",
      "pearson： 0.9374440994797818 ALL Pearson: 0.9338863994268913\n",
      "spearman： 0.9261579741449907 ALL Spearman: 0.9200584105913631\n",
      "time: 36.485920429229736\n",
      "0.03\n",
      "Train Epoch:92 [(0%)]\t Loss: 0.1535  Pearson:0.9483 Spearman:0.9264\n",
      "Train ALL Pearson: 0.9438340646174249\n",
      "Train  ALL Spearman: 0.9275269076081663\n",
      "32.52453064918518\n",
      "Test Epoch:92 [(0%)]\t Loss: 0.1606  Pearson:0.9256 Spearman:0.9117\n",
      "Test : Loss:0.1591 \n",
      "pearson： 0.9319900923902931 ALL Pearson: 0.932681711396538\n",
      "spearman： 0.916432540445475 ALL Spearman: 0.9184734547308196\n",
      "time: 37.148581743240356\n",
      "0.03\n",
      "Train Epoch:93 [(0%)]\t Loss: 0.1391  Pearson:0.9550 Spearman:0.9354\n",
      "Train ALL Pearson: 0.940816005675635\n",
      "Train  ALL Spearman: 0.9227248527905177\n",
      "32.04475688934326\n",
      "Test Epoch:93 [(0%)]\t Loss: 0.1922  Pearson:0.9379 Spearman:0.9264\n",
      "Test : Loss:0.1927 \n",
      "pearson： 0.9296215628644791 ALL Pearson: 0.9327276121692492\n",
      "spearman： 0.9196141548687844 ALL Spearman: 0.9183017241794165\n",
      "time: 36.48138189315796\n",
      "0.03\n",
      "Train Epoch:94 [(0%)]\t Loss: 0.1328  Pearson:0.9604 Spearman:0.9461\n",
      "Train ALL Pearson: 0.9429822551599936\n",
      "Train  ALL Spearman: 0.9247117243833324\n",
      "31.937904357910156\n",
      "Test Epoch:94 [(0%)]\t Loss: 0.1869  Pearson:0.9357 Spearman:0.9043\n",
      "Test : Loss:0.1957 \n",
      "pearson： 0.9288621812109763 ALL Pearson: 0.9327786759317058\n",
      "spearman： 0.9137972473690241 ALL Spearman: 0.9200090896181661\n",
      "time: 36.37648153305054\n",
      "0.03\n",
      "Train Epoch:95 [(0%)]\t Loss: 0.1173  Pearson:0.9681 Spearman:0.9492\n",
      "Train ALL Pearson: 0.9419487979358437\n",
      "Train  ALL Spearman: 0.9243993272336413\n",
      "32.0817551612854\n",
      "Test Epoch:95 [(0%)]\t Loss: 0.1532  Pearson:0.9480 Spearman:0.9432\n",
      "Test : Loss:0.1683 \n",
      "pearson： 0.9359335417915784 ALL Pearson: 0.9335910676513791\n",
      "spearman： 0.9223336998816017 ALL Spearman: 0.9204299833862397\n",
      "time: 36.68927788734436\n",
      "0.03\n",
      "Train Epoch:96 [(0%)]\t Loss: 0.1238  Pearson:0.9540 Spearman:0.9380\n",
      "Train ALL Pearson: 0.943461452629245\n",
      "Train  ALL Spearman: 0.9246249414582506\n",
      "31.59643578529358\n",
      "Test Epoch:96 [(0%)]\t Loss: 0.2084  Pearson:0.9407 Spearman:0.9430\n",
      "Test : Loss:0.1999 \n",
      "pearson： 0.9332681539437702 ALL Pearson: 0.9329541958720807\n",
      "spearman： 0.9237080916842489 ALL Spearman: 0.9197930627873545\n",
      "time: 36.066521883010864\n",
      "0.03\n",
      "Train Epoch:97 [(0%)]\t Loss: 0.1333  Pearson:0.9631 Spearman:0.9554\n",
      "Train ALL Pearson: 0.9450428380571754\n",
      "Train  ALL Spearman: 0.9295157556181347\n",
      "32.047961711883545\n",
      "Test Epoch:97 [(0%)]\t Loss: 0.2137  Pearson:0.9354 Spearman:0.8953\n",
      "Test : Loss:0.2079 \n",
      "pearson： 0.9344883420302037 ALL Pearson: 0.9327843796187014\n",
      "spearman： 0.9154478037783822 ALL Spearman: 0.919894298008553\n",
      "time: 36.61535882949829\n",
      "0.03\n",
      "Train Epoch:98 [(0%)]\t Loss: 0.1283  Pearson:0.9665 Spearman:0.9447\n",
      "Train ALL Pearson: 0.9415124857839193\n",
      "Train  ALL Spearman: 0.9235210233955943\n",
      "32.397040128707886\n",
      "Test Epoch:98 [(0%)]\t Loss: 0.2085  Pearson:0.9425 Spearman:0.9357\n",
      "Test : Loss:0.2088 \n",
      "pearson： 0.9322452928816776 ALL Pearson: 0.9324202961002908\n",
      "spearman： 0.9199541797801072 ALL Spearman: 0.918495685544186\n",
      "time: 36.763169050216675\n",
      "0.03\n",
      "Train Epoch:99 [(0%)]\t Loss: 0.1510  Pearson:0.9533 Spearman:0.9452\n",
      "Train ALL Pearson: 0.950981061508166\n",
      "Train  ALL Spearman: 0.936610722112689\n",
      "32.219664096832275\n",
      "Test Epoch:99 [(0%)]\t Loss: 0.1592  Pearson:0.9314 Spearman:0.9069\n",
      "Test : Loss:0.1536 \n",
      "pearson： 0.9357539223973341 ALL Pearson: 0.9338276027755393\n",
      "spearman： 0.9176620481532871 ALL Spearman: 0.9187446738807437\n",
      "time: 36.85717725753784\n",
      "0.03\n",
      "Train Epoch:100 [(0%)]\t Loss: 0.1563  Pearson:0.9517 Spearman:0.9340\n",
      "Train ALL Pearson: 0.9451543486285295\n",
      "Train  ALL Spearman: 0.9281305795340903\n",
      "31.87376832962036\n",
      "Test Epoch:100 [(0%)]\t Loss: 0.1675  Pearson:0.9330 Spearman:0.9152\n",
      "Test : Loss:0.1563 \n",
      "pearson： 0.9313645771235634 ALL Pearson: 0.9337382252533577\n",
      "spearman： 0.9128187936118838 ALL Spearman: 0.9182752013155399\n",
      "time: 36.64324164390564\n",
      "0.03\n",
      "Train Epoch:101 [(0%)]\t Loss: 0.1175  Pearson:0.9659 Spearman:0.9526\n",
      "Train ALL Pearson: 0.9444539420367569\n",
      "Train  ALL Spearman: 0.9292168597120045\n",
      "32.129151821136475\n",
      "Test Epoch:101 [(0%)]\t Loss: 0.1463  Pearson:0.9131 Spearman:0.9007\n",
      "Test : Loss:0.1544 \n",
      "pearson： 0.9268700170290636 ALL Pearson: 0.9336732405130109\n",
      "spearman： 0.9145008105233167 ALL Spearman: 0.9188353018359379\n",
      "time: 36.44376802444458\n",
      "0.009\n",
      "Train Epoch:102 [(0%)]\t Loss: 0.1636  Pearson:0.9515 Spearman:0.9538\n",
      "Train ALL Pearson: 0.9558302639946957\n",
      "Train  ALL Spearman: 0.943179953360389\n",
      "32.02035737037659\n",
      "Test Epoch:102 [(0%)]\t Loss: 0.1796  Pearson:0.9223 Spearman:0.9132\n",
      "Test : Loss:0.1687 \n",
      "pearson： 0.9308849609570321 ALL Pearson: 0.9352546647516594\n",
      "spearman： 0.9201730867846795 ALL Spearman: 0.9222327063189778\n",
      "time: 36.478928089141846\n",
      "0.009\n",
      "Train Epoch:103 [(0%)]\t Loss: 0.1150  Pearson:0.9608 Spearman:0.9431\n",
      "Train ALL Pearson: 0.9576353232801412\n",
      "Train  ALL Spearman: 0.9452239802535509\n",
      "31.85423183441162\n",
      "Test Epoch:103 [(0%)]\t Loss: 0.1750  Pearson:0.9140 Spearman:0.9018\n",
      "Test : Loss:0.1658 \n",
      "pearson： 0.9323463690139427 ALL Pearson: 0.9359058963589456\n",
      "spearman： 0.9186898539389913 ALL Spearman: 0.9224497480827428\n",
      "time: 36.36359715461731\n",
      "0.009\n",
      "Train Epoch:104 [(0%)]\t Loss: 0.1173  Pearson:0.9560 Spearman:0.9440\n",
      "Train ALL Pearson: 0.957567729267461\n",
      "Train  ALL Spearman: 0.945614656580623\n",
      "31.851170301437378\n",
      "Test Epoch:104 [(0%)]\t Loss: 0.1836  Pearson:0.9353 Spearman:0.9221\n",
      "Test : Loss:0.1749 \n",
      "pearson： 0.9318501527574473 ALL Pearson: 0.9356547156474596\n",
      "spearman： 0.9174024731999304 ALL Spearman: 0.9223243775306691\n",
      "time: 36.27829670906067\n",
      "0.009\n",
      "Train Epoch:105 [(0%)]\t Loss: 0.1150  Pearson:0.9574 Spearman:0.9444\n",
      "Train ALL Pearson: 0.9577564488643538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  ALL Spearman: 0.9447946852796263\n",
      "32.38521981239319\n",
      "Test Epoch:105 [(0%)]\t Loss: 0.1804  Pearson:0.9262 Spearman:0.9010\n",
      "Test : Loss:0.1664 \n",
      "pearson： 0.9339970490201732 ALL Pearson: 0.9360374967772979\n",
      "spearman： 0.9188798241692528 ALL Spearman: 0.9226757234325444\n",
      "time: 37.0157356262207\n",
      "0.009\n",
      "Train Epoch:106 [(0%)]\t Loss: 0.1307  Pearson:0.9554 Spearman:0.9421\n",
      "Train ALL Pearson: 0.9580320799490394\n",
      "Train  ALL Spearman: 0.9461246567928714\n",
      "32.352569818496704\n",
      "Test Epoch:106 [(0%)]\t Loss: 0.1830  Pearson:0.9407 Spearman:0.9404\n",
      "Test : Loss:0.1687 \n",
      "pearson： 0.9357514638726818 ALL Pearson: 0.9359674506672517\n",
      "spearman： 0.9254320760858665 ALL Spearman: 0.9223660075943629\n",
      "time: 37.00434350967407\n",
      "0.009\n",
      "Train Epoch:107 [(0%)]\t Loss: 0.1150  Pearson:0.9615 Spearman:0.9432\n",
      "Train ALL Pearson: 0.9595069793710058\n",
      "Train  ALL Spearman: 0.9482106640429362\n",
      "32.13293719291687\n",
      "Test Epoch:107 [(0%)]\t Loss: 0.1750  Pearson:0.9360 Spearman:0.9153\n",
      "Test : Loss:0.1684 \n",
      "pearson： 0.937078236113659 ALL Pearson: 0.9358495561153128\n",
      "spearman： 0.9200034104161182 ALL Spearman: 0.9221758455116428\n",
      "time: 36.62819814682007\n",
      "0.009\n",
      "Train Epoch:108 [(0%)]\t Loss: 0.1163  Pearson:0.9491 Spearman:0.9207\n",
      "Train ALL Pearson: 0.959605914979286\n",
      "Train  ALL Spearman: 0.9479862481136351\n",
      "31.965168714523315\n",
      "Test Epoch:108 [(0%)]\t Loss: 0.1533  Pearson:0.9542 Spearman:0.9241\n",
      "Test : Loss:0.1675 \n",
      "pearson： 0.9396379868998235 ALL Pearson: 0.9359445887420051\n",
      "spearman： 0.9206960674774233 ALL Spearman: 0.9222998226501513\n",
      "time: 36.50171446800232\n",
      "0.009\n",
      "Train Epoch:109 [(0%)]\t Loss: 0.1201  Pearson:0.9464 Spearman:0.9405\n",
      "Train ALL Pearson: 0.959384334419803\n",
      "Train  ALL Spearman: 0.9477072360854959\n",
      "32.13203454017639\n",
      "Test Epoch:109 [(0%)]\t Loss: 0.1732  Pearson:0.9353 Spearman:0.9219\n",
      "Test : Loss:0.1744 \n",
      "pearson： 0.9364562199666384 ALL Pearson: 0.9359811435413299\n",
      "spearman： 0.9235823442875799 ALL Spearman: 0.9229426607693563\n",
      "time: 36.51562786102295\n",
      "0.009\n",
      "Train Epoch:110 [(0%)]\t Loss: 0.1206  Pearson:0.9665 Spearman:0.9565\n",
      "Train ALL Pearson: 0.9589981897786021\n",
      "Train  ALL Spearman: 0.9470670347507046\n",
      "31.82199764251709\n",
      "Test Epoch:110 [(0%)]\t Loss: 0.1582  Pearson:0.9473 Spearman:0.9302\n",
      "Test : Loss:0.1691 \n",
      "pearson： 0.9383889142946011 ALL Pearson: 0.936505206182446\n",
      "spearman： 0.9223030393469931 ALL Spearman: 0.9235675132481123\n",
      "time: 36.39307713508606\n",
      "0.009\n",
      "Train Epoch:111 [(0%)]\t Loss: 0.1237  Pearson:0.9629 Spearman:0.9541\n",
      "Train ALL Pearson: 0.9592550330255059\n",
      "Train  ALL Spearman: 0.9478960263647082\n",
      "32.0112726688385\n",
      "Test Epoch:111 [(0%)]\t Loss: 0.1470  Pearson:0.9462 Spearman:0.9394\n",
      "Test : Loss:0.1597 \n",
      "pearson： 0.9336517945896062 ALL Pearson: 0.9365458226104685\n",
      "spearman： 0.9238091701183819 ALL Spearman: 0.9236142883360781\n",
      "time: 36.48351979255676\n",
      "0.009\n",
      "Train Epoch:112 [(0%)]\t Loss: 0.1079  Pearson:0.9652 Spearman:0.9509\n",
      "Train ALL Pearson: 0.9597122368324864\n",
      "Train  ALL Spearman: 0.9476298942828625\n",
      "31.75019860267639\n",
      "Test Epoch:112 [(0%)]\t Loss: 0.1696  Pearson:0.9349 Spearman:0.9145\n",
      "Test : Loss:0.1593 \n",
      "pearson： 0.9358604338927177 ALL Pearson: 0.9364771215012844\n",
      "spearman： 0.9192687247002485 ALL Spearman: 0.9234642499636501\n",
      "time: 36.18277716636658\n",
      "0.009\n",
      "Train Epoch:113 [(0%)]\t Loss: 0.1248  Pearson:0.9565 Spearman:0.9434\n",
      "Train ALL Pearson: 0.960028000854064\n",
      "Train  ALL Spearman: 0.9480412141670282\n",
      "31.667986631393433\n",
      "Test Epoch:113 [(0%)]\t Loss: 0.1769  Pearson:0.9340 Spearman:0.9174\n",
      "Test : Loss:0.1622 \n",
      "pearson： 0.9322427211793101 ALL Pearson: 0.9364962219091846\n",
      "spearman： 0.9166442151053399 ALL Spearman: 0.9236964315324869\n",
      "time: 36.36048173904419\n",
      "0.009\n",
      "Train Epoch:114 [(0%)]\t Loss: 0.1113  Pearson:0.9555 Spearman:0.9286\n",
      "Train ALL Pearson: 0.9597357898941961\n",
      "Train  ALL Spearman: 0.9478847007758696\n",
      "32.09426474571228\n",
      "Test Epoch:114 [(0%)]\t Loss: 0.1523  Pearson:0.9418 Spearman:0.9235\n",
      "Test : Loss:0.1639 \n",
      "pearson： 0.9385329750791279 ALL Pearson: 0.936666567927264\n",
      "spearman： 0.9264674105352771 ALL Spearman: 0.9236894453968972\n",
      "time: 36.74277353286743\n",
      "0.009\n",
      "Train Epoch:115 [(0%)]\t Loss: 0.1306  Pearson:0.9528 Spearman:0.9451\n",
      "Train ALL Pearson: 0.9604901562652186\n",
      "Train  ALL Spearman: 0.949034821509456\n",
      "31.4630925655365\n",
      "Test Epoch:115 [(0%)]\t Loss: 0.1762  Pearson:0.9218 Spearman:0.9005\n",
      "Test : Loss:0.1694 \n",
      "pearson： 0.9337959346672627 ALL Pearson: 0.9365188339474354\n",
      "spearman： 0.9167201897499498 ALL Spearman: 0.9239619369255364\n",
      "time: 35.94220161437988\n",
      "0.009\n",
      "Train Epoch:116 [(0%)]\t Loss: 0.1076  Pearson:0.9682 Spearman:0.9553\n",
      "Train ALL Pearson: 0.9605694996173154\n",
      "Train  ALL Spearman: 0.9486068013558667\n",
      "31.8006374835968\n",
      "Test Epoch:116 [(0%)]\t Loss: 0.1550  Pearson:0.9387 Spearman:0.9319\n",
      "Test : Loss:0.1641 \n",
      "pearson： 0.9384585944326318 ALL Pearson: 0.9371160410903735\n",
      "spearman： 0.928050309474517 ALL Spearman: 0.9243664757131173\n",
      "time: 36.26791739463806\n",
      "0.009\n",
      "Train Epoch:117 [(0%)]\t Loss: 0.1112  Pearson:0.9613 Spearman:0.9579\n",
      "Train ALL Pearson: 0.961335970300319\n",
      "Train  ALL Spearman: 0.9496554835214053\n",
      "31.86551284790039\n",
      "Test Epoch:117 [(0%)]\t Loss: 0.1641  Pearson:0.9396 Spearman:0.9344\n",
      "Test : Loss:0.1667 \n",
      "pearson： 0.9352154783475871 ALL Pearson: 0.9367662247919926\n",
      "spearman： 0.925661259215505 ALL Spearman: 0.9242838631563495\n",
      "time: 36.470548152923584\n",
      "0.009\n",
      "Train Epoch:118 [(0%)]\t Loss: 0.1312  Pearson:0.9585 Spearman:0.9435\n",
      "Train ALL Pearson: 0.9619443063843962\n",
      "Train  ALL Spearman: 0.9509878054233261\n",
      "31.511908054351807\n",
      "Test Epoch:118 [(0%)]\t Loss: 0.1623  Pearson:0.9346 Spearman:0.9123\n",
      "Test : Loss:0.1622 \n",
      "pearson： 0.9375674242239164 ALL Pearson: 0.9366981686657798\n",
      "spearman： 0.923513712048316 ALL Spearman: 0.923882525580973\n",
      "time: 35.902498960494995\n",
      "0.009\n",
      "Train Epoch:119 [(0%)]\t Loss: 0.1168  Pearson:0.9612 Spearman:0.9489\n",
      "Train ALL Pearson: 0.9614709886433604\n",
      "Train  ALL Spearman: 0.9503652501450112\n",
      "31.77588939666748\n",
      "Test Epoch:119 [(0%)]\t Loss: 0.1645  Pearson:0.9377 Spearman:0.8993\n",
      "Test : Loss:0.1639 \n",
      "pearson： 0.9383528511767589 ALL Pearson: 0.9368790158113046\n",
      "spearman： 0.9240466653896816 ALL Spearman: 0.9242094556053532\n",
      "time: 36.49937343597412\n",
      "0.009\n",
      "Train Epoch:120 [(0%)]\t Loss: 0.1105  Pearson:0.9660 Spearman:0.9463\n",
      "Train ALL Pearson: 0.9617473403936516\n",
      "Train  ALL Spearman: 0.9497219576789374\n",
      "31.94806671142578\n",
      "Test Epoch:120 [(0%)]\t Loss: 0.1519  Pearson:0.9434 Spearman:0.9255\n",
      "Test : Loss:0.1621 \n",
      "pearson： 0.9386288530222053 ALL Pearson: 0.9372231261257492\n",
      "spearman： 0.9249903108009387 ALL Spearman: 0.9239363912825186\n",
      "time: 36.51060247421265\n",
      "0.009\n",
      "Train Epoch:121 [(0%)]\t Loss: 0.1171  Pearson:0.9619 Spearman:0.9555\n",
      "Train ALL Pearson: 0.9623029709241525\n",
      "Train  ALL Spearman: 0.9510551829466635\n",
      "31.710240125656128\n",
      "Test Epoch:121 [(0%)]\t Loss: 0.1783  Pearson:0.9178 Spearman:0.9177\n",
      "Test : Loss:0.1653 \n",
      "pearson： 0.9331771851624354 ALL Pearson: 0.9366812172060057\n",
      "spearman： 0.9210165059048211 ALL Spearman: 0.9231741060573865\n",
      "time: 36.22376346588135\n",
      "0.009\n",
      "Train Epoch:122 [(0%)]\t Loss: 0.1215  Pearson:0.9606 Spearman:0.9453\n",
      "Train ALL Pearson: 0.9621332136326547\n",
      "Train  ALL Spearman: 0.9506643142169472\n",
      "31.92561674118042\n",
      "Test Epoch:122 [(0%)]\t Loss: 0.1512  Pearson:0.9446 Spearman:0.9253\n",
      "Test : Loss:0.1607 \n",
      "pearson： 0.9358701695246371 ALL Pearson: 0.9366444957709418\n",
      "spearman： 0.9190178976203175 ALL Spearman: 0.923478840470427\n",
      "time: 36.41317677497864\n",
      "0.009\n",
      "Train Epoch:123 [(0%)]\t Loss: 0.1105  Pearson:0.9712 Spearman:0.9605\n",
      "Train ALL Pearson: 0.9624012762951658\n",
      "Train  ALL Spearman: 0.9509971047449016\n",
      "31.903210401535034\n",
      "Test Epoch:123 [(0%)]\t Loss: 0.1689  Pearson:0.9328 Spearman:0.9261\n",
      "Test : Loss:0.1667 \n",
      "pearson： 0.9382076136923572 ALL Pearson: 0.9368645819035238\n",
      "spearman： 0.9224030940193187 ALL Spearman: 0.9237178614997414\n",
      "time: 36.27680683135986\n",
      "0.009\n",
      "Train Epoch:124 [(0%)]\t Loss: 0.1167  Pearson:0.9539 Spearman:0.9491\n",
      "Train ALL Pearson: 0.9625605132072492\n",
      "Train  ALL Spearman: 0.9523070802131495\n",
      "32.27662920951843\n",
      "Test Epoch:124 [(0%)]\t Loss: 0.1669  Pearson:0.9383 Spearman:0.9358\n",
      "Test : Loss:0.1659 \n",
      "pearson： 0.9385715015746005 ALL Pearson: 0.9369231965869016\n",
      "spearman： 0.9271192509306694 ALL Spearman: 0.9236307456223403\n",
      "time: 36.72520184516907\n",
      "0.009\n",
      "Train Epoch:125 [(0%)]\t Loss: 0.1119  Pearson:0.9561 Spearman:0.9424\n",
      "Train ALL Pearson: 0.9633719656952733\n",
      "Train  ALL Spearman: 0.9525403683502346\n",
      "31.90155863761902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:125 [(0%)]\t Loss: 0.1627  Pearson:0.9398 Spearman:0.9264\n",
      "Test : Loss:0.1593 \n",
      "pearson： 0.9370118101272453 ALL Pearson: 0.9371196800653657\n",
      "spearman： 0.9221799005547169 ALL Spearman: 0.9238580411045089\n",
      "time: 36.314143896102905\n",
      "0.009\n",
      "Train Epoch:126 [(0%)]\t Loss: 0.0987  Pearson:0.9717 Spearman:0.9601\n",
      "Train ALL Pearson: 0.9629873230332767\n",
      "Train  ALL Spearman: 0.9524491218153582\n",
      "31.875537157058716\n",
      "Test Epoch:126 [(0%)]\t Loss: 0.1632  Pearson:0.9272 Spearman:0.9155\n",
      "Test : Loss:0.1606 \n",
      "pearson： 0.9332315581488596 ALL Pearson: 0.93712056676922\n",
      "spearman： 0.9200677394475747 ALL Spearman: 0.923758035079572\n",
      "time: 36.2271888256073\n",
      "0.009\n",
      "Train Epoch:127 [(0%)]\t Loss: 0.1055  Pearson:0.9699 Spearman:0.9620\n",
      "Train ALL Pearson: 0.9637013191677222\n",
      "Train  ALL Spearman: 0.9531549099793541\n",
      "31.884449005126953\n",
      "Test Epoch:127 [(0%)]\t Loss: 0.1594  Pearson:0.9351 Spearman:0.9298\n",
      "Test : Loss:0.1627 \n",
      "pearson： 0.9362026604416154 ALL Pearson: 0.9370156949811004\n",
      "spearman： 0.926109741330558 ALL Spearman: 0.9240395192872166\n",
      "time: 36.40599822998047\n",
      "0.009\n",
      "Train Epoch:128 [(0%)]\t Loss: 0.1091  Pearson:0.9504 Spearman:0.9335\n",
      "Train ALL Pearson: 0.9635221565797196\n",
      "Train  ALL Spearman: 0.9528182486794072\n",
      "32.43733096122742\n",
      "Test Epoch:128 [(0%)]\t Loss: 0.1601  Pearson:0.9464 Spearman:0.9316\n",
      "Test : Loss:0.1599 \n",
      "pearson： 0.9421480468917423 ALL Pearson: 0.9369690819394986\n",
      "spearman： 0.9294658689568258 ALL Spearman: 0.9237580739484766\n",
      "time: 37.006866216659546\n",
      "0.009\n",
      "Train Epoch:129 [(0%)]\t Loss: 0.1238  Pearson:0.9451 Spearman:0.9252\n",
      "Train ALL Pearson: 0.9638940668332329\n",
      "Train  ALL Spearman: 0.9529871971713406\n",
      "32.05743646621704\n",
      "Test Epoch:129 [(0%)]\t Loss: 0.1901  Pearson:0.9341 Spearman:0.9117\n",
      "Test : Loss:0.1748 \n",
      "pearson： 0.9362555003237248 ALL Pearson: 0.9365593282220221\n",
      "spearman： 0.9212482346613772 ALL Spearman: 0.9230239584120002\n",
      "time: 36.43603301048279\n",
      "0.009\n",
      "Train Epoch:130 [(0%)]\t Loss: 0.1079  Pearson:0.9634 Spearman:0.9474\n",
      "Train ALL Pearson: 0.9637766773632869\n",
      "Train  ALL Spearman: 0.952817198751871\n",
      "31.906168699264526\n",
      "Test Epoch:130 [(0%)]\t Loss: 0.1567  Pearson:0.9410 Spearman:0.9291\n",
      "Test : Loss:0.1568 \n",
      "pearson： 0.9366606285202912 ALL Pearson: 0.937073574738686\n",
      "spearman： 0.9225940726214201 ALL Spearman: 0.9238298446197144\n",
      "time: 36.39872741699219\n",
      "0.009\n",
      "Train Epoch:131 [(0%)]\t Loss: 0.1194  Pearson:0.9519 Spearman:0.9425\n",
      "Train ALL Pearson: 0.9645293868680288\n",
      "Train  ALL Spearman: 0.9542453180054409\n",
      "31.90060520172119\n",
      "Test Epoch:131 [(0%)]\t Loss: 0.1468  Pearson:0.9345 Spearman:0.9293\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.933747718809655 ALL Pearson: 0.9374152540477797\n",
      "spearman： 0.9252569899145157 ALL Spearman: 0.9241895539927669\n",
      "time: 36.32747006416321\n",
      "0.009\n",
      "Train Epoch:132 [(0%)]\t Loss: 0.1129  Pearson:0.9666 Spearman:0.9564\n",
      "Train ALL Pearson: 0.9644472893001419\n",
      "Train  ALL Spearman: 0.9532228701325485\n",
      "32.197423458099365\n",
      "Test Epoch:132 [(0%)]\t Loss: 0.1465  Pearson:0.9360 Spearman:0.9104\n",
      "Test : Loss:0.1565 \n",
      "pearson： 0.9392399621375163 ALL Pearson: 0.9371125850987083\n",
      "spearman： 0.9231146288035568 ALL Spearman: 0.9240585272259887\n",
      "time: 37.09085392951965\n",
      "0.009\n",
      "Train Epoch:133 [(0%)]\t Loss: 0.1166  Pearson:0.9613 Spearman:0.9452\n",
      "Train ALL Pearson: 0.9651236417731339\n",
      "Train  ALL Spearman: 0.9539806129479639\n",
      "31.969690799713135\n",
      "Test Epoch:133 [(0%)]\t Loss: 0.1633  Pearson:0.9473 Spearman:0.9389\n",
      "Test : Loss:0.1693 \n",
      "pearson： 0.9430708833492755 ALL Pearson: 0.9368821291740488\n",
      "spearman： 0.9318518545153903 ALL Spearman: 0.9236857946253734\n",
      "time: 36.41526532173157\n",
      "0.009\n",
      "Train Epoch:134 [(0%)]\t Loss: 0.1104  Pearson:0.9673 Spearman:0.9425\n",
      "Train ALL Pearson: 0.9641707619794222\n",
      "Train  ALL Spearman: 0.9535763639951425\n",
      "31.7134051322937\n",
      "Test Epoch:134 [(0%)]\t Loss: 0.1440  Pearson:0.9381 Spearman:0.9218\n",
      "Test : Loss:0.1545 \n",
      "pearson： 0.9346475311498683 ALL Pearson: 0.9370717987901297\n",
      "spearman： 0.9229161256790863 ALL Spearman: 0.9235897649907091\n",
      "time: 36.051562786102295\n",
      "0.009\n",
      "Train Epoch:135 [(0%)]\t Loss: 0.1129  Pearson:0.9745 Spearman:0.9624\n",
      "Train ALL Pearson: 0.9643614639507516\n",
      "Train  ALL Spearman: 0.9530211640967473\n",
      "31.858372449874878\n",
      "Test Epoch:135 [(0%)]\t Loss: 0.1484  Pearson:0.9389 Spearman:0.9245\n",
      "Test : Loss:0.1626 \n",
      "pearson： 0.9353390572818696 ALL Pearson: 0.9370535076176346\n",
      "spearman： 0.9221111033979125 ALL Spearman: 0.9241472865978965\n",
      "time: 36.57286071777344\n",
      "0.009\n",
      "Train Epoch:136 [(0%)]\t Loss: 0.1140  Pearson:0.9563 Spearman:0.9345\n",
      "Train ALL Pearson: 0.9651649927401996\n",
      "Train  ALL Spearman: 0.9546256690963215\n",
      "32.078370571136475\n",
      "Test Epoch:136 [(0%)]\t Loss: 0.1664  Pearson:0.9289 Spearman:0.9074\n",
      "Test : Loss:0.1673 \n",
      "pearson： 0.9364185748796086 ALL Pearson: 0.9371492113492511\n",
      "spearman： 0.9214762460853017 ALL Spearman: 0.9238362605837366\n",
      "time: 36.58762860298157\n",
      "0.009\n",
      "Train Epoch:137 [(0%)]\t Loss: 0.1050  Pearson:0.9696 Spearman:0.9637\n",
      "Train ALL Pearson: 0.9643293715162449\n",
      "Train  ALL Spearman: 0.9539642353712496\n",
      "31.74000072479248\n",
      "Test Epoch:137 [(0%)]\t Loss: 0.1697  Pearson:0.9227 Spearman:0.9192\n",
      "Test : Loss:0.1550 \n",
      "pearson： 0.9314729624567626 ALL Pearson: 0.9370816722121108\n",
      "spearman： 0.9218253699048483 ALL Spearman: 0.9237265688398029\n",
      "time: 36.27754545211792\n",
      "0.009\n",
      "Train Epoch:138 [(0%)]\t Loss: 0.0995  Pearson:0.9750 Spearman:0.9645\n",
      "Train ALL Pearson: 0.9657662207694631\n",
      "Train  ALL Spearman: 0.955914068536445\n",
      "31.59171438217163\n",
      "Test Epoch:138 [(0%)]\t Loss: 0.1554  Pearson:0.9287 Spearman:0.9282\n",
      "Test : Loss:0.1623 \n",
      "pearson： 0.9365451721341871 ALL Pearson: 0.93701813700994\n",
      "spearman： 0.9257852811192877 ALL Spearman: 0.9237710488288935\n",
      "time: 36.355738162994385\n",
      "0.009\n",
      "Train Epoch:139 [(0%)]\t Loss: 0.1090  Pearson:0.9659 Spearman:0.9625\n",
      "Train ALL Pearson: 0.9648362612009116\n",
      "Train  ALL Spearman: 0.9545483163537101\n",
      "32.1960232257843\n",
      "Test Epoch:139 [(0%)]\t Loss: 0.1670  Pearson:0.9398 Spearman:0.9275\n",
      "Test : Loss:0.1657 \n",
      "pearson： 0.9385756299715289 ALL Pearson: 0.9363124921017623\n",
      "spearman： 0.9245190385873667 ALL Spearman: 0.9227059594351961\n",
      "time: 36.55053448677063\n",
      "0.009\n",
      "Train Epoch:140 [(0%)]\t Loss: 0.1170  Pearson:0.9675 Spearman:0.9584\n",
      "Train ALL Pearson: 0.9651463051786019\n",
      "Train  ALL Spearman: 0.9544058325698579\n",
      "32.01540541648865\n",
      "Test Epoch:140 [(0%)]\t Loss: 0.1583  Pearson:0.9488 Spearman:0.9224\n",
      "Test : Loss:0.1605 \n",
      "pearson： 0.9408305839396657 ALL Pearson: 0.9367689280122029\n",
      "spearman： 0.9239961910502089 ALL Spearman: 0.9230645962185605\n",
      "time: 36.39403223991394\n",
      "0.009\n",
      "Train Epoch:141 [(0%)]\t Loss: 0.1023  Pearson:0.9713 Spearman:0.9496\n",
      "Train ALL Pearson: 0.9652987471750261\n",
      "Train  ALL Spearman: 0.9540783437675927\n",
      "32.362056016922\n",
      "Test Epoch:141 [(0%)]\t Loss: 0.1531  Pearson:0.9563 Spearman:0.9327\n",
      "Test : Loss:0.1726 \n",
      "pearson： 0.9400547519517463 ALL Pearson: 0.9366767879464939\n",
      "spearman： 0.9238662593701702 ALL Spearman: 0.9232242198095646\n",
      "time: 36.86461329460144\n",
      "0.009\n",
      "Train Epoch:142 [(0%)]\t Loss: 0.1147  Pearson:0.9678 Spearman:0.9565\n",
      "Train ALL Pearson: 0.9658783386151127\n",
      "Train  ALL Spearman: 0.955845158211998\n",
      "32.22801113128662\n",
      "Test Epoch:142 [(0%)]\t Loss: 0.1576  Pearson:0.9496 Spearman:0.9444\n",
      "Test : Loss:0.1611 \n",
      "pearson： 0.9422007738474626 ALL Pearson: 0.9369160173379535\n",
      "spearman： 0.9261995931667926 ALL Spearman: 0.9233287063362593\n",
      "time: 36.88925361633301\n",
      "0.009\n",
      "Train Epoch:143 [(0%)]\t Loss: 0.0966  Pearson:0.9789 Spearman:0.9590\n",
      "Train ALL Pearson: 0.966574700606211\n",
      "Train  ALL Spearman: 0.9562203792325878\n",
      "31.568094730377197\n",
      "Test Epoch:143 [(0%)]\t Loss: 0.1572  Pearson:0.9366 Spearman:0.9137\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.9378061881867423 ALL Pearson: 0.9368883586675057\n",
      "spearman： 0.9206382995140312 ALL Spearman: 0.923464018216973\n",
      "time: 35.93181562423706\n",
      "0.009\n",
      "Train Epoch:144 [(0%)]\t Loss: 0.1135  Pearson:0.9651 Spearman:0.9549\n",
      "Train ALL Pearson: 0.9670502023504197\n",
      "Train  ALL Spearman: 0.9573516680123644\n",
      "32.51086616516113\n",
      "Test Epoch:144 [(0%)]\t Loss: 0.1718  Pearson:0.9326 Spearman:0.9136\n",
      "Test : Loss:0.1676 \n",
      "pearson： 0.9373404925203725 ALL Pearson: 0.9365581179281998\n",
      "spearman： 0.9240700372777066 ALL Spearman: 0.9233434534749475\n",
      "time: 36.983431816101074\n",
      "0.009\n",
      "Train Epoch:145 [(0%)]\t Loss: 0.0964  Pearson:0.9705 Spearman:0.9625\n",
      "Train ALL Pearson: 0.966999225982198\n",
      "Train  ALL Spearman: 0.9573881358159523\n",
      "32.021580934524536\n",
      "Test Epoch:145 [(0%)]\t Loss: 0.1611  Pearson:0.9368 Spearman:0.9172\n",
      "Test : Loss:0.1625 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9391632630399402 ALL Pearson: 0.9370619587386116\n",
      "spearman： 0.9231633647963016 ALL Spearman: 0.9239039958838831\n",
      "time: 36.64709806442261\n",
      "0.009\n",
      "Train Epoch:146 [(0%)]\t Loss: 0.0935  Pearson:0.9703 Spearman:0.9717\n",
      "Train ALL Pearson: 0.9672486300078897\n",
      "Train  ALL Spearman: 0.9565589463601232\n",
      "32.023183822631836\n",
      "Test Epoch:146 [(0%)]\t Loss: 0.1611  Pearson:0.9210 Spearman:0.9086\n",
      "Test : Loss:0.1531 \n",
      "pearson： 0.9319297290388779 ALL Pearson: 0.9372271140660033\n",
      "spearman： 0.9202483661893068 ALL Spearman: 0.9242106784813091\n",
      "time: 36.614710569381714\n",
      "0.009\n",
      "Train Epoch:147 [(0%)]\t Loss: 0.1007  Pearson:0.9719 Spearman:0.9539\n",
      "Train ALL Pearson: 0.9668918861769573\n",
      "Train  ALL Spearman: 0.9562437450997934\n",
      "31.98573589324951\n",
      "Test Epoch:147 [(0%)]\t Loss: 0.1503  Pearson:0.9318 Spearman:0.9167\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.938003478175244 ALL Pearson: 0.9372705431968342\n",
      "spearman： 0.9251006065153593 ALL Spearman: 0.924342021277308\n",
      "time: 36.60425329208374\n",
      "0.009\n",
      "Train Epoch:148 [(0%)]\t Loss: 0.1037  Pearson:0.9595 Spearman:0.9487\n",
      "Train ALL Pearson: 0.9677332947865691\n",
      "Train  ALL Spearman: 0.9581993751774588\n",
      "31.624534606933594\n",
      "Test Epoch:148 [(0%)]\t Loss: 0.1665  Pearson:0.9346 Spearman:0.9136\n",
      "Test : Loss:0.1591 \n",
      "pearson： 0.9377708282040338 ALL Pearson: 0.9372654743099124\n",
      "spearman： 0.9234675334124826 ALL Spearman: 0.9242106051437531\n",
      "time: 36.36901307106018\n",
      "0.009\n",
      "Train Epoch:149 [(0%)]\t Loss: 0.1061  Pearson:0.9701 Spearman:0.9560\n",
      "Train ALL Pearson: 0.9672340887721076\n",
      "Train  ALL Spearman: 0.9563428914041358\n",
      "32.063963174819946\n",
      "Test Epoch:149 [(0%)]\t Loss: 0.1499  Pearson:0.9183 Spearman:0.9221\n",
      "Test : Loss:0.1575 \n",
      "pearson： 0.9360614544977388 ALL Pearson: 0.9368742001714166\n",
      "spearman： 0.9274460321234201 ALL Spearman: 0.9229968095821246\n",
      "time: 36.34558963775635\n",
      "0.009\n",
      "Train Epoch:150 [(0%)]\t Loss: 0.1034  Pearson:0.9684 Spearman:0.9618\n",
      "Train ALL Pearson: 0.9668407289355835\n",
      "Train  ALL Spearman: 0.9563307737984658\n",
      "32.25967359542847\n",
      "Test Epoch:150 [(0%)]\t Loss: 0.1548  Pearson:0.9354 Spearman:0.9218\n",
      "Test : Loss:0.1565 \n",
      "pearson： 0.9371323657104423 ALL Pearson: 0.9372983190382557\n",
      "spearman： 0.9232641539922283 ALL Spearman: 0.9245035238042413\n",
      "time: 36.8207585811615\n",
      "0.009\n",
      "Train Epoch:151 [(0%)]\t Loss: 0.1058  Pearson:0.9691 Spearman:0.9432\n",
      "Train ALL Pearson: 0.9669861870597588\n",
      "Train  ALL Spearman: 0.9563732603666547\n",
      "31.716498613357544\n",
      "Test Epoch:151 [(0%)]\t Loss: 0.1772  Pearson:0.9322 Spearman:0.9167\n",
      "Test : Loss:0.1703 \n",
      "pearson： 0.9362541207425514 ALL Pearson: 0.9367843651021589\n",
      "spearman： 0.922848975796303 ALL Spearman: 0.9240938191470947\n",
      "time: 36.431986808776855\n",
      "0.009\n",
      "Train Epoch:152 [(0%)]\t Loss: 0.1065  Pearson:0.9670 Spearman:0.9602\n",
      "Train ALL Pearson: 0.9677914396225559\n",
      "Train  ALL Spearman: 0.9583155402303374\n",
      "32.3856258392334\n",
      "Test Epoch:152 [(0%)]\t Loss: 0.1678  Pearson:0.9246 Spearman:0.9216\n",
      "Test : Loss:0.1526 \n",
      "pearson： 0.9340033091391042 ALL Pearson: 0.9377766616184825\n",
      "spearman： 0.9243711539075985 ALL Spearman: 0.9251039779779323\n",
      "time: 36.89018106460571\n",
      "0.009\n",
      "Train Epoch:153 [(0%)]\t Loss: 0.1069  Pearson:0.9661 Spearman:0.9452\n",
      "Train ALL Pearson: 0.9678152960565022\n",
      "Train  ALL Spearman: 0.9572393961266124\n",
      "31.437805891036987\n",
      "Test Epoch:153 [(0%)]\t Loss: 0.1715  Pearson:0.9320 Spearman:0.9145\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.9360881889403353 ALL Pearson: 0.9379420174416647\n",
      "spearman： 0.9225187969272338 ALL Spearman: 0.925003835150953\n",
      "time: 35.91036796569824\n",
      "0.009\n",
      "Train Epoch:154 [(0%)]\t Loss: 0.1043  Pearson:0.9681 Spearman:0.9537\n",
      "Train ALL Pearson: 0.9677024509287608\n",
      "Train  ALL Spearman: 0.9575156482798302\n",
      "31.916499853134155\n",
      "Test Epoch:154 [(0%)]\t Loss: 0.1668  Pearson:0.9269 Spearman:0.9016\n",
      "Test : Loss:0.1544 \n",
      "pearson： 0.937387630568293 ALL Pearson: 0.9379622838457897\n",
      "spearman： 0.9217274525040089 ALL Spearman: 0.9243364406282331\n",
      "time: 36.41005873680115\n",
      "0.009\n",
      "Train Epoch:155 [(0%)]\t Loss: 0.1009  Pearson:0.9725 Spearman:0.9580\n",
      "Train ALL Pearson: 0.9684874390635894\n",
      "Train  ALL Spearman: 0.9583480907283949\n",
      "31.962833404541016\n",
      "Test Epoch:155 [(0%)]\t Loss: 0.1448  Pearson:0.9315 Spearman:0.9195\n",
      "Test : Loss:0.1498 \n",
      "pearson： 0.9343037702220734 ALL Pearson: 0.937765285680014\n",
      "spearman： 0.9194739797419147 ALL Spearman: 0.9245902718657947\n",
      "time: 36.51919174194336\n",
      "0.009\n",
      "Train Epoch:156 [(0%)]\t Loss: 0.1086  Pearson:0.9659 Spearman:0.9510\n",
      "Train ALL Pearson: 0.96772635242631\n",
      "Train  ALL Spearman: 0.9570824084502106\n",
      "31.337302684783936\n",
      "Test Epoch:156 [(0%)]\t Loss: 0.1482  Pearson:0.9397 Spearman:0.9212\n",
      "Test : Loss:0.1559 \n",
      "pearson： 0.9367755883990608 ALL Pearson: 0.9372760336922701\n",
      "spearman： 0.9251277160890748 ALL Spearman: 0.9243256611630467\n",
      "time: 35.92283296585083\n",
      "0.009\n",
      "Train Epoch:157 [(0%)]\t Loss: 0.0953  Pearson:0.9788 Spearman:0.9591\n",
      "Train ALL Pearson: 0.9688016175086982\n",
      "Train  ALL Spearman: 0.95840155957542\n",
      "31.952087879180908\n",
      "Test Epoch:157 [(0%)]\t Loss: 0.1621  Pearson:0.9492 Spearman:0.9370\n",
      "Test : Loss:0.1582 \n",
      "pearson： 0.939198075405785 ALL Pearson: 0.9364973637899927\n",
      "spearman： 0.9251406710307741 ALL Spearman: 0.9229900067621863\n",
      "time: 36.31485629081726\n",
      "0.009\n",
      "Train Epoch:158 [(0%)]\t Loss: 0.0910  Pearson:0.9772 Spearman:0.9596\n",
      "Train ALL Pearson: 0.9687307038085566\n",
      "Train  ALL Spearman: 0.959011549899114\n",
      "32.18681716918945\n",
      "Test Epoch:158 [(0%)]\t Loss: 0.1522  Pearson:0.9376 Spearman:0.9285\n",
      "Test : Loss:0.1595 \n",
      "pearson： 0.9367175590186307 ALL Pearson: 0.9370961726457372\n",
      "spearman： 0.9233475635496311 ALL Spearman: 0.9241528195498149\n",
      "time: 36.96928334236145\n",
      "0.009\n",
      "Train Epoch:159 [(0%)]\t Loss: 0.1084  Pearson:0.9708 Spearman:0.9521\n",
      "Train ALL Pearson: 0.9684538155126275\n",
      "Train  ALL Spearman: 0.9582282371071487\n",
      "31.942737340927124\n",
      "Test Epoch:159 [(0%)]\t Loss: 0.1599  Pearson:0.9373 Spearman:0.9360\n",
      "Test : Loss:0.1598 \n",
      "pearson： 0.94012450039728 ALL Pearson: 0.937163761160386\n",
      "spearman： 0.929744711014736 ALL Spearman: 0.9242025164057993\n",
      "time: 36.383519887924194\n",
      "0.009\n",
      "Train Epoch:160 [(0%)]\t Loss: 0.0957  Pearson:0.9705 Spearman:0.9656\n",
      "Train ALL Pearson: 0.9690537868441708\n",
      "Train  ALL Spearman: 0.95971462479564\n",
      "31.849900722503662\n",
      "Test Epoch:160 [(0%)]\t Loss: 0.1545  Pearson:0.9340 Spearman:0.9172\n",
      "Test : Loss:0.1562 \n",
      "pearson： 0.9363551558461278 ALL Pearson: 0.9373832643318926\n",
      "spearman： 0.9240506943991886 ALL Spearman: 0.9246265196862498\n",
      "time: 36.55154204368591\n",
      "0.009\n",
      "Train Epoch:161 [(0%)]\t Loss: 0.1028  Pearson:0.9629 Spearman:0.9559\n",
      "Train ALL Pearson: 0.9689450368375914\n",
      "Train  ALL Spearman: 0.9587694738461786\n",
      "32.07569193840027\n",
      "Test Epoch:161 [(0%)]\t Loss: 0.1503  Pearson:0.9414 Spearman:0.9254\n",
      "Test : Loss:0.1551 \n",
      "pearson： 0.9364514292771008 ALL Pearson: 0.9371789353538561\n",
      "spearman： 0.9242748183501951 ALL Spearman: 0.9244596811464812\n",
      "time: 36.47328162193298\n",
      "0.009\n",
      "Train Epoch:162 [(0%)]\t Loss: 0.1110  Pearson:0.9660 Spearman:0.9608\n",
      "Train ALL Pearson: 0.9692084989233396\n",
      "Train  ALL Spearman: 0.9586168959998883\n",
      "31.770021677017212\n",
      "Test Epoch:162 [(0%)]\t Loss: 0.1464  Pearson:0.9437 Spearman:0.9236\n",
      "Test : Loss:0.1521 \n",
      "pearson： 0.9367099835633398 ALL Pearson: 0.9373439648616917\n",
      "spearman： 0.9248578467004875 ALL Spearman: 0.9247872814761238\n",
      "time: 36.25523281097412\n",
      "0.009\n",
      "Train Epoch:163 [(0%)]\t Loss: 0.1042  Pearson:0.9730 Spearman:0.9500\n",
      "Train ALL Pearson: 0.9693461094185121\n",
      "Train  ALL Spearman: 0.9600425529462853\n",
      "31.814334392547607\n",
      "Test Epoch:163 [(0%)]\t Loss: 0.1655  Pearson:0.9296 Spearman:0.9312\n",
      "Test : Loss:0.1649 \n",
      "pearson： 0.9366222919585511 ALL Pearson: 0.9373066170919421\n",
      "spearman： 0.925720134814237 ALL Spearman: 0.9249234645232444\n",
      "time: 36.64578461647034\n",
      "0.009\n",
      "Train Epoch:164 [(0%)]\t Loss: 0.1142  Pearson:0.9574 Spearman:0.9432\n",
      "Train ALL Pearson: 0.969147899751917\n",
      "Train  ALL Spearman: 0.959587402202297\n",
      "32.15001106262207\n",
      "Test Epoch:164 [(0%)]\t Loss: 0.1615  Pearson:0.9246 Spearman:0.9277\n",
      "Test : Loss:0.1608 \n",
      "pearson： 0.9360536777667593 ALL Pearson: 0.9372001666912712\n",
      "spearman： 0.9274707565834089 ALL Spearman: 0.9240772389924219\n",
      "time: 36.82851028442383\n",
      "0.009\n",
      "Train Epoch:165 [(0%)]\t Loss: 0.0952  Pearson:0.9744 Spearman:0.9651\n",
      "Train ALL Pearson: 0.9704083586723403\n",
      "Train  ALL Spearman: 0.9609524398712003\n",
      "32.200759410858154\n",
      "Test Epoch:165 [(0%)]\t Loss: 0.1504  Pearson:0.9368 Spearman:0.9115\n",
      "Test : Loss:0.1537 \n",
      "pearson： 0.9387760497832414 ALL Pearson: 0.937350024356056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9238660504861188 ALL Spearman: 0.9245553807901268\n",
      "time: 36.51637554168701\n",
      "0.009\n",
      "Train Epoch:166 [(0%)]\t Loss: 0.1160  Pearson:0.9692 Spearman:0.9445\n",
      "Train ALL Pearson: 0.9697207830824142\n",
      "Train  ALL Spearman: 0.959520822071945\n",
      "32.08553504943848\n",
      "Test Epoch:166 [(0%)]\t Loss: 0.1407  Pearson:0.9448 Spearman:0.9212\n",
      "Test : Loss:0.1539 \n",
      "pearson： 0.937694486542702 ALL Pearson: 0.9371663880636346\n",
      "spearman： 0.9225331420094564 ALL Spearman: 0.9240304910395215\n",
      "time: 36.71505093574524\n",
      "0.009\n",
      "Train Epoch:167 [(0%)]\t Loss: 0.0885  Pearson:0.9706 Spearman:0.9667\n",
      "Train ALL Pearson: 0.9698745857851774\n",
      "Train  ALL Spearman: 0.9604525200529572\n",
      "32.05581021308899\n",
      "Test Epoch:167 [(0%)]\t Loss: 0.1634  Pearson:0.9377 Spearman:0.9259\n",
      "Test : Loss:0.1597 \n",
      "pearson： 0.9359545902287902 ALL Pearson: 0.9372005762540442\n",
      "spearman： 0.9216431540962895 ALL Spearman: 0.9240582544381271\n",
      "time: 36.651336669921875\n",
      "0.009\n",
      "Train Epoch:168 [(0%)]\t Loss: 0.1053  Pearson:0.9699 Spearman:0.9582\n",
      "Train ALL Pearson: 0.9696642320322673\n",
      "Train  ALL Spearman: 0.9600029528282512\n",
      "31.918835639953613\n",
      "Test Epoch:168 [(0%)]\t Loss: 0.1507  Pearson:0.9377 Spearman:0.9252\n",
      "Test : Loss:0.1567 \n",
      "pearson： 0.9383468466027725 ALL Pearson: 0.9370839485083924\n",
      "spearman： 0.9233480202283398 ALL Spearman: 0.9235796436745982\n",
      "time: 36.2760272026062\n",
      "0.009\n",
      "Train Epoch:169 [(0%)]\t Loss: 0.0965  Pearson:0.9695 Spearman:0.9602\n",
      "Train ALL Pearson: 0.9700055837258552\n",
      "Train  ALL Spearman: 0.9604668786474039\n",
      "32.241989612579346\n",
      "Test Epoch:169 [(0%)]\t Loss: 0.1764  Pearson:0.9116 Spearman:0.9008\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.9328129679400047 ALL Pearson: 0.9368732420856953\n",
      "spearman： 0.9164056476590287 ALL Spearman: 0.9233473858347032\n",
      "time: 36.872504234313965\n",
      "0.009\n",
      "Train Epoch:170 [(0%)]\t Loss: 0.0950  Pearson:0.9766 Spearman:0.9499\n",
      "Train ALL Pearson: 0.970999276118618\n",
      "Train  ALL Spearman: 0.9618509365709185\n",
      "31.94899582862854\n",
      "Test Epoch:170 [(0%)]\t Loss: 0.1707  Pearson:0.9410 Spearman:0.9469\n",
      "Test : Loss:0.1629 \n",
      "pearson： 0.9393307862699671 ALL Pearson: 0.9363170627366126\n",
      "spearman： 0.9292392128248242 ALL Spearman: 0.9228702084258603\n",
      "time: 36.48853802680969\n",
      "0.009\n",
      "Train Epoch:171 [(0%)]\t Loss: 0.0899  Pearson:0.9794 Spearman:0.9652\n",
      "Train ALL Pearson: 0.9698800432270548\n",
      "Train  ALL Spearman: 0.9606629997297511\n",
      "31.839494705200195\n",
      "Test Epoch:171 [(0%)]\t Loss: 0.1748  Pearson:0.9335 Spearman:0.9126\n",
      "Test : Loss:0.1659 \n",
      "pearson： 0.9360903678954031 ALL Pearson: 0.9363169096692732\n",
      "spearman： 0.9234887847427018 ALL Spearman: 0.9232754541595991\n",
      "time: 36.4530143737793\n",
      "0.009\n",
      "Train Epoch:172 [(0%)]\t Loss: 0.1125  Pearson:0.9686 Spearman:0.9630\n",
      "Train ALL Pearson: 0.9698155420123946\n",
      "Train  ALL Spearman: 0.9593560874374053\n",
      "32.08021140098572\n",
      "Test Epoch:172 [(0%)]\t Loss: 0.1554  Pearson:0.9136 Spearman:0.8876\n",
      "Test : Loss:0.1555 \n",
      "pearson： 0.9332513286811518 ALL Pearson: 0.9369052731178963\n",
      "spearman： 0.9158701463060338 ALL Spearman: 0.9239319257587305\n",
      "time: 36.49879455566406\n",
      "0.009\n",
      "Train Epoch:173 [(0%)]\t Loss: 0.1021  Pearson:0.9741 Spearman:0.9599\n",
      "Train ALL Pearson: 0.9699636736639937\n",
      "Train  ALL Spearman: 0.9606058396713207\n",
      "31.80291771888733\n",
      "Test Epoch:173 [(0%)]\t Loss: 0.1521  Pearson:0.9389 Spearman:0.9256\n",
      "Test : Loss:0.1523 \n",
      "pearson： 0.9356811007463304 ALL Pearson: 0.9362477579148323\n",
      "spearman： 0.9255560194698641 ALL Spearman: 0.924074529847419\n",
      "time: 36.23304200172424\n",
      "0.009\n",
      "Train Epoch:174 [(0%)]\t Loss: 0.1040  Pearson:0.9711 Spearman:0.9517\n",
      "Train ALL Pearson: 0.9701311518344972\n",
      "Train  ALL Spearman: 0.9609494789975885\n",
      "31.543111324310303\n",
      "Test Epoch:174 [(0%)]\t Loss: 0.1571  Pearson:0.9419 Spearman:0.9122\n",
      "Test : Loss:0.1512 \n",
      "pearson： 0.9413366254579212 ALL Pearson: 0.9367050142161251\n",
      "spearman： 0.9223103793549549 ALL Spearman: 0.9234045744609137\n",
      "time: 36.19461965560913\n",
      "0.009\n",
      "Train Epoch:175 [(0%)]\t Loss: 0.1137  Pearson:0.9675 Spearman:0.9549\n",
      "Train ALL Pearson: 0.9710542707149163\n",
      "Train  ALL Spearman: 0.9614493019556217\n",
      "32.13742637634277\n",
      "Test Epoch:175 [(0%)]\t Loss: 0.1557  Pearson:0.9441 Spearman:0.9207\n",
      "Test : Loss:0.1737 \n",
      "pearson： 0.9389292873506517 ALL Pearson: 0.9365697862161364\n",
      "spearman： 0.923522791508551 ALL Spearman: 0.9237624544006996\n",
      "time: 36.74494743347168\n",
      "0.0026999999999999997\n",
      "Train Epoch:176 [(0%)]\t Loss: 0.1020  Pearson:0.9662 Spearman:0.9623\n",
      "Train ALL Pearson: 0.9687761971079666\n",
      "Train  ALL Spearman: 0.9583964533007115\n",
      "31.773672580718994\n",
      "Test Epoch:176 [(0%)]\t Loss: 0.1571  Pearson:0.9458 Spearman:0.9181\n",
      "Test : Loss:0.1541 \n",
      "pearson： 0.9326830102721001 ALL Pearson: 0.9378423807469723\n",
      "spearman： 0.9188399692540206 ALL Spearman: 0.9244786528388567\n",
      "time: 36.4501736164093\n",
      "0.0026999999999999997\n",
      "Train Epoch:177 [(0%)]\t Loss: 0.1053  Pearson:0.9706 Spearman:0.9542\n",
      "Train ALL Pearson: 0.9690772302930256\n",
      "Train  ALL Spearman: 0.9585631979407472\n",
      "31.904337406158447\n",
      "Test Epoch:177 [(0%)]\t Loss: 0.1469  Pearson:0.9351 Spearman:0.9062\n",
      "Test : Loss:0.1559 \n",
      "pearson： 0.9380741978535186 ALL Pearson: 0.9378808137787396\n",
      "spearman： 0.9213128242331359 ALL Spearman: 0.9247324836542405\n",
      "time: 36.266937255859375\n",
      "0.0026999999999999997\n",
      "Train Epoch:178 [(0%)]\t Loss: 0.1101  Pearson:0.9553 Spearman:0.9511\n",
      "Train ALL Pearson: 0.9697870928841781\n",
      "Train  ALL Spearman: 0.9600738326016324\n",
      "31.90336036682129\n",
      "Test Epoch:178 [(0%)]\t Loss: 0.1578  Pearson:0.9296 Spearman:0.9223\n",
      "Test : Loss:0.1575 \n",
      "pearson： 0.9357115780217555 ALL Pearson: 0.9377832539267684\n",
      "spearman： 0.9239635543715082 ALL Spearman: 0.9245923428907266\n",
      "time: 36.251965284347534\n",
      "0.0026999999999999997\n",
      "Train Epoch:179 [(0%)]\t Loss: 0.1045  Pearson:0.9712 Spearman:0.9535\n",
      "Train ALL Pearson: 0.9697871914174347\n",
      "Train  ALL Spearman: 0.9604713395183123\n",
      "31.97974991798401\n",
      "Test Epoch:179 [(0%)]\t Loss: 0.1681  Pearson:0.9331 Spearman:0.9310\n",
      "Test : Loss:0.1610 \n",
      "pearson： 0.9384858044907477 ALL Pearson: 0.9375451737353963\n",
      "spearman： 0.9280242774330124 ALL Spearman: 0.9245722850967971\n",
      "time: 36.67024540901184\n",
      "0.0026999999999999997\n",
      "Train Epoch:180 [(0%)]\t Loss: 0.0973  Pearson:0.9702 Spearman:0.9525\n",
      "Train ALL Pearson: 0.9690096842718614\n",
      "Train  ALL Spearman: 0.9586415891821308\n",
      "31.85994791984558\n",
      "Test Epoch:180 [(0%)]\t Loss: 0.1487  Pearson:0.9441 Spearman:0.9239\n",
      "Test : Loss:0.1564 \n",
      "pearson： 0.9372947504518575 ALL Pearson: 0.9378563317315232\n",
      "spearman： 0.9224772924734578 ALL Spearman: 0.9249284584716538\n",
      "time: 36.3484570980072\n",
      "0.0026999999999999997\n",
      "Train Epoch:181 [(0%)]\t Loss: 0.1073  Pearson:0.9626 Spearman:0.9520\n",
      "Train ALL Pearson: 0.9694583090213559\n",
      "Train  ALL Spearman: 0.9597137369578401\n",
      "31.482070684432983\n",
      "Test Epoch:181 [(0%)]\t Loss: 0.1496  Pearson:0.9366 Spearman:0.9174\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.9369797014252957 ALL Pearson: 0.9376476564737093\n",
      "spearman： 0.9246048420416236 ALL Spearman: 0.9248417606187662\n",
      "time: 35.801684856414795\n",
      "0.0026999999999999997\n",
      "Train Epoch:182 [(0%)]\t Loss: 0.0998  Pearson:0.9625 Spearman:0.9497\n",
      "Train ALL Pearson: 0.9698581564783983\n",
      "Train  ALL Spearman: 0.959791989556414\n",
      "31.757783889770508\n",
      "Test Epoch:182 [(0%)]\t Loss: 0.1566  Pearson:0.9385 Spearman:0.9251\n",
      "Test : Loss:0.1591 \n",
      "pearson： 0.9370014084213815 ALL Pearson: 0.9375249657615907\n",
      "spearman： 0.9224526348484537 ALL Spearman: 0.9244705860466829\n",
      "time: 36.46227550506592\n",
      "0.0026999999999999997\n",
      "Train Epoch:183 [(0%)]\t Loss: 0.0921  Pearson:0.9795 Spearman:0.9702\n",
      "Train ALL Pearson: 0.9695774677504886\n",
      "Train  ALL Spearman: 0.9593558754893948\n",
      "31.863498210906982\n",
      "Test Epoch:183 [(0%)]\t Loss: 0.1477  Pearson:0.9474 Spearman:0.9324\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.9406195892563683 ALL Pearson: 0.937698073276403\n",
      "spearman： 0.9275323104314327 ALL Spearman: 0.9247760013990433\n",
      "time: 36.34705948829651\n",
      "0.0026999999999999997\n",
      "Train Epoch:184 [(0%)]\t Loss: 0.0958  Pearson:0.9752 Spearman:0.9688\n",
      "Train ALL Pearson: 0.9694740297179314\n",
      "Train  ALL Spearman: 0.9596759689489253\n",
      "32.17343044281006\n",
      "Test Epoch:184 [(0%)]\t Loss: 0.1498  Pearson:0.9406 Spearman:0.9124\n",
      "Test : Loss:0.1592 \n",
      "pearson： 0.939142578531663 ALL Pearson: 0.9377088119887419\n",
      "spearman： 0.9231843689187407 ALL Spearman: 0.9248132367097043\n",
      "time: 36.804574966430664\n",
      "0.0026999999999999997\n",
      "Train Epoch:185 [(0%)]\t Loss: 0.0980  Pearson:0.9701 Spearman:0.9453\n",
      "Train ALL Pearson: 0.9702264721830461\n",
      "Train  ALL Spearman: 0.9605777307732171\n",
      "32.18826699256897\n",
      "Test Epoch:185 [(0%)]\t Loss: 0.1636  Pearson:0.9442 Spearman:0.9240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : Loss:0.1621 \n",
      "pearson： 0.9378941189448189 ALL Pearson: 0.9372019476110579\n",
      "spearman： 0.9240078056073723 ALL Spearman: 0.9243642631190511\n",
      "time: 36.49771237373352\n",
      "0.0026999999999999997\n",
      "Train Epoch:186 [(0%)]\t Loss: 0.1014  Pearson:0.9770 Spearman:0.9637\n",
      "Train ALL Pearson: 0.9695426519313882\n",
      "Train  ALL Spearman: 0.9592262804136471\n",
      "31.6862473487854\n",
      "Test Epoch:186 [(0%)]\t Loss: 0.1624  Pearson:0.9315 Spearman:0.9210\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9353031114950244 ALL Pearson: 0.9374682826800926\n",
      "spearman： 0.9245434467711164 ALL Spearman: 0.9244242239048804\n",
      "time: 36.208796977996826\n",
      "0.0026999999999999997\n",
      "Train Epoch:187 [(0%)]\t Loss: 0.0975  Pearson:0.9715 Spearman:0.9530\n",
      "Train ALL Pearson: 0.9696687868274784\n",
      "Train  ALL Spearman: 0.9601159747995909\n",
      "31.994954109191895\n",
      "Test Epoch:187 [(0%)]\t Loss: 0.1696  Pearson:0.9317 Spearman:0.9105\n",
      "Test : Loss:0.1566 \n",
      "pearson： 0.93352283105732 ALL Pearson: 0.9373807679366939\n",
      "spearman： 0.9196452631371894 ALL Spearman: 0.9243058009306668\n",
      "time: 36.48251485824585\n",
      "0.0026999999999999997\n",
      "Train Epoch:188 [(0%)]\t Loss: 0.0985  Pearson:0.9730 Spearman:0.9449\n",
      "Train ALL Pearson: 0.9701364799600258\n",
      "Train  ALL Spearman: 0.9607112962475093\n",
      "32.256218910217285\n",
      "Test Epoch:188 [(0%)]\t Loss: 0.1747  Pearson:0.9308 Spearman:0.9176\n",
      "Test : Loss:0.1583 \n",
      "pearson： 0.9372873707882335 ALL Pearson: 0.9372795379795779\n",
      "spearman： 0.9238120967493866 ALL Spearman: 0.9241787917064158\n",
      "time: 36.84629034996033\n",
      "0.0026999999999999997\n",
      "Train Epoch:189 [(0%)]\t Loss: 0.0980  Pearson:0.9693 Spearman:0.9567\n",
      "Train ALL Pearson: 0.969906180807344\n",
      "Train  ALL Spearman: 0.9601991950252893\n",
      "32.010056018829346\n",
      "Test Epoch:189 [(0%)]\t Loss: 0.1623  Pearson:0.9275 Spearman:0.9131\n",
      "Test : Loss:0.1561 \n",
      "pearson： 0.9346492143358116 ALL Pearson: 0.9376440730050621\n",
      "spearman： 0.9204310605007002 ALL Spearman: 0.9248723860430461\n",
      "time: 36.38735842704773\n",
      "0.0026999999999999997\n",
      "Train Epoch:190 [(0%)]\t Loss: 0.1019  Pearson:0.9699 Spearman:0.9533\n",
      "Train ALL Pearson: 0.9696971846150161\n",
      "Train  ALL Spearman: 0.960679620801644\n",
      "31.87634253501892\n",
      "Test Epoch:190 [(0%)]\t Loss: 0.1559  Pearson:0.9358 Spearman:0.9355\n",
      "Test : Loss:0.1588 \n",
      "pearson： 0.937741880065738 ALL Pearson: 0.9375339740463549\n",
      "spearman： 0.9283950415537373 ALL Spearman: 0.9245133855054039\n",
      "time: 36.21985745429993\n",
      "0.0026999999999999997\n",
      "Train Epoch:191 [(0%)]\t Loss: 0.1045  Pearson:0.9662 Spearman:0.9512\n",
      "Train ALL Pearson: 0.9699702059483876\n",
      "Train  ALL Spearman: 0.9601781123584477\n",
      "32.069162130355835\n",
      "Test Epoch:191 [(0%)]\t Loss: 0.1449  Pearson:0.9494 Spearman:0.9344\n",
      "Test : Loss:0.1607 \n",
      "pearson： 0.9405480434288301 ALL Pearson: 0.9374069826026434\n",
      "spearman： 0.926726700505803 ALL Spearman: 0.9242877911158518\n",
      "time: 36.657689809799194\n",
      "0.0026999999999999997\n",
      "Train Epoch:192 [(0%)]\t Loss: 0.0922  Pearson:0.9758 Spearman:0.9678\n",
      "Train ALL Pearson: 0.9700074653524968\n",
      "Train  ALL Spearman: 0.9596391537922307\n",
      "31.960346460342407\n",
      "Test Epoch:192 [(0%)]\t Loss: 0.1643  Pearson:0.9390 Spearman:0.9226\n",
      "Test : Loss:0.1623 \n",
      "pearson： 0.9358460247001286 ALL Pearson: 0.937396847580626\n",
      "spearman： 0.923271406587055 ALL Spearman: 0.9242823421354369\n",
      "time: 36.32994556427002\n",
      "0.0026999999999999997\n",
      "Train Epoch:193 [(0%)]\t Loss: 0.0963  Pearson:0.9719 Spearman:0.9568\n",
      "Train ALL Pearson: 0.9704898135946838\n",
      "Train  ALL Spearman: 0.961037050539693\n",
      "32.2396936416626\n",
      "Test Epoch:193 [(0%)]\t Loss: 0.1584  Pearson:0.9362 Spearman:0.9333\n",
      "Test : Loss:0.1619 \n",
      "pearson： 0.939516098266004 ALL Pearson: 0.9374117539083\n",
      "spearman： 0.927189045344144 ALL Spearman: 0.924189410251157\n",
      "time: 36.840219020843506\n",
      "0.0026999999999999997\n",
      "Train Epoch:194 [(0%)]\t Loss: 0.1061  Pearson:0.9577 Spearman:0.9508\n",
      "Train ALL Pearson: 0.9709561983631468\n",
      "Train  ALL Spearman: 0.9612851000666045\n",
      "32.20920777320862\n",
      "Test Epoch:194 [(0%)]\t Loss: 0.1528  Pearson:0.9412 Spearman:0.9233\n",
      "Test : Loss:0.1595 \n",
      "pearson： 0.934490362818244 ALL Pearson: 0.9372092744292218\n",
      "spearman： 0.9226756708482853 ALL Spearman: 0.9241151570478897\n",
      "time: 36.61579370498657\n",
      "0.0026999999999999997\n",
      "Train Epoch:195 [(0%)]\t Loss: 0.0971  Pearson:0.9743 Spearman:0.9614\n",
      "Train ALL Pearson: 0.9699952872442379\n",
      "Train  ALL Spearman: 0.9604447856746696\n",
      "32.21061682701111\n",
      "Test Epoch:195 [(0%)]\t Loss: 0.1523  Pearson:0.9618 Spearman:0.9512\n",
      "Test : Loss:0.1565 \n",
      "pearson： 0.9443402675557361 ALL Pearson: 0.9372752457758103\n",
      "spearman： 0.9283082988477417 ALL Spearman: 0.9240396883024297\n",
      "time: 36.731876373291016\n",
      "0.0026999999999999997\n",
      "Train Epoch:196 [(0%)]\t Loss: 0.0866  Pearson:0.9817 Spearman:0.9737\n",
      "Train ALL Pearson: 0.9700189699624757\n",
      "Train  ALL Spearman: 0.9604491869380782\n",
      "32.53486728668213\n",
      "Test Epoch:196 [(0%)]\t Loss: 0.1654  Pearson:0.9292 Spearman:0.9245\n",
      "Test : Loss:0.1559 \n",
      "pearson： 0.9411378593364308 ALL Pearson: 0.9374797979017828\n",
      "spearman： 0.9319866641448763 ALL Spearman: 0.9243547695169432\n",
      "time: 37.050580739974976\n",
      "0.0008099999999999998\n",
      "Train Epoch:197 [(0%)]\t Loss: 0.0974  Pearson:0.9729 Spearman:0.9614\n",
      "Train ALL Pearson: 0.9688140555777723\n",
      "Train  ALL Spearman: 0.9588018810281729\n",
      "31.8035409450531\n",
      "Test Epoch:197 [(0%)]\t Loss: 0.1610  Pearson:0.9376 Spearman:0.9167\n",
      "Test : Loss:0.1589 \n",
      "pearson： 0.9371003241883133 ALL Pearson: 0.9379884793350282\n",
      "spearman： 0.9228991768067857 ALL Spearman: 0.924709525332318\n",
      "time: 36.25811243057251\n",
      "0.0008099999999999998\n",
      "Train Epoch:198 [(0%)]\t Loss: 0.1028  Pearson:0.9638 Spearman:0.9517\n",
      "Train ALL Pearson: 0.9686161576116638\n",
      "Train  ALL Spearman: 0.9582322459297233\n",
      "31.900418758392334\n",
      "Test Epoch:198 [(0%)]\t Loss: 0.1724  Pearson:0.9289 Spearman:0.9154\n",
      "Test : Loss:0.1562 \n",
      "pearson： 0.9345882183601296 ALL Pearson: 0.9380595102191234\n",
      "spearman： 0.9199840973826579 ALL Spearman: 0.9249109784651682\n",
      "time: 36.30300712585449\n",
      "0.0008099999999999998\n",
      "Train Epoch:199 [(0%)]\t Loss: 0.0941  Pearson:0.9757 Spearman:0.9626\n",
      "Train ALL Pearson: 0.968679874451609\n",
      "Train  ALL Spearman: 0.9583681061239591\n",
      "32.21617770195007\n",
      "Test Epoch:199 [(0%)]\t Loss: 0.1507  Pearson:0.9446 Spearman:0.9390\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9381357638218389 ALL Pearson: 0.938049268538628\n",
      "spearman： 0.9252133870736675 ALL Spearman: 0.9249469692099681\n",
      "time: 36.69474148750305\n",
      "0.0008099999999999998\n",
      "Train Epoch:200 [(0%)]\t Loss: 0.0968  Pearson:0.9743 Spearman:0.9671\n",
      "Train ALL Pearson: 0.9690287133734768\n",
      "Train  ALL Spearman: 0.9584874141374684\n",
      "31.937408924102783\n",
      "Test Epoch:200 [(0%)]\t Loss: 0.1664  Pearson:0.9058 Spearman:0.9027\n",
      "Test : Loss:0.1586 \n",
      "pearson： 0.9311687528755935 ALL Pearson: 0.9380192724128019\n",
      "spearman： 0.9181136402418346 ALL Spearman: 0.9249548529697285\n",
      "time: 36.49713611602783\n",
      "0.0008099999999999998\n",
      "Train Epoch:201 [(0%)]\t Loss: 0.0945  Pearson:0.9744 Spearman:0.9655\n",
      "Train ALL Pearson: 0.9691282152788279\n",
      "Train  ALL Spearman: 0.9591137341841499\n",
      "32.48190999031067\n",
      "Test Epoch:201 [(0%)]\t Loss: 0.1636  Pearson:0.9281 Spearman:0.9019\n",
      "Test : Loss:0.1567 \n",
      "pearson： 0.9349238102461569 ALL Pearson: 0.9379271468579446\n",
      "spearman： 0.9188240275275542 ALL Spearman: 0.9248682417377535\n",
      "time: 36.97420573234558\n",
      "0.0008099999999999998\n",
      "Train Epoch:202 [(0%)]\t Loss: 0.0949  Pearson:0.9726 Spearman:0.9659\n",
      "Train ALL Pearson: 0.9693239747413126\n",
      "Train  ALL Spearman: 0.9598105941084805\n",
      "32.35958003997803\n",
      "Test Epoch:202 [(0%)]\t Loss: 0.1643  Pearson:0.9424 Spearman:0.9312\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9357968910698399 ALL Pearson: 0.9378155832060905\n",
      "spearman： 0.9220224408889082 ALL Spearman: 0.9247208618517329\n",
      "time: 36.982733964920044\n",
      "0.0008099999999999998\n",
      "Train Epoch:203 [(0%)]\t Loss: 0.1066  Pearson:0.9508 Spearman:0.9448\n",
      "Train ALL Pearson: 0.968983706182543\n",
      "Train  ALL Spearman: 0.9593840242761233\n",
      "31.882560968399048\n",
      "Test Epoch:203 [(0%)]\t Loss: 0.1579  Pearson:0.9420 Spearman:0.9322\n",
      "Test : Loss:0.1606 \n",
      "pearson： 0.937632432853995 ALL Pearson: 0.9377125990678142\n",
      "spearman： 0.9250419527585464 ALL Spearman: 0.9246624536219649\n",
      "time: 36.193636417388916\n",
      "0.0008099999999999998\n",
      "Train Epoch:204 [(0%)]\t Loss: 0.1059  Pearson:0.9628 Spearman:0.9407\n",
      "Train ALL Pearson: 0.9693239410531151\n",
      "Train  ALL Spearman: 0.9598965324745049\n",
      "31.948073625564575\n",
      "Test Epoch:204 [(0%)]\t Loss: 0.1545  Pearson:0.9269 Spearman:0.9289\n",
      "Test : Loss:0.1597 \n",
      "pearson： 0.9351955313452492 ALL Pearson: 0.9376762784506588\n",
      "spearman： 0.9247568992336409 ALL Spearman: 0.9246233295025614\n",
      "time: 36.443631649017334\n",
      "0.0008099999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:205 [(0%)]\t Loss: 0.0964  Pearson:0.9759 Spearman:0.9571\n",
      "Train ALL Pearson: 0.96984730931814\n",
      "Train  ALL Spearman: 0.9599737427225749\n",
      "31.948406219482422\n",
      "Test Epoch:205 [(0%)]\t Loss: 0.1483  Pearson:0.9474 Spearman:0.9339\n",
      "Test : Loss:0.1600 \n",
      "pearson： 0.9428192053345811 ALL Pearson: 0.9376919143029816\n",
      "spearman： 0.9292584021050018 ALL Spearman: 0.9247254872513929\n",
      "time: 36.58188557624817\n",
      "0.0008099999999999998\n",
      "Train Epoch:206 [(0%)]\t Loss: 0.0955  Pearson:0.9613 Spearman:0.9593\n",
      "Train ALL Pearson: 0.9692973011588859\n",
      "Train  ALL Spearman: 0.9588969857885103\n",
      "32.05206775665283\n",
      "Test Epoch:206 [(0%)]\t Loss: 0.1564  Pearson:0.9244 Spearman:0.9133\n",
      "Test : Loss:0.1557 \n",
      "pearson： 0.9351086034598128 ALL Pearson: 0.9378038473474889\n",
      "spearman： 0.9242212036471478 ALL Spearman: 0.9248884161660482\n",
      "time: 36.49164390563965\n",
      "0.0008099999999999998\n",
      "Train Epoch:207 [(0%)]\t Loss: 0.1019  Pearson:0.9708 Spearman:0.9618\n",
      "Train ALL Pearson: 0.9687610471820433\n",
      "Train  ALL Spearman: 0.9583169857977385\n",
      "31.842418670654297\n",
      "Test Epoch:207 [(0%)]\t Loss: 0.1670  Pearson:0.9244 Spearman:0.9190\n",
      "Test : Loss:0.1571 \n",
      "pearson： 0.9344313495622214 ALL Pearson: 0.9377148452605654\n",
      "spearman： 0.9233695713143145 ALL Spearman: 0.9248391490683943\n",
      "time: 36.40567326545715\n",
      "0.0008099999999999998\n",
      "Train Epoch:208 [(0%)]\t Loss: 0.0954  Pearson:0.9692 Spearman:0.9665\n",
      "Train ALL Pearson: 0.9697556999280107\n",
      "Train  ALL Spearman: 0.960342711046445\n",
      "31.9846351146698\n",
      "Test Epoch:208 [(0%)]\t Loss: 0.1387  Pearson:0.9517 Spearman:0.9360\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.939328432525782 ALL Pearson: 0.9377187822009272\n",
      "spearman： 0.9236731732492977 ALL Spearman: 0.9248931706398075\n",
      "time: 36.52911710739136\n",
      "0.0008099999999999998\n",
      "Train Epoch:209 [(0%)]\t Loss: 0.0925  Pearson:0.9723 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9695482952018253\n",
      "Train  ALL Spearman: 0.9597630716931943\n",
      "32.18275260925293\n",
      "Test Epoch:209 [(0%)]\t Loss: 0.1614  Pearson:0.9458 Spearman:0.9204\n",
      "Test : Loss:0.1569 \n",
      "pearson： 0.9373614159119928 ALL Pearson: 0.9377146281586692\n",
      "spearman： 0.9231424534214852 ALL Spearman: 0.9249161641637569\n",
      "time: 36.51936221122742\n",
      "0.0008099999999999998\n",
      "Train Epoch:210 [(0%)]\t Loss: 0.1019  Pearson:0.9711 Spearman:0.9633\n",
      "Train ALL Pearson: 0.969658042372249\n",
      "Train  ALL Spearman: 0.9599961577485131\n",
      "32.342459201812744\n",
      "Test Epoch:210 [(0%)]\t Loss: 0.1547  Pearson:0.9282 Spearman:0.9143\n",
      "Test : Loss:0.1585 \n",
      "pearson： 0.9369894405850193 ALL Pearson: 0.9376380585187232\n",
      "spearman： 0.9225337632706377 ALL Spearman: 0.9248332003200955\n",
      "time: 36.93198895454407\n",
      "0.0008099999999999998\n",
      "Train Epoch:211 [(0%)]\t Loss: 0.1108  Pearson:0.9613 Spearman:0.9354\n",
      "Train ALL Pearson: 0.9698271932171806\n",
      "Train  ALL Spearman: 0.960395998660513\n",
      "31.814380407333374\n",
      "Test Epoch:211 [(0%)]\t Loss: 0.1587  Pearson:0.9422 Spearman:0.9188\n",
      "Test : Loss:0.1586 \n",
      "pearson： 0.9386118669200482 ALL Pearson: 0.9376971973439516\n",
      "spearman： 0.9218730489562983 ALL Spearman: 0.9248912528627165\n",
      "time: 36.11500096321106\n",
      "0.0008099999999999998\n",
      "Train Epoch:212 [(0%)]\t Loss: 0.1037  Pearson:0.9752 Spearman:0.9613\n",
      "Train ALL Pearson: 0.9701266621659093\n",
      "Train  ALL Spearman: 0.9602645066295705\n",
      "32.002405405044556\n",
      "Test Epoch:212 [(0%)]\t Loss: 0.1572  Pearson:0.9407 Spearman:0.9267\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9371888519631082 ALL Pearson: 0.9376419033475536\n",
      "spearman： 0.9223624097349443 ALL Spearman: 0.9248382400769475\n",
      "time: 36.328017473220825\n",
      "0.0008099999999999998\n",
      "Train Epoch:213 [(0%)]\t Loss: 0.1030  Pearson:0.9660 Spearman:0.9520\n",
      "Train ALL Pearson: 0.9695603698308037\n",
      "Train  ALL Spearman: 0.9598558535566741\n",
      "31.85114049911499\n",
      "Test Epoch:213 [(0%)]\t Loss: 0.1560  Pearson:0.9485 Spearman:0.9243\n",
      "Test : Loss:0.1597 \n",
      "pearson： 0.9414955268551146 ALL Pearson: 0.937549651920111\n",
      "spearman： 0.924286172595468 ALL Spearman: 0.9247403927153761\n",
      "time: 36.61061763763428\n",
      "0.0008099999999999998\n",
      "Train Epoch:214 [(0%)]\t Loss: 0.1044  Pearson:0.9731 Spearman:0.9663\n",
      "Train ALL Pearson: 0.9702320289930235\n",
      "Train  ALL Spearman: 0.9606478053816256\n",
      "31.615434646606445\n",
      "Test Epoch:214 [(0%)]\t Loss: 0.1671  Pearson:0.9348 Spearman:0.9269\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.9372933571316815 ALL Pearson: 0.9376072716137898\n",
      "spearman： 0.9258312314518048 ALL Spearman: 0.9246518798131331\n",
      "time: 35.932050466537476\n",
      "0.0008099999999999998\n",
      "Train Epoch:215 [(0%)]\t Loss: 0.0970  Pearson:0.9694 Spearman:0.9586\n",
      "Train ALL Pearson: 0.9702068978370219\n",
      "Train  ALL Spearman: 0.9609229685126386\n",
      "32.22829031944275\n",
      "Test Epoch:215 [(0%)]\t Loss: 0.1477  Pearson:0.9252 Spearman:0.9232\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9362059275397965 ALL Pearson: 0.9375635648008619\n",
      "spearman： 0.9235480235277472 ALL Spearman: 0.9246851515955635\n",
      "time: 36.947776317596436\n",
      "0.0008099999999999998\n",
      "Train Epoch:216 [(0%)]\t Loss: 0.1011  Pearson:0.9683 Spearman:0.9492\n",
      "Train ALL Pearson: 0.9706046692404416\n",
      "Train  ALL Spearman: 0.9612084907469531\n",
      "31.757720232009888\n",
      "Test Epoch:216 [(0%)]\t Loss: 0.1584  Pearson:0.9321 Spearman:0.9332\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.9321723295400145 ALL Pearson: 0.9375067812677619\n",
      "spearman： 0.9220556692389971 ALL Spearman: 0.9246612299572162\n",
      "time: 36.09132981300354\n",
      "0.0008099999999999998\n",
      "Train Epoch:217 [(0%)]\t Loss: 0.1003  Pearson:0.9743 Spearman:0.9621\n",
      "Train ALL Pearson: 0.9701568667408518\n",
      "Train  ALL Spearman: 0.9604692185138742\n",
      "32.00224304199219\n",
      "Test Epoch:217 [(0%)]\t Loss: 0.1490  Pearson:0.9474 Spearman:0.9256\n",
      "Test : Loss:0.1572 \n",
      "pearson： 0.9388284832198316 ALL Pearson: 0.9375045420244054\n",
      "spearman： 0.924250938044696 ALL Spearman: 0.9246304637800145\n",
      "time: 36.41282868385315\n",
      "0.0008099999999999998\n",
      "Train Epoch:218 [(0%)]\t Loss: 0.1038  Pearson:0.9617 Spearman:0.9559\n",
      "Train ALL Pearson: 0.9694315676382341\n",
      "Train  ALL Spearman: 0.9600231969836885\n",
      "32.00961971282959\n",
      "Test Epoch:218 [(0%)]\t Loss: 0.1503  Pearson:0.9339 Spearman:0.9147\n",
      "Test : Loss:0.1594 \n",
      "pearson： 0.939810488644936 ALL Pearson: 0.9375380491329731\n",
      "spearman： 0.9231964529080492 ALL Spearman: 0.9246129251034841\n",
      "time: 36.41220712661743\n",
      "0.0008099999999999998\n",
      "Train Epoch:219 [(0%)]\t Loss: 0.1107  Pearson:0.9578 Spearman:0.9503\n",
      "Train ALL Pearson: 0.969536314343609\n",
      "Train  ALL Spearman: 0.9593299053983496\n",
      "32.07529306411743\n",
      "Test Epoch:219 [(0%)]\t Loss: 0.1505  Pearson:0.9485 Spearman:0.9245\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.9397595294091541 ALL Pearson: 0.937628389839663\n",
      "spearman： 0.9244081715224257 ALL Spearman: 0.9247030994956567\n",
      "time: 36.56285381317139\n",
      "0.00024299999999999994\n",
      "Train Epoch:220 [(0%)]\t Loss: 0.0988  Pearson:0.9748 Spearman:0.9667\n",
      "Train ALL Pearson: 0.9695590410142234\n",
      "Train  ALL Spearman: 0.9597874168246551\n",
      "32.18493962287903\n",
      "Test Epoch:220 [(0%)]\t Loss: 0.1537  Pearson:0.9419 Spearman:0.9190\n",
      "Test : Loss:0.1585 \n",
      "pearson： 0.9398212008413942 ALL Pearson: 0.9380339163142918\n",
      "spearman： 0.9267730871420535 ALL Spearman: 0.9249021808919441\n",
      "time: 36.83045029640198\n",
      "0.00024299999999999994\n",
      "Train Epoch:221 [(0%)]\t Loss: 0.0969  Pearson:0.9610 Spearman:0.9557\n",
      "Train ALL Pearson: 0.9691560531474084\n",
      "Train  ALL Spearman: 0.9603608485781727\n",
      "32.213446855545044\n",
      "Test Epoch:221 [(0%)]\t Loss: 0.1458  Pearson:0.9480 Spearman:0.9315\n",
      "Test : Loss:0.1583 \n",
      "pearson： 0.9398036640621427 ALL Pearson: 0.9380368082858143\n",
      "spearman： 0.9286813329229271 ALL Spearman: 0.9248862886435474\n",
      "time: 36.69300985336304\n",
      "0.00024299999999999994\n",
      "Train Epoch:222 [(0%)]\t Loss: 0.1053  Pearson:0.9582 Spearman:0.9418\n",
      "Train ALL Pearson: 0.9693761436161731\n",
      "Train  ALL Spearman: 0.9590232192211284\n",
      "31.70078420639038\n",
      "Test Epoch:222 [(0%)]\t Loss: 0.1659  Pearson:0.9379 Spearman:0.9219\n",
      "Test : Loss:0.1583 \n",
      "pearson： 0.936739970149586 ALL Pearson: 0.9380142026954782\n",
      "spearman： 0.923273016911886 ALL Spearman: 0.9249171072847276\n",
      "time: 36.04839110374451\n",
      "0.00024299999999999994\n",
      "Train Epoch:223 [(0%)]\t Loss: 0.0905  Pearson:0.9736 Spearman:0.9612\n",
      "Train ALL Pearson: 0.9700397417260535\n",
      "Train  ALL Spearman: 0.9601779086606249\n",
      "31.878620386123657\n",
      "Test Epoch:223 [(0%)]\t Loss: 0.1532  Pearson:0.9476 Spearman:0.9266\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9382689288630182 ALL Pearson: 0.9380058810664593\n",
      "spearman： 0.9232422767914946 ALL Spearman: 0.9249151484386055\n",
      "time: 36.350186347961426\n",
      "0.00024299999999999994\n",
      "Train Epoch:224 [(0%)]\t Loss: 0.1058  Pearson:0.9657 Spearman:0.9440\n",
      "Train ALL Pearson: 0.9691190415137056\n",
      "Train  ALL Spearman: 0.9591801701097212\n",
      "32.186883211135864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:224 [(0%)]\t Loss: 0.1653  Pearson:0.9243 Spearman:0.9146\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.9371094690862882 ALL Pearson: 0.9379835961233856\n",
      "spearman： 0.9218821020632966 ALL Spearman: 0.9249346987642981\n",
      "time: 36.61031222343445\n",
      "0.00024299999999999994\n",
      "Train Epoch:225 [(0%)]\t Loss: 0.1043  Pearson:0.9654 Spearman:0.9484\n",
      "Train ALL Pearson: 0.9690321797866605\n",
      "Train  ALL Spearman: 0.9592588815422219\n",
      "31.905237436294556\n",
      "Test Epoch:225 [(0%)]\t Loss: 0.1546  Pearson:0.9379 Spearman:0.9245\n",
      "Test : Loss:0.1571 \n",
      "pearson： 0.9402740009555124 ALL Pearson: 0.9379716784766208\n",
      "spearman： 0.9256441758505045 ALL Spearman: 0.924942163794129\n",
      "time: 36.35281038284302\n",
      "0.00024299999999999994\n",
      "Train Epoch:226 [(0%)]\t Loss: 0.1164  Pearson:0.9399 Spearman:0.9381\n",
      "Train ALL Pearson: 0.9694184018624687\n",
      "Train  ALL Spearman: 0.9596310153837961\n",
      "32.33602261543274\n",
      "Test Epoch:226 [(0%)]\t Loss: 0.1643  Pearson:0.9164 Spearman:0.9016\n",
      "Test : Loss:0.1598 \n",
      "pearson： 0.9341342111812981 ALL Pearson: 0.9379833071442958\n",
      "spearman： 0.9206367985639627 ALL Spearman: 0.9249590893686904\n",
      "time: 37.002952337265015\n",
      "0.00024299999999999994\n",
      "Train Epoch:227 [(0%)]\t Loss: 0.0989  Pearson:0.9657 Spearman:0.9492\n",
      "Train ALL Pearson: 0.9698687299499786\n",
      "Train  ALL Spearman: 0.9601564511615369\n",
      "32.177417516708374\n",
      "Test Epoch:227 [(0%)]\t Loss: 0.1562  Pearson:0.9440 Spearman:0.9389\n",
      "Test : Loss:0.1583 \n",
      "pearson： 0.9416851481923949 ALL Pearson: 0.9379860399640264\n",
      "spearman： 0.9295798648902296 ALL Spearman: 0.9249671579666076\n",
      "time: 36.71096324920654\n",
      "0.00024299999999999994\n",
      "Train Epoch:228 [(0%)]\t Loss: 0.0991  Pearson:0.9726 Spearman:0.9634\n",
      "Train ALL Pearson: 0.9694144213690195\n",
      "Train  ALL Spearman: 0.9597484412854365\n",
      "32.49966287612915\n",
      "Test Epoch:228 [(0%)]\t Loss: 0.1556  Pearson:0.9379 Spearman:0.9310\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9379024272286959 ALL Pearson: 0.9379945382665138\n",
      "spearman： 0.9264089262072264 ALL Spearman: 0.9249806058192435\n",
      "time: 36.95923209190369\n",
      "0.00024299999999999994\n",
      "Train Epoch:229 [(0%)]\t Loss: 0.1013  Pearson:0.9664 Spearman:0.9532\n",
      "Train ALL Pearson: 0.9700495524092524\n",
      "Train  ALL Spearman: 0.9606965072152475\n",
      "32.23658847808838\n",
      "Test Epoch:229 [(0%)]\t Loss: 0.1562  Pearson:0.9250 Spearman:0.9191\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9363067291305722 ALL Pearson: 0.9379713306482198\n",
      "spearman： 0.9248335272277537 ALL Spearman: 0.9249479218648217\n",
      "time: 36.64617443084717\n",
      "0.00024299999999999994\n",
      "Train Epoch:230 [(0%)]\t Loss: 0.1006  Pearson:0.9776 Spearman:0.9562\n",
      "Train ALL Pearson: 0.9695510389138892\n",
      "Train  ALL Spearman: 0.9598843201428503\n",
      "32.237112522125244\n",
      "Test Epoch:230 [(0%)]\t Loss: 0.1498  Pearson:0.9139 Spearman:0.9033\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.931269842287136 ALL Pearson: 0.9379932838504664\n",
      "spearman： 0.9178835331551242 ALL Spearman: 0.9249596672686323\n",
      "time: 36.56380224227905\n",
      "0.00024299999999999994\n",
      "Train Epoch:231 [(0%)]\t Loss: 0.0949  Pearson:0.9659 Spearman:0.9484\n",
      "Train ALL Pearson: 0.9696144242819738\n",
      "Train  ALL Spearman: 0.9605067693687335\n",
      "32.16619610786438\n",
      "Test Epoch:231 [(0%)]\t Loss: 0.1577  Pearson:0.9351 Spearman:0.8883\n",
      "Test : Loss:0.1574 \n",
      "pearson： 0.9379702471567234 ALL Pearson: 0.9379836557156214\n",
      "spearman： 0.917707517228767 ALL Spearman: 0.9249670897626804\n",
      "time: 36.43551564216614\n",
      "0.00024299999999999994\n",
      "Train Epoch:232 [(0%)]\t Loss: 0.1043  Pearson:0.9672 Spearman:0.9442\n",
      "Train ALL Pearson: 0.9708180915089382\n",
      "Train  ALL Spearman: 0.9610072459003113\n",
      "32.03630518913269\n",
      "Test Epoch:232 [(0%)]\t Loss: 0.1558  Pearson:0.9448 Spearman:0.9209\n",
      "Test : Loss:0.1593 \n",
      "pearson： 0.9430230915693202 ALL Pearson: 0.9379483501592573\n",
      "spearman： 0.9250300114229639 ALL Spearman: 0.92494067357499\n",
      "time: 36.54885673522949\n",
      "0.00024299999999999994\n",
      "Train Epoch:233 [(0%)]\t Loss: 0.1020  Pearson:0.9714 Spearman:0.9597\n",
      "Train ALL Pearson: 0.969608558617001\n",
      "Train  ALL Spearman: 0.9599919109696009\n",
      "32.35786533355713\n",
      "Test Epoch:233 [(0%)]\t Loss: 0.1484  Pearson:0.9464 Spearman:0.9224\n",
      "Test : Loss:0.1600 \n",
      "pearson： 0.9413516515897633 ALL Pearson: 0.9379279253702234\n",
      "spearman： 0.9282383654847796 ALL Spearman: 0.9249405019651087\n",
      "time: 36.95039176940918\n",
      "0.00024299999999999994\n",
      "Train Epoch:234 [(0%)]\t Loss: 0.0973  Pearson:0.9660 Spearman:0.9578\n",
      "Train ALL Pearson: 0.9693948603154714\n",
      "Train  ALL Spearman: 0.9593509032982054\n",
      "32.03344011306763\n",
      "Test Epoch:234 [(0%)]\t Loss: 0.1495  Pearson:0.9395 Spearman:0.9339\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.9383247776256038 ALL Pearson: 0.9379154881207021\n",
      "spearman： 0.9262564105896726 ALL Spearman: 0.9249241454349231\n",
      "time: 36.474015951156616\n",
      "0.00024299999999999994\n",
      "Train Epoch:235 [(0%)]\t Loss: 0.0978  Pearson:0.9715 Spearman:0.9596\n",
      "Train ALL Pearson: 0.9697992730401853\n",
      "Train  ALL Spearman: 0.9600550246957663\n",
      "32.15463709831238\n",
      "Test Epoch:235 [(0%)]\t Loss: 0.1567  Pearson:0.9498 Spearman:0.9456\n",
      "Test : Loss:0.1581 \n",
      "pearson： 0.9406643066636268 ALL Pearson: 0.937909855272806\n",
      "spearman： 0.9275309084460243 ALL Spearman: 0.9249243801701615\n",
      "time: 36.88711953163147\n",
      "0.00024299999999999994\n",
      "Train Epoch:236 [(0%)]\t Loss: 0.0921  Pearson:0.9764 Spearman:0.9616\n",
      "Train ALL Pearson: 0.9702409219398019\n",
      "Train  ALL Spearman: 0.9609999277862097\n",
      "31.9329776763916\n",
      "Test Epoch:236 [(0%)]\t Loss: 0.1477  Pearson:0.9480 Spearman:0.9248\n",
      "Test : Loss:0.1573 \n",
      "pearson： 0.9420888445656885 ALL Pearson: 0.9379001269199785\n",
      "spearman： 0.9279164052114856 ALL Spearman: 0.9249200385868431\n",
      "time: 36.32056951522827\n",
      "0.00024299999999999994\n",
      "Train Epoch:237 [(0%)]\t Loss: 0.0929  Pearson:0.9662 Spearman:0.9541\n",
      "Train ALL Pearson: 0.9696173190820943\n",
      "Train  ALL Spearman: 0.9601960947917112\n",
      "32.03857064247131\n",
      "Test Epoch:237 [(0%)]\t Loss: 0.1568  Pearson:0.9470 Spearman:0.9268\n",
      "Test : Loss:0.1580 \n",
      "pearson： 0.9393367811161969 ALL Pearson: 0.9379080691015024\n",
      "spearman： 0.9243889072708545 ALL Spearman: 0.9249305749659941\n",
      "time: 36.559120655059814\n",
      "0.00024299999999999994\n",
      "Train Epoch:238 [(0%)]\t Loss: 0.0894  Pearson:0.9767 Spearman:0.9709\n",
      "Train ALL Pearson: 0.9699543673161358\n",
      "Train  ALL Spearman: 0.9603793376928864\n",
      "32.06216621398926\n",
      "Test Epoch:238 [(0%)]\t Loss: 0.1518  Pearson:0.9498 Spearman:0.9488\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.9405590049348619 ALL Pearson: 0.9378702334182258\n",
      "spearman： 0.9282380109531014 ALL Spearman: 0.924896397464734\n",
      "time: 36.41260886192322\n",
      "0.00024299999999999994\n",
      "Train Epoch:239 [(0%)]\t Loss: 0.1038  Pearson:0.9702 Spearman:0.9561\n",
      "Train ALL Pearson: 0.9687123676273054\n",
      "Train  ALL Spearman: 0.9581899518221977\n",
      "32.04187297821045\n",
      "Test Epoch:239 [(0%)]\t Loss: 0.1557  Pearson:0.9386 Spearman:0.9256\n",
      "Test : Loss:0.1573 \n",
      "pearson： 0.9395819560141055 ALL Pearson: 0.9378449182394671\n",
      "spearman： 0.9265002917697255 ALL Spearman: 0.9248723721089105\n",
      "time: 36.47844934463501\n",
      "0.00024299999999999994\n",
      "Train Epoch:240 [(0%)]\t Loss: 0.1031  Pearson:0.9751 Spearman:0.9670\n",
      "Train ALL Pearson: 0.9694779081176714\n",
      "Train  ALL Spearman: 0.959902687266757\n",
      "32.03764629364014\n",
      "Test Epoch:240 [(0%)]\t Loss: 0.1562  Pearson:0.9422 Spearman:0.9246\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.9356804053226714 ALL Pearson: 0.9378360549300386\n",
      "spearman： 0.9193055076321744 ALL Spearman: 0.9248976200293331\n",
      "time: 36.38325238227844\n",
      "7.289999999999998e-05\n",
      "Train Epoch:241 [(0%)]\t Loss: 0.0963  Pearson:0.9721 Spearman:0.9452\n",
      "Train ALL Pearson: 0.9690792086171478\n",
      "Train  ALL Spearman: 0.9584087749859317\n",
      "31.955146312713623\n",
      "Test Epoch:241 [(0%)]\t Loss: 0.1520  Pearson:0.9370 Spearman:0.9279\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9355667252540282 ALL Pearson: 0.9380544398939804\n",
      "spearman： 0.9219892984103253 ALL Spearman: 0.924941389349537\n",
      "time: 36.19978404045105\n",
      "7.289999999999998e-05\n",
      "Train Epoch:242 [(0%)]\t Loss: 0.0993  Pearson:0.9706 Spearman:0.9445\n",
      "Train ALL Pearson: 0.9693034826019054\n",
      "Train  ALL Spearman: 0.9592319128247173\n",
      "32.07501745223999\n",
      "Test Epoch:242 [(0%)]\t Loss: 0.1480  Pearson:0.9348 Spearman:0.9193\n",
      "Test : Loss:0.1569 \n",
      "pearson： 0.936979021328428 ALL Pearson: 0.9380444245869042\n",
      "spearman： 0.9236496184197268 ALL Spearman: 0.9249340372595426\n",
      "time: 36.77251148223877\n",
      "7.289999999999998e-05\n",
      "Train Epoch:243 [(0%)]\t Loss: 0.1036  Pearson:0.9620 Spearman:0.9542\n",
      "Train ALL Pearson: 0.9689227967813915\n",
      "Train  ALL Spearman: 0.9594491279960201\n",
      "32.11174201965332\n",
      "Test Epoch:243 [(0%)]\t Loss: 0.1607  Pearson:0.9385 Spearman:0.9247\n",
      "Test : Loss:0.1577 \n",
      "pearson： 0.9390234796282809 ALL Pearson: 0.9380421833616654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9267876651700242 ALL Spearman: 0.9249427952304865\n",
      "time: 36.60730028152466\n",
      "7.289999999999998e-05\n",
      "Train Epoch:244 [(0%)]\t Loss: 0.0902  Pearson:0.9737 Spearman:0.9672\n",
      "Train ALL Pearson: 0.9692783417791907\n",
      "Train  ALL Spearman: 0.958963655203122\n",
      "32.24888586997986\n",
      "Test Epoch:244 [(0%)]\t Loss: 0.1624  Pearson:0.9399 Spearman:0.9302\n",
      "Test : Loss:0.1575 \n",
      "pearson： 0.9390287916519643 ALL Pearson: 0.9380506790780355\n",
      "spearman： 0.9275653897746732 ALL Spearman: 0.924946879371462\n",
      "time: 36.84328579902649\n",
      "7.289999999999998e-05\n",
      "Train Epoch:245 [(0%)]\t Loss: 0.1118  Pearson:0.9569 Spearman:0.9493\n",
      "Train ALL Pearson: 0.9685660720869005\n",
      "Train  ALL Spearman: 0.9588248817823756\n",
      "32.301207065582275\n",
      "Test Epoch:245 [(0%)]\t Loss: 0.1554  Pearson:0.9412 Spearman:0.9308\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9373500797117675 ALL Pearson: 0.9380473605347112\n",
      "spearman： 0.922827456050268 ALL Spearman: 0.9249251281857042\n",
      "time: 36.74178194999695\n",
      "7.289999999999998e-05\n",
      "Train Epoch:246 [(0%)]\t Loss: 0.0977  Pearson:0.9750 Spearman:0.9630\n",
      "Train ALL Pearson: 0.9697726019474456\n",
      "Train  ALL Spearman: 0.960217734987225\n",
      "32.154337882995605\n",
      "Test Epoch:246 [(0%)]\t Loss: 0.1535  Pearson:0.9446 Spearman:0.9400\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.9400596707565825 ALL Pearson: 0.9380393195104879\n",
      "spearman： 0.9302151373917646 ALL Spearman: 0.9249275439523298\n",
      "time: 36.816842555999756\n",
      "7.289999999999998e-05\n",
      "Train Epoch:247 [(0%)]\t Loss: 0.1095  Pearson:0.9655 Spearman:0.9591\n",
      "Train ALL Pearson: 0.9690906282751945\n",
      "Train  ALL Spearman: 0.9593343456049906\n",
      "31.212532997131348\n",
      "Test Epoch:247 [(0%)]\t Loss: 0.1519  Pearson:0.9279 Spearman:0.9147\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9341534830492971 ALL Pearson: 0.9380247467642832\n",
      "spearman： 0.9223321513706967 ALL Spearman: 0.9249241528237377\n",
      "time: 35.598717212677\n",
      "7.289999999999998e-05\n",
      "Train Epoch:248 [(0%)]\t Loss: 0.1032  Pearson:0.9668 Spearman:0.9529\n",
      "Train ALL Pearson: 0.9690710722672505\n",
      "Train  ALL Spearman: 0.9591432486000495\n",
      "32.0520498752594\n",
      "Test Epoch:248 [(0%)]\t Loss: 0.1547  Pearson:0.9432 Spearman:0.9147\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9386151526340917 ALL Pearson: 0.9380199024401581\n",
      "spearman： 0.9242070522563789 ALL Spearman: 0.9249352403070988\n",
      "time: 36.34132385253906\n",
      "7.289999999999998e-05\n",
      "Train Epoch:249 [(0%)]\t Loss: 0.1034  Pearson:0.9720 Spearman:0.9568\n",
      "Train ALL Pearson: 0.9695237362756555\n",
      "Train  ALL Spearman: 0.9594574109360655\n",
      "31.712028741836548\n",
      "Test Epoch:249 [(0%)]\t Loss: 0.1652  Pearson:0.9304 Spearman:0.9166\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.93684663921956 ALL Pearson: 0.9380112031691189\n",
      "spearman： 0.9211796070612394 ALL Spearman: 0.9249350903868475\n",
      "time: 36.16765594482422\n",
      "7.289999999999998e-05\n",
      "Train Epoch:250 [(0%)]\t Loss: 0.1048  Pearson:0.9671 Spearman:0.9535\n",
      "Train ALL Pearson: 0.9689182598720439\n",
      "Train  ALL Spearman: 0.958642566013143\n",
      "31.775057792663574\n",
      "Test Epoch:250 [(0%)]\t Loss: 0.1577  Pearson:0.9462 Spearman:0.9295\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9375291541079733 ALL Pearson: 0.9380165348406682\n",
      "spearman： 0.9255456799913798 ALL Spearman: 0.9249327142500314\n",
      "time: 36.18664264678955\n",
      "7.289999999999998e-05\n",
      "Train Epoch:251 [(0%)]\t Loss: 0.0974  Pearson:0.9747 Spearman:0.9632\n",
      "Train ALL Pearson: 0.9694690967951437\n",
      "Train  ALL Spearman: 0.9590135519562176\n",
      "32.46644067764282\n",
      "Test Epoch:251 [(0%)]\t Loss: 0.1541  Pearson:0.9406 Spearman:0.9127\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.9383214378563118 ALL Pearson: 0.9380158041785839\n",
      "spearman： 0.9248080947866839 ALL Spearman: 0.9249134888097119\n",
      "time: 37.08695840835571\n",
      "7.289999999999998e-05\n",
      "Train Epoch:252 [(0%)]\t Loss: 0.1050  Pearson:0.9599 Spearman:0.9476\n",
      "Train ALL Pearson: 0.9689854655156521\n",
      "Train  ALL Spearman: 0.9588597659149696\n",
      "31.83411693572998\n",
      "Test Epoch:252 [(0%)]\t Loss: 0.1612  Pearson:0.9311 Spearman:0.9095\n",
      "Test : Loss:0.1570 \n",
      "pearson： 0.9349483675087079 ALL Pearson: 0.938020082383634\n",
      "spearman： 0.9189751659799757 ALL Spearman: 0.924921975770856\n",
      "time: 36.34482431411743\n",
      "7.289999999999998e-05\n",
      "Train Epoch:253 [(0%)]\t Loss: 0.1013  Pearson:0.9684 Spearman:0.9533\n",
      "Train ALL Pearson: 0.9693777539477317\n",
      "Train  ALL Spearman: 0.9596996386593838\n",
      "32.37047100067139\n",
      "Test Epoch:253 [(0%)]\t Loss: 0.1519  Pearson:0.9496 Spearman:0.9171\n",
      "Test : Loss:0.1572 \n",
      "pearson： 0.937332554457399 ALL Pearson: 0.9380152381006124\n",
      "spearman： 0.9203127809256373 ALL Spearman: 0.9249391000902007\n",
      "time: 36.79707431793213\n",
      "7.289999999999998e-05\n",
      "Train Epoch:254 [(0%)]\t Loss: 0.1138  Pearson:0.9581 Spearman:0.9548\n",
      "Train ALL Pearson: 0.9690492153869686\n",
      "Train  ALL Spearman: 0.9588259545922355\n",
      "32.09781312942505\n",
      "Test Epoch:254 [(0%)]\t Loss: 0.1525  Pearson:0.9358 Spearman:0.9186\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9376900813764462 ALL Pearson: 0.9380056556904238\n",
      "spearman： 0.924679875638745 ALL Spearman: 0.9249264629567535\n",
      "time: 36.526204109191895\n",
      "7.289999999999998e-05\n",
      "Train Epoch:255 [(0%)]\t Loss: 0.1038  Pearson:0.9703 Spearman:0.9530\n",
      "Train ALL Pearson: 0.9695191611774889\n",
      "Train  ALL Spearman: 0.9593569298585438\n",
      "31.547354459762573\n",
      "Test Epoch:255 [(0%)]\t Loss: 0.1703  Pearson:0.9343 Spearman:0.9325\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.937877826793559 ALL Pearson: 0.9380060703181179\n",
      "spearman： 0.9277066045013441 ALL Spearman: 0.9249296079827898\n",
      "time: 36.36380934715271\n",
      "7.289999999999998e-05\n",
      "Train Epoch:256 [(0%)]\t Loss: 0.0924  Pearson:0.9746 Spearman:0.9660\n",
      "Train ALL Pearson: 0.9692756761149133\n",
      "Train  ALL Spearman: 0.9588695065784644\n",
      "32.33319163322449\n",
      "Test Epoch:256 [(0%)]\t Loss: 0.1471  Pearson:0.9522 Spearman:0.9269\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9415884155010037 ALL Pearson: 0.9379982842609694\n",
      "spearman： 0.9256804773278179 ALL Spearman: 0.9249432997928723\n",
      "time: 36.76377081871033\n",
      "7.289999999999998e-05\n",
      "Train Epoch:257 [(0%)]\t Loss: 0.1048  Pearson:0.9579 Spearman:0.9389\n",
      "Train ALL Pearson: 0.9690252554346198\n",
      "Train  ALL Spearman: 0.959741163410632\n",
      "31.880908250808716\n",
      "Test Epoch:257 [(0%)]\t Loss: 0.1680  Pearson:0.9281 Spearman:0.9145\n",
      "Test : Loss:0.1584 \n",
      "pearson： 0.9344200103848099 ALL Pearson: 0.9379898641051211\n",
      "spearman： 0.9242503977867692 ALL Spearman: 0.9249424024803511\n",
      "time: 36.35846185684204\n",
      "7.289999999999998e-05\n",
      "Train Epoch:258 [(0%)]\t Loss: 0.1007  Pearson:0.9707 Spearman:0.9588\n",
      "Train ALL Pearson: 0.9700991175663571\n",
      "Train  ALL Spearman: 0.960453988400352\n",
      "31.600498914718628\n",
      "Test Epoch:258 [(0%)]\t Loss: 0.1645  Pearson:0.9389 Spearman:0.9265\n",
      "Test : Loss:0.1575 \n",
      "pearson： 0.9366044912915418 ALL Pearson: 0.9379933304514806\n",
      "spearman： 0.9264402955764812 ALL Spearman: 0.9249276088010102\n",
      "time: 36.19302725791931\n",
      "7.289999999999998e-05\n",
      "Train Epoch:259 [(0%)]\t Loss: 0.1074  Pearson:0.9736 Spearman:0.9522\n",
      "Train ALL Pearson: 0.9697078524609255\n",
      "Train  ALL Spearman: 0.9594787724811151\n",
      "32.15671968460083\n",
      "Test Epoch:259 [(0%)]\t Loss: 0.1562  Pearson:0.9385 Spearman:0.9136\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.9398805993495066 ALL Pearson: 0.9379976752840772\n",
      "spearman： 0.924999807760809 ALL Spearman: 0.9249432616573432\n",
      "time: 36.53381419181824\n",
      "7.289999999999998e-05\n",
      "Train Epoch:260 [(0%)]\t Loss: 0.0992  Pearson:0.9642 Spearman:0.9576\n",
      "Train ALL Pearson: 0.969889200903066\n",
      "Train  ALL Spearman: 0.9600550170941735\n",
      "32.35506463050842\n",
      "Test Epoch:260 [(0%)]\t Loss: 0.1524  Pearson:0.9474 Spearman:0.9390\n",
      "Test : Loss:0.1591 \n",
      "pearson： 0.9422436928395158 ALL Pearson: 0.9379899552093529\n",
      "spearman： 0.9285753677384849 ALL Spearman: 0.9249355934824821\n",
      "time: 36.78464341163635\n",
      "7.289999999999998e-05\n",
      "Train Epoch:261 [(0%)]\t Loss: 0.1051  Pearson:0.9701 Spearman:0.9607\n",
      "Train ALL Pearson: 0.9693269721664607\n",
      "Train  ALL Spearman: 0.9590123362450078\n",
      "31.874245166778564\n",
      "Test Epoch:261 [(0%)]\t Loss: 0.1629  Pearson:0.9247 Spearman:0.9066\n",
      "Test : Loss:0.1597 \n",
      "pearson： 0.9332190146194609 ALL Pearson: 0.9379873720701832\n",
      "spearman： 0.9179652207314778 ALL Spearman: 0.9249372233821407\n",
      "time: 36.196857929229736\n",
      "Test Epoch:-1 [(0%)]\t Loss: 0.1565  Pearson:0.9309 Spearman:0.9145\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.939119031928782 ALL Pearson: 0.9380595102191237\n",
      "spearman： 0.9233302458577627 ALL Spearman: 0.9249109784651682\n",
      "PLCC: [0.9390283689521908, 0.9380595102191237] SRCC: [0.9231706745873007, 0.9249109784651682]\n",
      "Split: 1 Median PLCC: 0.9385439395856572 SRCC: 0.9240408265262345\n",
      "Test Epoch:-1 [(0%)]\t Loss: 2.7623  Pearson:0.1074 Spearman:0.1133\n",
      "Test : Loss:2.7810 \n",
      "pearson： 0.0704275145789043 ALL Pearson: 0.018432473704297982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.07449762041538394 ALL Spearman: 0.023222534994157478\n",
      "0.01\n",
      "Train Epoch:0 [(0%)]\t Loss: 2.7480  Pearson:-0.0194 Spearman:0.0067\n",
      "Train ALL Pearson: -0.019867986643963064\n",
      "Train  ALL Spearman: -0.020621961108308186\n",
      "19.785245656967163\n",
      "Test Epoch:0 [(0%)]\t Loss: 0.7599  Pearson:0.1146 Spearman:0.0451\n",
      "Test : Loss:0.7351 \n",
      "pearson： 0.13213240138902949 ALL Pearson: 0.11756760629752104\n",
      "spearman： 0.09410363448045443 ALL Spearman: 0.08667513221304134\n",
      "time: 24.307794332504272\n",
      "0.01\n",
      "Train Epoch:1 [(0%)]\t Loss: 0.6317  Pearson:-0.0199 Spearman:0.0186\n",
      "Train ALL Pearson: 0.26713166930076754\n",
      "Train  ALL Spearman: 0.2443517261144972\n",
      "19.981250762939453\n",
      "Test Epoch:1 [(0%)]\t Loss: 0.6035  Pearson:0.4608 Spearman:0.4298\n",
      "Test : Loss:0.5997 \n",
      "pearson： 0.5360909038487794 ALL Pearson: 0.5387958705587976\n",
      "spearman： 0.4937905515732701 ALL Spearman: 0.49443980611954635\n",
      "time: 24.51595687866211\n",
      "0.01\n",
      "Train Epoch:2 [(0%)]\t Loss: 0.4812  Pearson:0.4546 Spearman:0.3785\n",
      "Train ALL Pearson: 0.45988772471862255\n",
      "Train  ALL Spearman: 0.4229325743529206\n",
      "19.9445321559906\n",
      "Test Epoch:2 [(0%)]\t Loss: 0.5941  Pearson:0.6403 Spearman:0.6101\n",
      "Test : Loss:0.5833 \n",
      "pearson： 0.6268335512302641 ALL Pearson: 0.6331528939216717\n",
      "spearman： 0.5875353194970905 ALL Spearman: 0.5931229852269136\n",
      "time: 24.554054975509644\n",
      "0.01\n",
      "Train Epoch:3 [(0%)]\t Loss: 0.4558  Pearson:0.4514 Spearman:0.4082\n",
      "Train ALL Pearson: 0.5371806302563051\n",
      "Train  ALL Spearman: 0.506357580334612\n",
      "19.866765022277832\n",
      "Test Epoch:3 [(0%)]\t Loss: 0.5507  Pearson:0.7606 Spearman:0.6946\n",
      "Test : Loss:0.5629 \n",
      "pearson： 0.6833862776091251 ALL Pearson: 0.6778557045791607\n",
      "spearman： 0.644674048708793 ALL Spearman: 0.6393568834107445\n",
      "time: 24.746199369430542\n",
      "0.01\n",
      "Train Epoch:4 [(0%)]\t Loss: 0.3598  Pearson:0.6001 Spearman:0.5525\n",
      "Train ALL Pearson: 0.5611441588961785\n",
      "Train  ALL Spearman: 0.5248265712699394\n",
      "20.05016779899597\n",
      "Test Epoch:4 [(0%)]\t Loss: 0.5718  Pearson:0.6760 Spearman:0.6214\n",
      "Test : Loss:0.5867 \n",
      "pearson： 0.702290046483663 ALL Pearson: 0.7026515750085369\n",
      "spearman： 0.6680836886804797 ALL Spearman: 0.6707443282441897\n",
      "time: 24.722668409347534\n",
      "0.01\n",
      "Train Epoch:5 [(0%)]\t Loss: 0.3747  Pearson:0.6072 Spearman:0.5329\n",
      "Train ALL Pearson: 0.5980130980006436\n",
      "Train  ALL Spearman: 0.5589948767523298\n",
      "19.622255563735962\n",
      "Test Epoch:5 [(0%)]\t Loss: 0.5417  Pearson:0.7064 Spearman:0.6867\n",
      "Test : Loss:0.5434 \n",
      "pearson： 0.6959657408454856 ALL Pearson: 0.7162018335876357\n",
      "spearman： 0.6677745342287692 ALL Spearman: 0.6888282258776931\n",
      "time: 24.433712482452393\n",
      "0.01\n",
      "Train Epoch:6 [(0%)]\t Loss: 0.4074  Pearson:0.5913 Spearman:0.5791\n",
      "Train ALL Pearson: 0.6126738181290667\n",
      "Train  ALL Spearman: 0.57975740392068\n",
      "20.178645849227905\n",
      "Test Epoch:6 [(0%)]\t Loss: 0.5645  Pearson:0.7099 Spearman:0.6797\n",
      "Test : Loss:0.5487 \n",
      "pearson： 0.7230442678561201 ALL Pearson: 0.7297870390135578\n",
      "spearman： 0.6985286678959818 ALL Spearman: 0.7055845542633542\n",
      "time: 24.927502870559692\n",
      "0.01\n",
      "Train Epoch:7 [(0%)]\t Loss: 0.3500  Pearson:0.6540 Spearman:0.6210\n",
      "Train ALL Pearson: 0.6401394201269741\n",
      "Train  ALL Spearman: 0.6119212951964662\n",
      "19.981594800949097\n",
      "Test Epoch:7 [(0%)]\t Loss: 0.4991  Pearson:0.7048 Spearman:0.6662\n",
      "Test : Loss:0.5315 \n",
      "pearson： 0.7292381074328077 ALL Pearson: 0.7375493239374212\n",
      "spearman： 0.704769883484284 ALL Spearman: 0.7141449255744206\n",
      "time: 24.8057062625885\n",
      "0.01\n",
      "Train Epoch:8 [(0%)]\t Loss: 0.3802  Pearson:0.6306 Spearman:0.5709\n",
      "Train ALL Pearson: 0.6381626929190453\n",
      "Train  ALL Spearman: 0.6065527612591392\n",
      "19.939608097076416\n",
      "Test Epoch:8 [(0%)]\t Loss: 0.5683  Pearson:0.7042 Spearman:0.6325\n",
      "Test : Loss:0.5325 \n",
      "pearson： 0.7292787535417599 ALL Pearson: 0.7379032270570128\n",
      "spearman： 0.699368092133145 ALL Spearman: 0.7146014598819488\n",
      "time: 24.650655508041382\n",
      "0.01\n",
      "Train Epoch:9 [(0%)]\t Loss: 0.3472  Pearson:0.6641 Spearman:0.5745\n",
      "Train ALL Pearson: 0.6459133198872778\n",
      "Train  ALL Spearman: 0.6112721664317559\n",
      "19.944435834884644\n",
      "Test Epoch:9 [(0%)]\t Loss: 0.5317  Pearson:0.7495 Spearman:0.6986\n",
      "Test : Loss:0.5345 \n",
      "pearson： 0.7424970684244788 ALL Pearson: 0.7422641675350875\n",
      "spearman： 0.7186729922756175 ALL Spearman: 0.721185688763992\n",
      "time: 24.56395435333252\n",
      "0.01\n",
      "Train Epoch:10 [(0%)]\t Loss: 0.3448  Pearson:0.6755 Spearman:0.6715\n",
      "Train ALL Pearson: 0.6500291489548929\n",
      "Train  ALL Spearman: 0.6179069147579191\n",
      "19.92924189567566\n",
      "Test Epoch:10 [(0%)]\t Loss: 0.5379  Pearson:0.6998 Spearman:0.7208\n",
      "Test : Loss:0.5207 \n",
      "pearson： 0.728508688135134 ALL Pearson: 0.7439951686152444\n",
      "spearman： 0.7187495671114824 ALL Spearman: 0.7236398426603969\n",
      "time: 24.833669900894165\n",
      "0.001\n",
      "Train Epoch:11 [(0%)]\t Loss: 0.3562  Pearson:0.6556 Spearman:0.6403\n",
      "Train ALL Pearson: 0.6684977952522372\n",
      "Train  ALL Spearman: 0.6354619475778108\n",
      "32.0638587474823\n",
      "Test Epoch:11 [(0%)]\t Loss: 0.5253  Pearson:0.7349 Spearman:0.7574\n",
      "Test : Loss:0.5103 \n",
      "pearson： 0.7426048965099729 ALL Pearson: 0.7530913127677451\n",
      "spearman： 0.7316471845919268 ALL Spearman: 0.7343223699442256\n",
      "time: 36.76535105705261\n",
      "0.001\n",
      "Train Epoch:12 [(0%)]\t Loss: 0.3184  Pearson:0.6883 Spearman:0.6392\n",
      "Train ALL Pearson: 0.6806453536891744\n",
      "Train  ALL Spearman: 0.6549806494972475\n",
      "31.486538648605347\n",
      "Test Epoch:12 [(0%)]\t Loss: 0.4958  Pearson:0.7922 Spearman:0.7663\n",
      "Test : Loss:0.5154 \n",
      "pearson： 0.761942310239236 ALL Pearson: 0.7593711605928322\n",
      "spearman： 0.7394331376048426 ALL Spearman: 0.7424249648713894\n",
      "time: 36.31071376800537\n",
      "0.001\n",
      "Train Epoch:13 [(0%)]\t Loss: 0.3576  Pearson:0.6731 Spearman:0.6417\n",
      "Train ALL Pearson: 0.6850155641061968\n",
      "Train  ALL Spearman: 0.6562154864363503\n",
      "31.710206031799316\n",
      "Test Epoch:13 [(0%)]\t Loss: 0.5376  Pearson:0.7441 Spearman:0.7191\n",
      "Test : Loss:0.5154 \n",
      "pearson： 0.7554593623920358 ALL Pearson: 0.7656572974951408\n",
      "spearman： 0.7324512965989659 ALL Spearman: 0.7487550863110864\n",
      "time: 36.25674629211426\n",
      "0.001\n",
      "Train Epoch:14 [(0%)]\t Loss: 0.2881  Pearson:0.7611 Spearman:0.7197\n",
      "Train ALL Pearson: 0.6972378386153978\n",
      "Train  ALL Spearman: 0.6725026821163667\n",
      "31.849801540374756\n",
      "Test Epoch:14 [(0%)]\t Loss: 0.4896  Pearson:0.8035 Spearman:0.7737\n",
      "Test : Loss:0.5109 \n",
      "pearson： 0.7763963053359447 ALL Pearson: 0.7721222375377156\n",
      "spearman： 0.7509253981514838 ALL Spearman: 0.755157819806855\n",
      "time: 36.718260765075684\n",
      "0.001\n",
      "Train Epoch:15 [(0%)]\t Loss: 0.3495  Pearson:0.6338 Spearman:0.5688\n",
      "Train ALL Pearson: 0.7072218199037836\n",
      "Train  ALL Spearman: 0.6791659545570906\n",
      "31.543174266815186\n",
      "Test Epoch:15 [(0%)]\t Loss: 0.4991  Pearson:0.7919 Spearman:0.7446\n",
      "Test : Loss:0.5104 \n",
      "pearson： 0.7742613108979655 ALL Pearson: 0.7775319914611938\n",
      "spearman： 0.7522714057127838 ALL Spearman: 0.7613419836649671\n",
      "time: 36.36462879180908\n",
      "0.001\n",
      "Train Epoch:16 [(0%)]\t Loss: 0.3408  Pearson:0.6258 Spearman:0.6494\n",
      "Train ALL Pearson: 0.7102741085275425\n",
      "Train  ALL Spearman: 0.6825653942953018\n",
      "31.867513179779053\n",
      "Test Epoch:16 [(0%)]\t Loss: 0.5173  Pearson:0.8220 Spearman:0.8150\n",
      "Test : Loss:0.5053 \n",
      "pearson： 0.7953096350581628 ALL Pearson: 0.7838764394295461\n",
      "spearman： 0.7787018576657135 ALL Spearman: 0.7677295643337257\n",
      "time: 36.668972969055176\n",
      "0.001\n",
      "Train Epoch:17 [(0%)]\t Loss: 0.3325  Pearson:0.6967 Spearman:0.7019\n",
      "Train ALL Pearson: 0.7183461570801399\n",
      "Train  ALL Spearman: 0.694157288134231\n",
      "31.839277267456055\n",
      "Test Epoch:17 [(0%)]\t Loss: 0.4702  Pearson:0.8062 Spearman:0.7779\n",
      "Test : Loss:0.4987 \n",
      "pearson： 0.7900510043903005 ALL Pearson: 0.7891884398398394\n",
      "spearman： 0.7759345769253511 ALL Spearman: 0.7733582988479447\n",
      "time: 36.611745834350586\n",
      "0.001\n",
      "Train Epoch:18 [(0%)]\t Loss: 0.2882  Pearson:0.7331 Spearman:0.7121\n",
      "Train ALL Pearson: 0.7356055668722056\n",
      "Train  ALL Spearman: 0.708030360429158\n",
      "31.536935806274414\n",
      "Test Epoch:18 [(0%)]\t Loss: 0.4793  Pearson:0.8207 Spearman:0.7805\n",
      "Test : Loss:0.5025 \n",
      "pearson： 0.8025810415152108 ALL Pearson: 0.7932402912168826\n",
      "spearman： 0.7847236647494417 ALL Spearman: 0.7776150442019594\n",
      "time: 36.310405015945435\n",
      "0.001\n",
      "Train Epoch:19 [(0%)]\t Loss: 0.3109  Pearson:0.7656 Spearman:0.7598\n",
      "Train ALL Pearson: 0.7359306031993268\n",
      "Train  ALL Spearman: 0.7118943527241844\n",
      "31.710809230804443\n",
      "Test Epoch:19 [(0%)]\t Loss: 0.5078  Pearson:0.7812 Spearman:0.7649\n",
      "Test : Loss:0.4989 \n",
      "pearson： 0.7943332879144956 ALL Pearson: 0.7969632304675839\n",
      "spearman： 0.7719689654856045 ALL Spearman: 0.7816748128843014\n",
      "time: 36.36255645751953\n",
      "0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:20 [(0%)]\t Loss: 0.3358  Pearson:0.6921 Spearman:0.6363\n",
      "Train ALL Pearson: 0.737058478500446\n",
      "Train  ALL Spearman: 0.7119119087038942\n",
      "32.01440119743347\n",
      "Test Epoch:20 [(0%)]\t Loss: 0.5176  Pearson:0.7848 Spearman:0.7547\n",
      "Test : Loss:0.4902 \n",
      "pearson： 0.7995525049549034 ALL Pearson: 0.8006322955097395\n",
      "spearman： 0.7865003874887961 ALL Spearman: 0.7863203637698003\n",
      "time: 36.629920959472656\n",
      "0.005\n",
      "Train Epoch:21 [(0%)]\t Loss: 0.3464  Pearson:0.6981 Spearman:0.6822\n",
      "Train ALL Pearson: 0.7449326311772887\n",
      "Train  ALL Spearman: 0.7200784528723083\n",
      "32.09762525558472\n",
      "Test Epoch:21 [(0%)]\t Loss: 0.4739  Pearson:0.8238 Spearman:0.8106\n",
      "Test : Loss:0.5042 \n",
      "pearson： 0.8083255516862284 ALL Pearson: 0.8168425610187788\n",
      "spearman： 0.7969847055692888 ALL Spearman: 0.8045285223456239\n",
      "time: 36.89117765426636\n",
      "0.005\n",
      "Train Epoch:22 [(0%)]\t Loss: 0.2984  Pearson:0.7630 Spearman:0.7279\n",
      "Train ALL Pearson: 0.7661802223987494\n",
      "Train  ALL Spearman: 0.7415128261821309\n",
      "31.487756490707397\n",
      "Test Epoch:22 [(0%)]\t Loss: 0.5163  Pearson:0.8209 Spearman:0.8023\n",
      "Test : Loss:0.4848 \n",
      "pearson： 0.8261792662700579 ALL Pearson: 0.8277967445312567\n",
      "spearman： 0.8100459455364998 ALL Spearman: 0.8165749092193348\n",
      "time: 36.26558995246887\n",
      "0.005\n",
      "Train Epoch:23 [(0%)]\t Loss: 0.2773  Pearson:0.7742 Spearman:0.7415\n",
      "Train ALL Pearson: 0.7841607823349994\n",
      "Train  ALL Spearman: 0.7599348349690422\n",
      "31.720924615859985\n",
      "Test Epoch:23 [(0%)]\t Loss: 0.4617  Pearson:0.8446 Spearman:0.8447\n",
      "Test : Loss:0.4404 \n",
      "pearson： 0.845213128593199 ALL Pearson: 0.8414481524277759\n",
      "spearman： 0.8318811920887653 ALL Spearman: 0.8307933423387891\n",
      "time: 36.28945875167847\n",
      "0.005\n",
      "Train Epoch:24 [(0%)]\t Loss: 0.2784  Pearson:0.7725 Spearman:0.7266\n",
      "Train ALL Pearson: 0.7883766613589916\n",
      "Train  ALL Spearman: 0.7631153026055807\n",
      "31.785894870758057\n",
      "Test Epoch:24 [(0%)]\t Loss: 0.4321  Pearson:0.8649 Spearman:0.8231\n",
      "Test : Loss:0.4354 \n",
      "pearson： 0.8564349953191737 ALL Pearson: 0.8516836648274329\n",
      "spearman： 0.8416012054195896 ALL Spearman: 0.8397995834697868\n",
      "time: 36.63034105300903\n",
      "0.005\n",
      "Train Epoch:25 [(0%)]\t Loss: 0.2456  Pearson:0.8254 Spearman:0.7866\n",
      "Train ALL Pearson: 0.8029929413236252\n",
      "Train  ALL Spearman: 0.7771761113252573\n",
      "31.29304075241089\n",
      "Test Epoch:25 [(0%)]\t Loss: 0.4436  Pearson:0.8365 Spearman:0.8237\n",
      "Test : Loss:0.4376 \n",
      "pearson： 0.8531384593766308 ALL Pearson: 0.8580627196857145\n",
      "spearman： 0.8423768220232812 ALL Spearman: 0.848773193058509\n",
      "time: 35.860575914382935\n",
      "0.005\n",
      "Train Epoch:26 [(0%)]\t Loss: 0.2680  Pearson:0.7998 Spearman:0.7917\n",
      "Train ALL Pearson: 0.8124344905500166\n",
      "Train  ALL Spearman: 0.7895162026642306\n",
      "31.87577795982361\n",
      "Test Epoch:26 [(0%)]\t Loss: 0.4301  Pearson:0.8419 Spearman:0.8310\n",
      "Test : Loss:0.4133 \n",
      "pearson： 0.8580509101724964 ALL Pearson: 0.8642595187644566\n",
      "spearman： 0.8474153313426299 ALL Spearman: 0.8532609860763383\n",
      "time: 36.609265089035034\n",
      "0.005\n",
      "Train Epoch:27 [(0%)]\t Loss: 0.2191  Pearson:0.8650 Spearman:0.8299\n",
      "Train ALL Pearson: 0.8284762271980753\n",
      "Train  ALL Spearman: 0.8057676174620204\n",
      "31.619378805160522\n",
      "Test Epoch:27 [(0%)]\t Loss: 0.3843  Pearson:0.8360 Spearman:0.8182\n",
      "Test : Loss:0.3986 \n",
      "pearson： 0.8627840887213588 ALL Pearson: 0.872715658756389\n",
      "spearman： 0.8506501791895246 ALL Spearman: 0.8627585154457978\n",
      "time: 36.46644997596741\n",
      "0.005\n",
      "Train Epoch:28 [(0%)]\t Loss: 0.2453  Pearson:0.8542 Spearman:0.8054\n",
      "Train ALL Pearson: 0.8329066266908061\n",
      "Train  ALL Spearman: 0.8058785798869598\n",
      "31.6204252243042\n",
      "Test Epoch:28 [(0%)]\t Loss: 0.4087  Pearson:0.9011 Spearman:0.8664\n",
      "Test : Loss:0.4191 \n",
      "pearson： 0.8778695060455768 ALL Pearson: 0.8757111984173142\n",
      "spearman： 0.8601670392991125 ALL Spearman: 0.8666715543454593\n",
      "time: 36.31392002105713\n",
      "0.005\n",
      "Train Epoch:29 [(0%)]\t Loss: 0.2227  Pearson:0.8545 Spearman:0.8265\n",
      "Train ALL Pearson: 0.8418610281586943\n",
      "Train  ALL Spearman: 0.8150003180171063\n",
      "31.858020544052124\n",
      "Test Epoch:29 [(0%)]\t Loss: 0.4189  Pearson:0.8778 Spearman:0.8639\n",
      "Test : Loss:0.4015 \n",
      "pearson： 0.8773579763422107 ALL Pearson: 0.8798032662335723\n",
      "spearman： 0.8685518169847216 ALL Spearman: 0.8704060346987845\n",
      "time: 36.491533517837524\n",
      "0.005\n",
      "Train Epoch:30 [(0%)]\t Loss: 0.2247  Pearson:0.8762 Spearman:0.8713\n",
      "Train ALL Pearson: 0.8438778262839218\n",
      "Train  ALL Spearman: 0.8182619303481735\n",
      "31.793114185333252\n",
      "Test Epoch:30 [(0%)]\t Loss: 0.4138  Pearson:0.8997 Spearman:0.9109\n",
      "Test : Loss:0.3750 \n",
      "pearson： 0.8867245218671002 ALL Pearson: 0.8835074273965152\n",
      "spearman： 0.8825138363393147 ALL Spearman: 0.8733222297645291\n",
      "time: 36.82361888885498\n",
      "0.03\n",
      "Train Epoch:31 [(0%)]\t Loss: 0.2412  Pearson:0.8232 Spearman:0.8010\n",
      "Train ALL Pearson: 0.3912897105244741\n",
      "Train  ALL Spearman: 0.42914564660444127\n",
      "31.65131115913391\n",
      "Test Epoch:31 [(0%)]\t Loss: 0.6224  Pearson:0.8392 Spearman:0.8429\n",
      "Test : Loss:0.5913 \n",
      "pearson： 0.839749778234684 ALL Pearson: 0.8385692668620698\n",
      "spearman： 0.8301099037459241 ALL Spearman: 0.8280332917621644\n",
      "time: 36.16386294364929\n",
      "0.03\n",
      "Train Epoch:32 [(0%)]\t Loss: 0.3589  Pearson:0.8099 Spearman:0.7697\n",
      "Train ALL Pearson: 0.7480326589216468\n",
      "Train  ALL Spearman: 0.7230764623709199\n",
      "32.25971865653992\n",
      "Test Epoch:32 [(0%)]\t Loss: 0.5814  Pearson:0.8709 Spearman:0.8797\n",
      "Test : Loss:0.6088 \n",
      "pearson： 0.8587145683784947 ALL Pearson: 0.8609759274466916\n",
      "spearman： 0.8599047296634111 ALL Spearman: 0.8599979148375582\n",
      "time: 37.07817339897156\n",
      "0.03\n",
      "Train Epoch:33 [(0%)]\t Loss: 0.3754  Pearson:0.8175 Spearman:0.7928\n",
      "Train ALL Pearson: 0.7688191461278585\n",
      "Train  ALL Spearman: 0.7403901954658009\n",
      "31.72744345664978\n",
      "Test Epoch:33 [(0%)]\t Loss: 0.4329  Pearson:0.8747 Spearman:0.8765\n",
      "Test : Loss:0.4386 \n",
      "pearson： 0.872485850061907 ALL Pearson: 0.8736652787222632\n",
      "spearman： 0.866020772474485 ALL Spearman: 0.8631344972566308\n",
      "time: 36.335965394973755\n",
      "0.03\n",
      "Train Epoch:34 [(0%)]\t Loss: 0.2685  Pearson:0.8199 Spearman:0.7798\n",
      "Train ALL Pearson: 0.7876998486003219\n",
      "Train  ALL Spearman: 0.7564699501087686\n",
      "31.590357780456543\n",
      "Test Epoch:34 [(0%)]\t Loss: 0.4491  Pearson:0.8759 Spearman:0.8694\n",
      "Test : Loss:0.4342 \n",
      "pearson： 0.8771243830102899 ALL Pearson: 0.8749764292132458\n",
      "spearman： 0.8681288411857179 ALL Spearman: 0.8686901194515727\n",
      "time: 36.24086546897888\n",
      "0.03\n",
      "Train Epoch:35 [(0%)]\t Loss: 0.2307  Pearson:0.8436 Spearman:0.7789\n",
      "Train ALL Pearson: 0.8191086816529688\n",
      "Train  ALL Spearman: 0.79061718291713\n",
      "31.70515489578247\n",
      "Test Epoch:35 [(0%)]\t Loss: 0.2304  Pearson:0.8815 Spearman:0.8734\n",
      "Test : Loss:0.2163 \n",
      "pearson： 0.8879489608996696 ALL Pearson: 0.8894640185890698\n",
      "spearman： 0.8713845519770174 ALL Spearman: 0.8712783680199594\n",
      "time: 36.441636085510254\n",
      "0.03\n",
      "Train Epoch:36 [(0%)]\t Loss: 0.2855  Pearson:0.8640 Spearman:0.8343\n",
      "Train ALL Pearson: 0.8236188344642398\n",
      "Train  ALL Spearman: 0.7973430747254693\n",
      "31.56114649772644\n",
      "Test Epoch:36 [(0%)]\t Loss: 0.3695  Pearson:0.8651 Spearman:0.8545\n",
      "Test : Loss:0.3621 \n",
      "pearson： 0.8796912026109124 ALL Pearson: 0.8849337824305461\n",
      "spearman： 0.8701610751105474 ALL Spearman: 0.8757500478627286\n",
      "time: 36.276633501052856\n",
      "0.03\n",
      "Train Epoch:37 [(0%)]\t Loss: 0.2417  Pearson:0.8758 Spearman:0.8750\n",
      "Train ALL Pearson: 0.835606329132702\n",
      "Train  ALL Spearman: 0.8090619615049865\n",
      "32.061134815216064\n",
      "Test Epoch:37 [(0%)]\t Loss: 0.4399  Pearson:0.8556 Spearman:0.8585\n",
      "Test : Loss:0.4281 \n",
      "pearson： 0.8799240582058413 ALL Pearson: 0.8837693893271826\n",
      "spearman： 0.8785675707775689 ALL Spearman: 0.8829677396465518\n",
      "time: 36.755629777908325\n",
      "0.03\n",
      "Train Epoch:38 [(0%)]\t Loss: 0.2172  Pearson:0.9015 Spearman:0.8865\n",
      "Train ALL Pearson: 0.8442130747915522\n",
      "Train  ALL Spearman: 0.8215211951121386\n",
      "32.15827298164368\n",
      "Test Epoch:38 [(0%)]\t Loss: 0.4311  Pearson:0.8743 Spearman:0.8689\n",
      "Test : Loss:0.4276 \n",
      "pearson： 0.8820598506493937 ALL Pearson: 0.8858107644230849\n",
      "spearman： 0.8813965848449411 ALL Spearman: 0.8868356473777481\n",
      "time: 36.957733154296875\n",
      "0.03\n",
      "Train Epoch:39 [(0%)]\t Loss: 0.2850  Pearson:0.8935 Spearman:0.8654\n",
      "Train ALL Pearson: 0.8409071468802948\n",
      "Train  ALL Spearman: 0.8134752926180119\n",
      "32.11512351036072\n",
      "Test Epoch:39 [(0%)]\t Loss: 0.4070  Pearson:0.8991 Spearman:0.9140\n",
      "Test : Loss:0.4215 \n",
      "pearson： 0.8829129724045288 ALL Pearson: 0.8849353908175439\n",
      "spearman： 0.8909972862675701 ALL Spearman: 0.8926821115203034\n",
      "time: 36.72619104385376\n",
      "0.03\n",
      "Train Epoch:40 [(0%)]\t Loss: 0.2451  Pearson:0.9027 Spearman:0.8995\n",
      "Train ALL Pearson: 0.8438037473248967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  ALL Spearman: 0.8204445213206084\n",
      "31.426153421401978\n",
      "Test Epoch:40 [(0%)]\t Loss: 0.2273  Pearson:0.9030 Spearman:0.8747\n",
      "Test : Loss:0.2156 \n",
      "pearson： 0.8986388390763629 ALL Pearson: 0.8964837120804501\n",
      "spearman： 0.8876150692804775 ALL Spearman: 0.8889607344634647\n",
      "time: 36.132089614868164\n",
      "0.03\n",
      "Train Epoch:41 [(0%)]\t Loss: 0.2023  Pearson:0.8997 Spearman:0.8618\n",
      "Train ALL Pearson: 0.8517478859053789\n",
      "Train  ALL Spearman: 0.8274986498430394\n",
      "31.991286277770996\n",
      "Test Epoch:41 [(0%)]\t Loss: 0.2034  Pearson:0.8991 Spearman:0.9210\n",
      "Test : Loss:0.2135 \n",
      "pearson： 0.8989611250841887 ALL Pearson: 0.8973387727675775\n",
      "spearman： 0.8967744760936117 ALL Spearman: 0.8908102615064215\n",
      "time: 36.806742429733276\n",
      "0.03\n",
      "Train Epoch:42 [(0%)]\t Loss: 0.1879  Pearson:0.9164 Spearman:0.8958\n",
      "Train ALL Pearson: 0.8657901937189281\n",
      "Train  ALL Spearman: 0.8457920558576969\n",
      "31.451828956604004\n",
      "Test Epoch:42 [(0%)]\t Loss: 0.3849  Pearson:0.8968 Spearman:0.8882\n",
      "Test : Loss:0.3668 \n",
      "pearson： 0.8967132158540548 ALL Pearson: 0.8940487373472317\n",
      "spearman： 0.8971612592452125 ALL Spearman: 0.8962446280724625\n",
      "time: 36.15731954574585\n",
      "0.03\n",
      "Train Epoch:43 [(0%)]\t Loss: 0.1980  Pearson:0.8988 Spearman:0.8594\n",
      "Train ALL Pearson: 0.8569064863602535\n",
      "Train  ALL Spearman: 0.8286965992797147\n",
      "31.966402530670166\n",
      "Test Epoch:43 [(0%)]\t Loss: 0.4020  Pearson:0.8870 Spearman:0.8782\n",
      "Test : Loss:0.3933 \n",
      "pearson： 0.8885781590358994 ALL Pearson: 0.8951142259566299\n",
      "spearman： 0.8880374339132189 ALL Spearman: 0.8960942493390623\n",
      "time: 36.70719337463379\n",
      "0.03\n",
      "Train Epoch:44 [(0%)]\t Loss: 0.2618  Pearson:0.9077 Spearman:0.8768\n",
      "Train ALL Pearson: 0.8565338617974072\n",
      "Train  ALL Spearman: 0.833240091756863\n",
      "31.53122329711914\n",
      "Test Epoch:44 [(0%)]\t Loss: 0.2969  Pearson:0.8847 Spearman:0.8822\n",
      "Test : Loss:0.3122 \n",
      "pearson： 0.8908782519846054 ALL Pearson: 0.8956107400194271\n",
      "spearman： 0.8897091295346022 ALL Spearman: 0.8925891574374945\n",
      "time: 36.13775396347046\n",
      "0.03\n",
      "Train Epoch:45 [(0%)]\t Loss: 0.1992  Pearson:0.9027 Spearman:0.9060\n",
      "Train ALL Pearson: 0.8674717099059421\n",
      "Train  ALL Spearman: 0.8442402807131824\n",
      "31.86952781677246\n",
      "Test Epoch:45 [(0%)]\t Loss: 0.2906  Pearson:0.9010 Spearman:0.9133\n",
      "Test : Loss:0.2895 \n",
      "pearson： 0.8998103901148443 ALL Pearson: 0.8981918196989485\n",
      "spearman： 0.8949034948461315 ALL Spearman: 0.8931756032346873\n",
      "time: 36.46445655822754\n",
      "0.03\n",
      "Train Epoch:46 [(0%)]\t Loss: 0.1714  Pearson:0.9145 Spearman:0.9207\n",
      "Train ALL Pearson: 0.8646585549853224\n",
      "Train  ALL Spearman: 0.8417419836571457\n",
      "31.67637801170349\n",
      "Test Epoch:46 [(0%)]\t Loss: 0.2156  Pearson:0.8879 Spearman:0.8717\n",
      "Test : Loss:0.2096 \n",
      "pearson： 0.8998223656695603 ALL Pearson: 0.9041755059472469\n",
      "spearman： 0.8892289198104071 ALL Spearman: 0.8932603178315796\n",
      "time: 36.30389356613159\n",
      "0.03\n",
      "Train Epoch:47 [(0%)]\t Loss: 0.2117  Pearson:0.9116 Spearman:0.9033\n",
      "Train ALL Pearson: 0.869481669601606\n",
      "Train  ALL Spearman: 0.8461143493314828\n",
      "31.373762130737305\n",
      "Test Epoch:47 [(0%)]\t Loss: 0.1834  Pearson:0.9022 Spearman:0.8708\n",
      "Test : Loss:0.1814 \n",
      "pearson： 0.9067944990619072 ALL Pearson: 0.9055515460591863\n",
      "spearman： 0.8946613937495961 ALL Spearman: 0.8944389999148769\n",
      "time: 35.91130614280701\n",
      "0.03\n",
      "Train Epoch:48 [(0%)]\t Loss: 0.2196  Pearson:0.9288 Spearman:0.8853\n",
      "Train ALL Pearson: 0.8747884975014358\n",
      "Train  ALL Spearman: 0.8537186916887461\n",
      "32.127864599227905\n",
      "Test Epoch:48 [(0%)]\t Loss: 0.1894  Pearson:0.8746 Spearman:0.8360\n",
      "Test : Loss:0.1759 \n",
      "pearson： 0.9054076136186722 ALL Pearson: 0.9085575496715315\n",
      "spearman： 0.883567797277988 ALL Spearman: 0.8953322045282942\n",
      "time: 37.05028533935547\n",
      "0.03\n",
      "Train Epoch:49 [(0%)]\t Loss: 0.2350  Pearson:0.9351 Spearman:0.9202\n",
      "Train ALL Pearson: 0.8677999582527565\n",
      "Train  ALL Spearman: 0.843499289970885\n",
      "31.747806310653687\n",
      "Test Epoch:49 [(0%)]\t Loss: 0.2066  Pearson:0.9104 Spearman:0.8971\n",
      "Test : Loss:0.2106 \n",
      "pearson： 0.9054598904157207 ALL Pearson: 0.9054813557486795\n",
      "spearman： 0.893478726579334 ALL Spearman: 0.8967953762955448\n",
      "time: 36.48028779029846\n",
      "0.03\n",
      "Train Epoch:50 [(0%)]\t Loss: 0.1867  Pearson:0.9061 Spearman:0.8711\n",
      "Train ALL Pearson: 0.8744180110493633\n",
      "Train  ALL Spearman: 0.8510160778798127\n",
      "31.68344759941101\n",
      "Test Epoch:50 [(0%)]\t Loss: 0.3571  Pearson:0.8752 Spearman:0.8740\n",
      "Test : Loss:0.3613 \n",
      "pearson： 0.8955530000858871 ALL Pearson: 0.8963785533886633\n",
      "spearman： 0.8990977817385262 ALL Spearman: 0.9023911404996473\n",
      "time: 36.39793562889099\n",
      "0.03\n",
      "Train Epoch:51 [(0%)]\t Loss: 0.2362  Pearson:0.8988 Spearman:0.8943\n",
      "Train ALL Pearson: 0.8790393495996683\n",
      "Train  ALL Spearman: 0.8563428114605729\n",
      "31.930295705795288\n",
      "Test Epoch:51 [(0%)]\t Loss: 0.1696  Pearson:0.9065 Spearman:0.9036\n",
      "Test : Loss:0.1733 \n",
      "pearson： 0.9101212222719454 ALL Pearson: 0.9099245340675203\n",
      "spearman： 0.8984238813460333 ALL Spearman: 0.8996350857969475\n",
      "time: 36.556812047958374\n",
      "0.03\n",
      "Train Epoch:52 [(0%)]\t Loss: 0.2350  Pearson:0.9077 Spearman:0.8946\n",
      "Train ALL Pearson: 0.8749464380805316\n",
      "Train  ALL Spearman: 0.8523963853283492\n",
      "32.11754393577576\n",
      "Test Epoch:52 [(0%)]\t Loss: 0.1885  Pearson:0.9168 Spearman:0.9086\n",
      "Test : Loss:0.1890 \n",
      "pearson： 0.9110433740964106 ALL Pearson: 0.90866017610643\n",
      "spearman： 0.9039555239423277 ALL Spearman: 0.8999570857721596\n",
      "time: 36.97198677062988\n",
      "0.03\n",
      "Train Epoch:53 [(0%)]\t Loss: 0.1877  Pearson:0.9157 Spearman:0.8872\n",
      "Train ALL Pearson: 0.8846417615267963\n",
      "Train  ALL Spearman: 0.8637870990759376\n",
      "31.65139865875244\n",
      "Test Epoch:53 [(0%)]\t Loss: 0.2207  Pearson:0.9166 Spearman:0.9052\n",
      "Test : Loss:0.2134 \n",
      "pearson： 0.9077104782388165 ALL Pearson: 0.9075081569575058\n",
      "spearman： 0.9005738384722816 ALL Spearman: 0.9021492311152933\n",
      "time: 36.62388277053833\n",
      "0.03\n",
      "Train Epoch:54 [(0%)]\t Loss: 0.1748  Pearson:0.9365 Spearman:0.9251\n",
      "Train ALL Pearson: 0.8929035139925929\n",
      "Train  ALL Spearman: 0.8738724402249821\n",
      "31.79301381111145\n",
      "Test Epoch:54 [(0%)]\t Loss: 0.3320  Pearson:0.9039 Spearman:0.9061\n",
      "Test : Loss:0.3256 \n",
      "pearson： 0.9107440009685447 ALL Pearson: 0.9060544118276344\n",
      "spearman： 0.9099046842317194 ALL Spearman: 0.9041757254503714\n",
      "time: 36.544490337371826\n",
      "0.03\n",
      "Train Epoch:55 [(0%)]\t Loss: 0.2084  Pearson:0.9181 Spearman:0.8951\n",
      "Train ALL Pearson: 0.8810817952662394\n",
      "Train  ALL Spearman: 0.8595402183688254\n",
      "31.910579204559326\n",
      "Test Epoch:55 [(0%)]\t Loss: 0.2666  Pearson:0.8879 Spearman:0.8981\n",
      "Test : Loss:0.2586 \n",
      "pearson： 0.9037266329870293 ALL Pearson: 0.9064884090341396\n",
      "spearman： 0.9011146798182254 ALL Spearman: 0.8999224634561072\n",
      "time: 36.75002670288086\n",
      "0.03\n",
      "Train Epoch:56 [(0%)]\t Loss: 0.1686  Pearson:0.9036 Spearman:0.9014\n",
      "Train ALL Pearson: 0.8874799921796878\n",
      "Train  ALL Spearman: 0.8670763117538625\n",
      "32.13626456260681\n",
      "Test Epoch:56 [(0%)]\t Loss: 0.4076  Pearson:0.9114 Spearman:0.9220\n",
      "Test : Loss:0.4003 \n",
      "pearson： 0.9048868689353714 ALL Pearson: 0.9023136465331695\n",
      "spearman： 0.905208552786321 ALL Spearman: 0.8993704267401654\n",
      "time: 36.86974549293518\n",
      "0.03\n",
      "Train Epoch:57 [(0%)]\t Loss: 0.2463  Pearson:0.9247 Spearman:0.8948\n",
      "Train ALL Pearson: 0.876450465296742\n",
      "Train  ALL Spearman: 0.8527886798682748\n",
      "32.10575318336487\n",
      "Test Epoch:57 [(0%)]\t Loss: 0.3764  Pearson:0.8941 Spearman:0.8920\n",
      "Test : Loss:0.3861 \n",
      "pearson： 0.898098124591425 ALL Pearson: 0.898790493854249\n",
      "spearman： 0.9008452874302691 ALL Spearman: 0.903619611673037\n",
      "time: 36.79385304450989\n",
      "0.03\n",
      "Train Epoch:58 [(0%)]\t Loss: 0.2341  Pearson:0.9337 Spearman:0.9179\n",
      "Train ALL Pearson: 0.8953107648148065\n",
      "Train  ALL Spearman: 0.8754856619632209\n",
      "31.696406841278076\n",
      "Test Epoch:58 [(0%)]\t Loss: 0.2876  Pearson:0.9278 Spearman:0.9353\n",
      "Test : Loss:0.2846 \n",
      "pearson： 0.9109679651053654 ALL Pearson: 0.9077869275707692\n",
      "spearman： 0.9137066057072624 ALL Spearman: 0.9071821370831585\n",
      "time: 36.47352957725525\n",
      "0.03\n",
      "Train Epoch:59 [(0%)]\t Loss: 0.1738  Pearson:0.9302 Spearman:0.9092\n",
      "Train ALL Pearson: 0.8812795051367097\n",
      "Train  ALL Spearman: 0.8576620102698537\n",
      "32.31475210189819\n",
      "Test Epoch:59 [(0%)]\t Loss: 0.2885  Pearson:0.8987 Spearman:0.9011\n",
      "Test : Loss:0.2963 \n",
      "pearson： 0.9037366289534241 ALL Pearson: 0.9071194949518337\n",
      "spearman： 0.9040187935265805 ALL Spearman: 0.9023268401850137\n",
      "time: 37.02524185180664\n",
      "0.03\n",
      "Train Epoch:60 [(0%)]\t Loss: 0.1679  Pearson:0.9276 Spearman:0.9272\n",
      "Train ALL Pearson: 0.8891639371261264\n",
      "Train  ALL Spearman: 0.8703831466766123\n",
      "32.06350040435791\n",
      "Test Epoch:60 [(0%)]\t Loss: 0.1902  Pearson:0.9231 Spearman:0.9078\n",
      "Test : Loss:0.1791 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9152599672035732 ALL Pearson: 0.9126154828362708\n",
      "spearman： 0.9059437987820455 ALL Spearman: 0.9018222375587684\n",
      "time: 36.84096884727478\n",
      "0.03\n",
      "Train Epoch:61 [(0%)]\t Loss: 0.1965  Pearson:0.9343 Spearman:0.9127\n",
      "Train ALL Pearson: 0.894255859016025\n",
      "Train  ALL Spearman: 0.8727251338129661\n",
      "31.30328631401062\n",
      "Test Epoch:61 [(0%)]\t Loss: 0.1801  Pearson:0.8978 Spearman:0.8986\n",
      "Test : Loss:0.1759 \n",
      "pearson： 0.9124347241410565 ALL Pearson: 0.9116268246515282\n",
      "spearman： 0.9026980169294752 ALL Spearman: 0.9027049448419608\n",
      "time: 35.98334264755249\n",
      "0.03\n",
      "Train Epoch:62 [(0%)]\t Loss: 0.2082  Pearson:0.9330 Spearman:0.9027\n",
      "Train ALL Pearson: 0.8911486627942082\n",
      "Train  ALL Spearman: 0.8710260761540755\n",
      "32.077314138412476\n",
      "Test Epoch:62 [(0%)]\t Loss: 0.1939  Pearson:0.8975 Spearman:0.8978\n",
      "Test : Loss:0.1781 \n",
      "pearson： 0.9080367212231315 ALL Pearson: 0.9134862971297754\n",
      "spearman： 0.8999594242544887 ALL Spearman: 0.9031953867283019\n",
      "time: 36.927669286727905\n",
      "0.03\n",
      "Train Epoch:63 [(0%)]\t Loss: 0.1778  Pearson:0.9192 Spearman:0.9040\n",
      "Train ALL Pearson: 0.8901829457815367\n",
      "Train  ALL Spearman: 0.8676376558115839\n",
      "31.647308111190796\n",
      "Test Epoch:63 [(0%)]\t Loss: 0.1748  Pearson:0.9193 Spearman:0.9155\n",
      "Test : Loss:0.1763 \n",
      "pearson： 0.9141235383441675 ALL Pearson: 0.9134627771313315\n",
      "spearman： 0.905703730216685 ALL Spearman: 0.9038933440913637\n",
      "time: 36.38310432434082\n",
      "0.03\n",
      "Train Epoch:64 [(0%)]\t Loss: 0.1850  Pearson:0.9172 Spearman:0.9035\n",
      "Train ALL Pearson: 0.8902451528558417\n",
      "Train  ALL Spearman: 0.8677669666820454\n",
      "31.83164882659912\n",
      "Test Epoch:64 [(0%)]\t Loss: 0.2767  Pearson:0.9030 Spearman:0.9065\n",
      "Test : Loss:0.2679 \n",
      "pearson： 0.9106517217053292 ALL Pearson: 0.9096636252764019\n",
      "spearman： 0.9084383323291289 ALL Spearman: 0.9044481003491835\n",
      "time: 36.52714419364929\n",
      "0.03\n",
      "Train Epoch:65 [(0%)]\t Loss: 0.1550  Pearson:0.9265 Spearman:0.9138\n",
      "Train ALL Pearson: 0.8843718429443254\n",
      "Train  ALL Spearman: 0.8631968014141758\n",
      "32.09539604187012\n",
      "Test Epoch:65 [(0%)]\t Loss: 0.4082  Pearson:0.9025 Spearman:0.8909\n",
      "Test : Loss:0.4194 \n",
      "pearson： 0.9014433711384636 ALL Pearson: 0.8998576670669777\n",
      "spearman： 0.9039667968122532 ALL Spearman: 0.905643182915804\n",
      "time: 36.71748661994934\n",
      "0.03\n",
      "Train Epoch:66 [(0%)]\t Loss: 0.2617  Pearson:0.9164 Spearman:0.9015\n",
      "Train ALL Pearson: 0.8979506993493015\n",
      "Train  ALL Spearman: 0.8801305795998909\n",
      "31.7873957157135\n",
      "Test Epoch:66 [(0%)]\t Loss: 0.2101  Pearson:0.9120 Spearman:0.8811\n",
      "Test : Loss:0.2139 \n",
      "pearson： 0.9133179578095326 ALL Pearson: 0.9125250699415031\n",
      "spearman： 0.9042380687839338 ALL Spearman: 0.9071600713791828\n",
      "time: 36.598498821258545\n",
      "0.03\n",
      "Train Epoch:67 [(0%)]\t Loss: 0.1616  Pearson:0.9168 Spearman:0.8972\n",
      "Train ALL Pearson: 0.905292034704513\n",
      "Train  ALL Spearman: 0.8891420724224645\n",
      "31.53263831138611\n",
      "Test Epoch:67 [(0%)]\t Loss: 0.1858  Pearson:0.9017 Spearman:0.8737\n",
      "Test : Loss:0.1677 \n",
      "pearson： 0.9147835190434802 ALL Pearson: 0.9171233342413792\n",
      "spearman： 0.901409175642921 ALL Spearman: 0.9075190623094054\n",
      "time: 36.13616061210632\n",
      "0.03\n",
      "Train Epoch:68 [(0%)]\t Loss: 0.2389  Pearson:0.9309 Spearman:0.9055\n",
      "Train ALL Pearson: 0.8883137314406039\n",
      "Train  ALL Spearman: 0.8674850237000551\n",
      "32.16423940658569\n",
      "Test Epoch:68 [(0%)]\t Loss: 0.1824  Pearson:0.9289 Spearman:0.9133\n",
      "Test : Loss:0.1813 \n",
      "pearson： 0.9161484552088719 ALL Pearson: 0.9152079872107038\n",
      "spearman： 0.906659208288611 ALL Spearman: 0.9053686191637699\n",
      "time: 36.9338858127594\n",
      "0.03\n",
      "Train Epoch:69 [(0%)]\t Loss: 0.1752  Pearson:0.9256 Spearman:0.9260\n",
      "Train ALL Pearson: 0.9006455638808928\n",
      "Train  ALL Spearman: 0.8821190287196382\n",
      "31.432509422302246\n",
      "Test Epoch:69 [(0%)]\t Loss: 0.2801  Pearson:0.9254 Spearman:0.9089\n",
      "Test : Loss:0.2867 \n",
      "pearson： 0.9131750679083691 ALL Pearson: 0.9121809333692474\n",
      "spearman： 0.9072438195955268 ALL Spearman: 0.9065285606063631\n",
      "time: 36.09901285171509\n",
      "0.03\n",
      "Train Epoch:70 [(0%)]\t Loss: 0.1422  Pearson:0.9427 Spearman:0.9450\n",
      "Train ALL Pearson: 0.9003610335504408\n",
      "Train  ALL Spearman: 0.8808639428931737\n",
      "31.662998914718628\n",
      "Test Epoch:70 [(0%)]\t Loss: 0.3619  Pearson:0.9205 Spearman:0.9197\n",
      "Test : Loss:0.3606 \n",
      "pearson： 0.9092907500428595 ALL Pearson: 0.9077630455071298\n",
      "spearman： 0.9104083914602146 ALL Spearman: 0.9075526736922902\n",
      "time: 36.65339922904968\n",
      "0.03\n",
      "Train Epoch:71 [(0%)]\t Loss: 0.2087  Pearson:0.9333 Spearman:0.9399\n",
      "Train ALL Pearson: 0.8983496332317925\n",
      "Train  ALL Spearman: 0.8780903613282179\n",
      "31.513707637786865\n",
      "Test Epoch:71 [(0%)]\t Loss: 0.3613  Pearson:0.9073 Spearman:0.8888\n",
      "Test : Loss:0.3601 \n",
      "pearson： 0.90949068684121 ALL Pearson: 0.9114335317480056\n",
      "spearman： 0.9055781369732047 ALL Spearman: 0.907093831588993\n",
      "time: 36.34115958213806\n",
      "0.03\n",
      "Train Epoch:72 [(0%)]\t Loss: 0.2233  Pearson:0.9478 Spearman:0.9330\n",
      "Train ALL Pearson: 0.8976970757577422\n",
      "Train  ALL Spearman: 0.8773047717788994\n",
      "31.819851398468018\n",
      "Test Epoch:72 [(0%)]\t Loss: 0.3707  Pearson:0.9189 Spearman:0.9072\n",
      "Test : Loss:0.3673 \n",
      "pearson： 0.9117312039251477 ALL Pearson: 0.9066410730872615\n",
      "spearman： 0.9108801852188393 ALL Spearman: 0.9074367031422869\n",
      "time: 36.31440997123718\n",
      "0.03\n",
      "Train Epoch:73 [(0%)]\t Loss: 0.2456  Pearson:0.9371 Spearman:0.9379\n",
      "Train ALL Pearson: 0.9010834676094709\n",
      "Train  ALL Spearman: 0.8804807863226483\n",
      "31.744197130203247\n",
      "Test Epoch:73 [(0%)]\t Loss: 0.2696  Pearson:0.8840 Spearman:0.8749\n",
      "Test : Loss:0.2518 \n",
      "pearson： 0.9082632861130364 ALL Pearson: 0.9139175042363749\n",
      "spearman： 0.9016042648987028 ALL Spearman: 0.9082267414198216\n",
      "time: 36.561853647232056\n",
      "0.03\n",
      "Train Epoch:74 [(0%)]\t Loss: 0.1592  Pearson:0.9445 Spearman:0.9367\n",
      "Train ALL Pearson: 0.8967221790752871\n",
      "Train  ALL Spearman: 0.8770686269498467\n",
      "32.00339984893799\n",
      "Test Epoch:74 [(0%)]\t Loss: 0.2224  Pearson:0.9273 Spearman:0.9105\n",
      "Test : Loss:0.1988 \n",
      "pearson： 0.9203982054697307 ALL Pearson: 0.9170744926071612\n",
      "spearman： 0.9049903747801032 ALL Spearman: 0.9025058131188733\n",
      "time: 36.60192561149597\n",
      "0.03\n",
      "Train Epoch:75 [(0%)]\t Loss: 0.1607  Pearson:0.9264 Spearman:0.8896\n",
      "Train ALL Pearson: 0.8972374611513592\n",
      "Train  ALL Spearman: 0.8742008358227754\n",
      "32.09863758087158\n",
      "Test Epoch:75 [(0%)]\t Loss: 0.1942  Pearson:0.8911 Spearman:0.8781\n",
      "Test : Loss:0.1944 \n",
      "pearson： 0.907352098308533 ALL Pearson: 0.9149239230358802\n",
      "spearman： 0.8951283552653297 ALL Spearman: 0.9047631427450139\n",
      "time: 36.841115951538086\n",
      "0.03\n",
      "Train Epoch:76 [(0%)]\t Loss: 0.1721  Pearson:0.9282 Spearman:0.9129\n",
      "Train ALL Pearson: 0.9045349312212921\n",
      "Train  ALL Spearman: 0.8870243514937715\n",
      "31.86872887611389\n",
      "Test Epoch:76 [(0%)]\t Loss: 0.1703  Pearson:0.9216 Spearman:0.8847\n",
      "Test : Loss:0.1792 \n",
      "pearson： 0.9143828961904248 ALL Pearson: 0.9151515815282317\n",
      "spearman： 0.8992077761025179 ALL Spearman: 0.903186414973868\n",
      "time: 36.681857109069824\n",
      "0.03\n",
      "Train Epoch:77 [(0%)]\t Loss: 0.1784  Pearson:0.9417 Spearman:0.9323\n",
      "Train ALL Pearson: 0.8968752014131668\n",
      "Train  ALL Spearman: 0.8730598849380753\n",
      "31.795923471450806\n",
      "Test Epoch:77 [(0%)]\t Loss: 0.1890  Pearson:0.8963 Spearman:0.9018\n",
      "Test : Loss:0.1840 \n",
      "pearson： 0.9167203670479966 ALL Pearson: 0.917520904455565\n",
      "spearman： 0.9110843140483111 ALL Spearman: 0.9067296974825824\n",
      "time: 36.47042489051819\n",
      "0.03\n",
      "Train Epoch:78 [(0%)]\t Loss: 0.1768  Pearson:0.9472 Spearman:0.9246\n",
      "Train ALL Pearson: 0.9050849597911456\n",
      "Train  ALL Spearman: 0.8850698379912221\n",
      "31.618409633636475\n",
      "Test Epoch:78 [(0%)]\t Loss: 0.1717  Pearson:0.9161 Spearman:0.9035\n",
      "Test : Loss:0.1803 \n",
      "pearson： 0.9171259568165862 ALL Pearson: 0.9176729786328977\n",
      "spearman： 0.902697953882824 ALL Spearman: 0.9051051856511445\n",
      "time: 36.275962591171265\n",
      "0.03\n",
      "Train Epoch:79 [(0%)]\t Loss: 0.1702  Pearson:0.9348 Spearman:0.9186\n",
      "Train ALL Pearson: 0.9046358555957135\n",
      "Train  ALL Spearman: 0.8831943076060738\n",
      "32.07125163078308\n",
      "Test Epoch:79 [(0%)]\t Loss: 0.1823  Pearson:0.9124 Spearman:0.8873\n",
      "Test : Loss:0.1776 \n",
      "pearson： 0.9157857264098342 ALL Pearson: 0.9174985567407725\n",
      "spearman： 0.8979716295073703 ALL Spearman: 0.906619705326975\n",
      "time: 36.83957552909851\n",
      "0.03\n",
      "Train Epoch:80 [(0%)]\t Loss: 0.1947  Pearson:0.9437 Spearman:0.9459\n",
      "Train ALL Pearson: 0.9038545478318081\n",
      "Train  ALL Spearman: 0.8836417072017378\n",
      "31.82458233833313\n",
      "Test Epoch:80 [(0%)]\t Loss: 0.1545  Pearson:0.9434 Spearman:0.9282\n",
      "Test : Loss:0.1685 \n",
      "pearson： 0.9244056118012682 ALL Pearson: 0.9180819661938303\n",
      "spearman： 0.9129046479275508 ALL Spearman: 0.90381997256608\n",
      "time: 36.38266682624817\n",
      "0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:81 [(0%)]\t Loss: 0.2197  Pearson:0.9415 Spearman:0.9258\n",
      "Train ALL Pearson: 0.9021321440399703\n",
      "Train  ALL Spearman: 0.8825010824120173\n",
      "31.359533309936523\n",
      "Test Epoch:81 [(0%)]\t Loss: 0.1785  Pearson:0.9184 Spearman:0.8848\n",
      "Test : Loss:0.1895 \n",
      "pearson： 0.9163843925106728 ALL Pearson: 0.9147553099256843\n",
      "spearman： 0.9040518280259704 ALL Spearman: 0.9072770603345923\n",
      "time: 36.01506519317627\n",
      "0.03\n",
      "Train Epoch:82 [(0%)]\t Loss: 0.1540  Pearson:0.9419 Spearman:0.9413\n",
      "Train ALL Pearson: 0.9106857811272564\n",
      "Train  ALL Spearman: 0.8899155630891202\n",
      "31.86231517791748\n",
      "Test Epoch:82 [(0%)]\t Loss: 0.1834  Pearson:0.9136 Spearman:0.8936\n",
      "Test : Loss:0.1841 \n",
      "pearson： 0.9180586444244926 ALL Pearson: 0.9176505549329425\n",
      "spearman： 0.9032865469493067 ALL Spearman: 0.9047009130688497\n",
      "time: 36.58779954910278\n",
      "0.03\n",
      "Train Epoch:83 [(0%)]\t Loss: 0.1556  Pearson:0.9364 Spearman:0.9277\n",
      "Train ALL Pearson: 0.9064822344403352\n",
      "Train  ALL Spearman: 0.8848884700610031\n",
      "31.85859966278076\n",
      "Test Epoch:83 [(0%)]\t Loss: 0.1878  Pearson:0.9243 Spearman:0.9146\n",
      "Test : Loss:0.1791 \n",
      "pearson： 0.9186907494150567 ALL Pearson: 0.9178457962820428\n",
      "spearman： 0.9087715618440139 ALL Spearman: 0.9076663957040024\n",
      "time: 36.4851176738739\n",
      "0.03\n",
      "Train Epoch:84 [(0%)]\t Loss: 0.1762  Pearson:0.9354 Spearman:0.9262\n",
      "Train ALL Pearson: 0.9082609996684827\n",
      "Train  ALL Spearman: 0.8889526536767517\n",
      "31.9614360332489\n",
      "Test Epoch:84 [(0%)]\t Loss: 0.1654  Pearson:0.9041 Spearman:0.9025\n",
      "Test : Loss:0.1699 \n",
      "pearson： 0.9182605939825063 ALL Pearson: 0.9185751917837105\n",
      "spearman： 0.9103160511524414 ALL Spearman: 0.9086634873414963\n",
      "time: 36.599947929382324\n",
      "0.03\n",
      "Train Epoch:85 [(0%)]\t Loss: 0.1888  Pearson:0.9323 Spearman:0.9200\n",
      "Train ALL Pearson: 0.9132298495549481\n",
      "Train  ALL Spearman: 0.8963617888593084\n",
      "31.630781173706055\n",
      "Test Epoch:85 [(0%)]\t Loss: 0.2469  Pearson:0.9000 Spearman:0.8954\n",
      "Test : Loss:0.2398 \n",
      "pearson： 0.9132918457217183 ALL Pearson: 0.9169255204613702\n",
      "spearman： 0.904103101374379 ALL Spearman: 0.9091852710365307\n",
      "time: 36.4542293548584\n",
      "0.03\n",
      "Train Epoch:86 [(0%)]\t Loss: 0.1497  Pearson:0.9448 Spearman:0.9365\n",
      "Train ALL Pearson: 0.9065763436621891\n",
      "Train  ALL Spearman: 0.8857044220252418\n",
      "31.394001245498657\n",
      "Test Epoch:86 [(0%)]\t Loss: 0.3172  Pearson:0.9139 Spearman:0.9026\n",
      "Test : Loss:0.3230 \n",
      "pearson： 0.9096735399363115 ALL Pearson: 0.9087313802661647\n",
      "spearman： 0.9079429425084279 ALL Spearman: 0.9114644083547069\n",
      "time: 36.27143740653992\n",
      "0.03\n",
      "Train Epoch:87 [(0%)]\t Loss: 0.2103  Pearson:0.9324 Spearman:0.9368\n",
      "Train ALL Pearson: 0.9074953155712582\n",
      "Train  ALL Spearman: 0.8897123853756579\n",
      "32.05325126647949\n",
      "Test Epoch:87 [(0%)]\t Loss: 0.3073  Pearson:0.9130 Spearman:0.9058\n",
      "Test : Loss:0.3039 \n",
      "pearson： 0.9149764268840239 ALL Pearson: 0.9138724800952273\n",
      "spearman： 0.9116635656069242 ALL Spearman: 0.9089569590787534\n",
      "time: 36.64977741241455\n",
      "0.03\n",
      "Train Epoch:88 [(0%)]\t Loss: 0.1717  Pearson:0.9386 Spearman:0.9240\n",
      "Train ALL Pearson: 0.91517573046607\n",
      "Train  ALL Spearman: 0.8974293677652029\n",
      "31.99484634399414\n",
      "Test Epoch:88 [(0%)]\t Loss: 0.1869  Pearson:0.9307 Spearman:0.9215\n",
      "Test : Loss:0.1833 \n",
      "pearson： 0.9206354102459717 ALL Pearson: 0.9186276262418923\n",
      "spearman： 0.9100509806685132 ALL Spearman: 0.9088375398483064\n",
      "time: 36.641356229782104\n",
      "0.03\n",
      "Train Epoch:89 [(0%)]\t Loss: 0.1383  Pearson:0.9598 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9158261146520058\n",
      "Train  ALL Spearman: 0.8992433546685098\n",
      "31.638794422149658\n",
      "Test Epoch:89 [(0%)]\t Loss: 0.1731  Pearson:0.9240 Spearman:0.9176\n",
      "Test : Loss:0.1656 \n",
      "pearson： 0.9210483358560665 ALL Pearson: 0.9193960799397355\n",
      "spearman： 0.9098106237377713 ALL Spearman: 0.906924556275963\n",
      "time: 36.35128331184387\n",
      "0.03\n",
      "Train Epoch:90 [(0%)]\t Loss: 0.2038  Pearson:0.9434 Spearman:0.9327\n",
      "Train ALL Pearson: 0.9118437843380178\n",
      "Train  ALL Spearman: 0.8938427780825269\n",
      "31.417842864990234\n",
      "Test Epoch:90 [(0%)]\t Loss: 0.1661  Pearson:0.9051 Spearman:0.8737\n",
      "Test : Loss:0.1693 \n",
      "pearson： 0.9168730962940818 ALL Pearson: 0.9181665473294534\n",
      "spearman： 0.9033428373536858 ALL Spearman: 0.9043853421712724\n",
      "time: 36.25329279899597\n",
      "0.03\n",
      "Train Epoch:91 [(0%)]\t Loss: 0.1720  Pearson:0.9533 Spearman:0.9417\n",
      "Train ALL Pearson: 0.906904080739556\n",
      "Train  ALL Spearman: 0.884931158915661\n",
      "31.826372146606445\n",
      "Test Epoch:91 [(0%)]\t Loss: 0.1599  Pearson:0.9240 Spearman:0.9151\n",
      "Test : Loss:0.1680 \n",
      "pearson： 0.9215290770795683 ALL Pearson: 0.9195167699503909\n",
      "spearman： 0.9086515168036757 ALL Spearman: 0.907645137150028\n",
      "time: 36.404903411865234\n",
      "0.03\n",
      "Train Epoch:92 [(0%)]\t Loss: 0.1635  Pearson:0.9473 Spearman:0.9262\n",
      "Train ALL Pearson: 0.9107345222053341\n",
      "Train  ALL Spearman: 0.8904388896508342\n",
      "32.1032772064209\n",
      "Test Epoch:92 [(0%)]\t Loss: 0.1669  Pearson:0.9202 Spearman:0.9112\n",
      "Test : Loss:0.1667 \n",
      "pearson： 0.9186523919785304 ALL Pearson: 0.9196680945356241\n",
      "spearman： 0.907931791914784 ALL Spearman: 0.9054369745141715\n",
      "time: 36.77877616882324\n",
      "0.03\n",
      "Train Epoch:93 [(0%)]\t Loss: 0.1803  Pearson:0.9448 Spearman:0.9310\n",
      "Train ALL Pearson: 0.9111776137705005\n",
      "Train  ALL Spearman: 0.8916201825705532\n",
      "31.518754959106445\n",
      "Test Epoch:93 [(0%)]\t Loss: 0.1617  Pearson:0.9034 Spearman:0.8841\n",
      "Test : Loss:0.1676 \n",
      "pearson： 0.9181263559684282 ALL Pearson: 0.9207351418195407\n",
      "spearman： 0.9017031501122598 ALL Spearman: 0.905289728670181\n",
      "time: 36.360201835632324\n",
      "0.03\n",
      "Train Epoch:94 [(0%)]\t Loss: 0.1629  Pearson:0.9482 Spearman:0.9273\n",
      "Train ALL Pearson: 0.9105709214074394\n",
      "Train  ALL Spearman: 0.8910819619844436\n",
      "31.684083700180054\n",
      "Test Epoch:94 [(0%)]\t Loss: 0.1825  Pearson:0.9366 Spearman:0.9339\n",
      "Test : Loss:0.1979 \n",
      "pearson： 0.9212339851490383 ALL Pearson: 0.9179649816034697\n",
      "spearman： 0.914389896875272 ALL Spearman: 0.9099036702959855\n",
      "time: 36.35658407211304\n",
      "0.03\n",
      "Train Epoch:95 [(0%)]\t Loss: 0.1426  Pearson:0.9417 Spearman:0.9339\n",
      "Train ALL Pearson: 0.9147000670039612\n",
      "Train  ALL Spearman: 0.896523963373245\n",
      "31.760560274124146\n",
      "Test Epoch:95 [(0%)]\t Loss: 0.2089  Pearson:0.9270 Spearman:0.9311\n",
      "Test : Loss:0.2143 \n",
      "pearson： 0.9185920089715741 ALL Pearson: 0.9182058648180739\n",
      "spearman： 0.9121132308379416 ALL Spearman: 0.9093599917841296\n",
      "time: 36.347089529037476\n",
      "0.03\n",
      "Train Epoch:96 [(0%)]\t Loss: 0.1438  Pearson:0.9423 Spearman:0.9295\n",
      "Train ALL Pearson: 0.9233578386213808\n",
      "Train  ALL Spearman: 0.9061789132386596\n",
      "32.15019178390503\n",
      "Test Epoch:96 [(0%)]\t Loss: 0.3287  Pearson:0.9203 Spearman:0.9251\n",
      "Test : Loss:0.3175 \n",
      "pearson： 0.9181477244471783 ALL Pearson: 0.9174365277974084\n",
      "spearman： 0.9142250516200098 ALL Spearman: 0.9106459237055793\n",
      "time: 37.01867651939392\n",
      "0.03\n",
      "Train Epoch:97 [(0%)]\t Loss: 0.2110  Pearson:0.9347 Spearman:0.9274\n",
      "Train ALL Pearson: 0.9098949411302638\n",
      "Train  ALL Spearman: 0.8884630788262821\n",
      "32.09732437133789\n",
      "Test Epoch:97 [(0%)]\t Loss: 0.3170  Pearson:0.9273 Spearman:0.9025\n",
      "Test : Loss:0.3199 \n",
      "pearson： 0.9198272656864865 ALL Pearson: 0.9172759535145733\n",
      "spearman： 0.9098313726141872 ALL Spearman: 0.9119046864452586\n",
      "time: 37.080307722091675\n",
      "0.03\n",
      "Train Epoch:98 [(0%)]\t Loss: 0.1786  Pearson:0.9468 Spearman:0.9319\n",
      "Train ALL Pearson: 0.9098531558541448\n",
      "Train  ALL Spearman: 0.8894650339742499\n",
      "31.920974493026733\n",
      "Test Epoch:98 [(0%)]\t Loss: 0.3580  Pearson:0.9152 Spearman:0.8946\n",
      "Test : Loss:0.3348 \n",
      "pearson： 0.9162314818295747 ALL Pearson: 0.9163144862443985\n",
      "spearman： 0.9094790364001212 ALL Spearman: 0.9111111399670337\n",
      "time: 36.81040549278259\n",
      "0.03\n",
      "Train Epoch:99 [(0%)]\t Loss: 0.1997  Pearson:0.9465 Spearman:0.9442\n",
      "Train ALL Pearson: 0.9191494336045556\n",
      "Train  ALL Spearman: 0.9018451682892115\n",
      "32.03329038619995\n",
      "Test Epoch:99 [(0%)]\t Loss: 0.3418  Pearson:0.9053 Spearman:0.8781\n",
      "Test : Loss:0.3329 \n",
      "pearson： 0.914401229504772 ALL Pearson: 0.9171744402122826\n",
      "spearman： 0.9047919298993335 ALL Spearman: 0.9117175308695015\n",
      "time: 36.67545485496521\n",
      "0.03\n",
      "Train Epoch:100 [(0%)]\t Loss: 0.2076  Pearson:0.9490 Spearman:0.9378\n",
      "Train ALL Pearson: 0.9115693652589278\n",
      "Train  ALL Spearman: 0.8901412041900226\n",
      "32.20423364639282\n",
      "Test Epoch:100 [(0%)]\t Loss: 0.3109  Pearson:0.9076 Spearman:0.8962\n",
      "Test : Loss:0.3105 \n",
      "pearson： 0.9125176019685006 ALL Pearson: 0.9158121726057741\n",
      "spearman： 0.9089627635905824 ALL Spearman: 0.9119221309570421\n",
      "time: 36.756773710250854\n",
      "0.03\n",
      "Train Epoch:101 [(0%)]\t Loss: 0.1984  Pearson:0.9405 Spearman:0.9372\n",
      "Train ALL Pearson: 0.9141227598225966\n",
      "Train  ALL Spearman: 0.8949967047021073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.98478674888611\n",
      "Test Epoch:101 [(0%)]\t Loss: 0.3145  Pearson:0.9220 Spearman:0.9019\n",
      "Test : Loss:0.3031 \n",
      "pearson： 0.9219954424277838 ALL Pearson: 0.9185944393294976\n",
      "spearman： 0.9123805335000448 ALL Spearman: 0.9097440958440861\n",
      "time: 36.96818923950195\n",
      "0.03\n",
      "Train Epoch:102 [(0%)]\t Loss: 0.1972  Pearson:0.9012 Spearman:0.8810\n",
      "Train ALL Pearson: 0.9172080628004262\n",
      "Train  ALL Spearman: 0.8984561513588258\n",
      "31.596399545669556\n",
      "Test Epoch:102 [(0%)]\t Loss: 0.2961  Pearson:0.9161 Spearman:0.9192\n",
      "Test : Loss:0.3140 \n",
      "pearson： 0.9164861377123615 ALL Pearson: 0.9169486948733654\n",
      "spearman： 0.9141292758915054 ALL Spearman: 0.9100999278439303\n",
      "time: 36.2289137840271\n",
      "0.03\n",
      "Train Epoch:103 [(0%)]\t Loss: 0.1950  Pearson:0.9393 Spearman:0.9293\n",
      "Train ALL Pearson: 0.913960556291753\n",
      "Train  ALL Spearman: 0.893345204659282\n",
      "31.950519800186157\n",
      "Test Epoch:103 [(0%)]\t Loss: 0.2708  Pearson:0.9020 Spearman:0.8803\n",
      "Test : Loss:0.2915 \n",
      "pearson： 0.9126399408819459 ALL Pearson: 0.9153710118277827\n",
      "spearman： 0.9029426770103566 ALL Spearman: 0.9091727001312758\n",
      "time: 36.47706723213196\n",
      "0.03\n",
      "Train Epoch:104 [(0%)]\t Loss: 0.1640  Pearson:0.9452 Spearman:0.9375\n",
      "Train ALL Pearson: 0.9184508322725786\n",
      "Train  ALL Spearman: 0.8996890743462913\n",
      "32.26896071434021\n",
      "Test Epoch:104 [(0%)]\t Loss: 0.3130  Pearson:0.8965 Spearman:0.8557\n",
      "Test : Loss:0.3018 \n",
      "pearson： 0.9123178314923649 ALL Pearson: 0.9165929091369699\n",
      "spearman： 0.8979640766506052 ALL Spearman: 0.9101250775245168\n",
      "time: 37.11440658569336\n",
      "0.03\n",
      "Train Epoch:105 [(0%)]\t Loss: 0.1748  Pearson:0.9323 Spearman:0.9164\n",
      "Train ALL Pearson: 0.9188286273305942\n",
      "Train  ALL Spearman: 0.9016733414475498\n",
      "31.723074436187744\n",
      "Test Epoch:105 [(0%)]\t Loss: 0.2943  Pearson:0.9080 Spearman:0.8998\n",
      "Test : Loss:0.2969 \n",
      "pearson： 0.9149315501705488 ALL Pearson: 0.9156542893643724\n",
      "spearman： 0.9104063469747691 ALL Spearman: 0.9086739134543806\n",
      "time: 36.42156791687012\n",
      "0.03\n",
      "Train Epoch:106 [(0%)]\t Loss: 0.1814  Pearson:0.9469 Spearman:0.9452\n",
      "Train ALL Pearson: 0.9228771287818706\n",
      "Train  ALL Spearman: 0.9054781758239119\n",
      "31.987539052963257\n",
      "Test Epoch:106 [(0%)]\t Loss: 0.1897  Pearson:0.9283 Spearman:0.9078\n",
      "Test : Loss:0.1959 \n",
      "pearson： 0.9219906324201329 ALL Pearson: 0.9203597480092696\n",
      "spearman： 0.9085355733181273 ALL Spearman: 0.9072522897402682\n",
      "time: 36.69702982902527\n",
      "0.03\n",
      "Train Epoch:107 [(0%)]\t Loss: 0.1426  Pearson:0.9560 Spearman:0.9462\n",
      "Train ALL Pearson: 0.9166677633783573\n",
      "Train  ALL Spearman: 0.8970707986035698\n",
      "31.81933569908142\n",
      "Test Epoch:107 [(0%)]\t Loss: 0.2711  Pearson:0.9210 Spearman:0.9109\n",
      "Test : Loss:0.2591 \n",
      "pearson： 0.9172430284935662 ALL Pearson: 0.9152181288251439\n",
      "spearman： 0.9082115898462306 ALL Spearman: 0.9086264567816422\n",
      "time: 36.59580373764038\n",
      "0.03\n",
      "Train Epoch:108 [(0%)]\t Loss: 0.1503  Pearson:0.9485 Spearman:0.9474\n",
      "Train ALL Pearson: 0.9197756216970717\n",
      "Train  ALL Spearman: 0.9026695466257088\n",
      "31.970643758773804\n",
      "Test Epoch:108 [(0%)]\t Loss: 0.2869  Pearson:0.8985 Spearman:0.8931\n",
      "Test : Loss:0.2951 \n",
      "pearson： 0.9097230617956843 ALL Pearson: 0.9137387484225614\n",
      "spearman： 0.9078332417689599 ALL Spearman: 0.9106119358892457\n",
      "time: 36.63514590263367\n",
      "0.03\n",
      "Train Epoch:109 [(0%)]\t Loss: 0.1566  Pearson:0.9407 Spearman:0.9300\n",
      "Train ALL Pearson: 0.9148251290065376\n",
      "Train  ALL Spearman: 0.8937249160715054\n",
      "31.875391721725464\n",
      "Test Epoch:109 [(0%)]\t Loss: 0.2562  Pearson:0.9252 Spearman:0.8965\n",
      "Test : Loss:0.2661 \n",
      "pearson： 0.9178931012789113 ALL Pearson: 0.9177358655537492\n",
      "spearman： 0.9095199047959912 ALL Spearman: 0.9090347925657092\n",
      "time: 36.54689335823059\n",
      "0.03\n",
      "Train Epoch:110 [(0%)]\t Loss: 0.1559  Pearson:0.9317 Spearman:0.9251\n",
      "Train ALL Pearson: 0.9268064935489589\n",
      "Train  ALL Spearman: 0.9123482379192995\n",
      "31.389856100082397\n",
      "Test Epoch:110 [(0%)]\t Loss: 0.2735  Pearson:0.9226 Spearman:0.9195\n",
      "Test : Loss:0.2864 \n",
      "pearson： 0.9165735104249504 ALL Pearson: 0.9159734988535292\n",
      "spearman： 0.9102947585335794 ALL Spearman: 0.9088283051868528\n",
      "time: 35.87141823768616\n",
      "0.03\n",
      "Train Epoch:111 [(0%)]\t Loss: 0.1522  Pearson:0.9465 Spearman:0.9383\n",
      "Train ALL Pearson: 0.9226239346838996\n",
      "Train  ALL Spearman: 0.905460811803708\n",
      "32.133546113967896\n",
      "Test Epoch:111 [(0%)]\t Loss: 0.2866  Pearson:0.9083 Spearman:0.9050\n",
      "Test : Loss:0.2905 \n",
      "pearson： 0.9148183813900168 ALL Pearson: 0.9151080895533513\n",
      "spearman： 0.912358822016953 ALL Spearman: 0.9099747346081023\n",
      "time: 36.83004188537598\n",
      "0.03\n",
      "Train Epoch:112 [(0%)]\t Loss: 0.1525  Pearson:0.9475 Spearman:0.9421\n",
      "Train ALL Pearson: 0.9201190087213894\n",
      "Train  ALL Spearman: 0.9022311208282161\n",
      "32.33811831474304\n",
      "Test Epoch:112 [(0%)]\t Loss: 0.3242  Pearson:0.8955 Spearman:0.8878\n",
      "Test : Loss:0.3172 \n",
      "pearson： 0.9142005486982213 ALL Pearson: 0.9187762341235108\n",
      "spearman： 0.8975616360245198 ALL Spearman: 0.9073524058142393\n",
      "time: 37.022616147994995\n",
      "0.03\n",
      "Train Epoch:113 [(0%)]\t Loss: 0.1825  Pearson:0.9563 Spearman:0.9575\n",
      "Train ALL Pearson: 0.9188278697146066\n",
      "Train  ALL Spearman: 0.8996069279724509\n",
      "31.265010833740234\n",
      "Test Epoch:113 [(0%)]\t Loss: 0.3237  Pearson:0.9365 Spearman:0.9286\n",
      "Test : Loss:0.3139 \n",
      "pearson： 0.9227086842370714 ALL Pearson: 0.9162280868917501\n",
      "spearman： 0.9109377104682277 ALL Spearman: 0.9075238228460857\n",
      "time: 35.97349977493286\n",
      "0.03\n",
      "Train Epoch:114 [(0%)]\t Loss: 0.1984  Pearson:0.9440 Spearman:0.9369\n",
      "Train ALL Pearson: 0.9217629319089731\n",
      "Train  ALL Spearman: 0.9036804045487485\n",
      "32.34175205230713\n",
      "Test Epoch:114 [(0%)]\t Loss: 0.3276  Pearson:0.9062 Spearman:0.8843\n",
      "Test : Loss:0.3203 \n",
      "pearson： 0.9197873688560008 ALL Pearson: 0.916287861015907\n",
      "spearman： 0.9084247425967079 ALL Spearman: 0.9101301375198325\n",
      "time: 36.86330199241638\n",
      "0.009\n",
      "Train Epoch:115 [(0%)]\t Loss: 0.1898  Pearson:0.9303 Spearman:0.9075\n",
      "Train ALL Pearson: 0.9408671801712364\n",
      "Train  ALL Spearman: 0.9307052047638915\n",
      "31.643967866897583\n",
      "Test Epoch:115 [(0%)]\t Loss: 0.2313  Pearson:0.9196 Spearman:0.9268\n",
      "Test : Loss:0.2224 \n",
      "pearson： 0.9185075125258664 ALL Pearson: 0.9196303974188934\n",
      "spearman： 0.9127280140275075 ALL Spearman: 0.9109737312521109\n",
      "time: 36.633368730545044\n",
      "0.009\n",
      "Train Epoch:116 [(0%)]\t Loss: 0.1390  Pearson:0.9488 Spearman:0.9496\n",
      "Train ALL Pearson: 0.9423756278944597\n",
      "Train  ALL Spearman: 0.9330502872865818\n",
      "31.63734006881714\n",
      "Test Epoch:116 [(0%)]\t Loss: 0.2105  Pearson:0.9288 Spearman:0.9051\n",
      "Test : Loss:0.2141 \n",
      "pearson： 0.9232463292110274 ALL Pearson: 0.9221427585179064\n",
      "spearman： 0.9130822866745135 ALL Spearman: 0.9135052339410644\n",
      "time: 36.565316915512085\n",
      "0.009\n",
      "Train Epoch:117 [(0%)]\t Loss: 0.1304  Pearson:0.9487 Spearman:0.9433\n",
      "Train ALL Pearson: 0.9442361956365938\n",
      "Train  ALL Spearman: 0.9345682217289363\n",
      "31.596870183944702\n",
      "Test Epoch:117 [(0%)]\t Loss: 0.2056  Pearson:0.9137 Spearman:0.9014\n",
      "Test : Loss:0.2000 \n",
      "pearson： 0.9232240919789 ALL Pearson: 0.9232011841078009\n",
      "spearman： 0.9138048182952662 ALL Spearman: 0.9134481041987222\n",
      "time: 36.50118446350098\n",
      "0.009\n",
      "Train Epoch:118 [(0%)]\t Loss: 0.1397  Pearson:0.9465 Spearman:0.9333\n",
      "Train ALL Pearson: 0.9451014960610024\n",
      "Train  ALL Spearman: 0.9360309262138082\n",
      "31.356929063796997\n",
      "Test Epoch:118 [(0%)]\t Loss: 0.2225  Pearson:0.9217 Spearman:0.9169\n",
      "Test : Loss:0.2128 \n",
      "pearson： 0.925822025998383 ALL Pearson: 0.9238996719276483\n",
      "spearman： 0.9149829539659972 ALL Spearman: 0.9140818099206909\n",
      "time: 36.00643754005432\n",
      "0.009\n",
      "Train Epoch:119 [(0%)]\t Loss: 0.1397  Pearson:0.9488 Spearman:0.9419\n",
      "Train ALL Pearson: 0.9455540653886897\n",
      "Train  ALL Spearman: 0.9363609024719479\n",
      "31.21403932571411\n",
      "Test Epoch:119 [(0%)]\t Loss: 0.1872  Pearson:0.9137 Spearman:0.8867\n",
      "Test : Loss:0.2039 \n",
      "pearson： 0.9234575594362249 ALL Pearson: 0.9238079192009306\n",
      "spearman： 0.9092542391663386 ALL Spearman: 0.9135471467229359\n",
      "time: 35.782572746276855\n",
      "0.009\n",
      "Train Epoch:120 [(0%)]\t Loss: 0.1275  Pearson:0.9490 Spearman:0.9361\n",
      "Train ALL Pearson: 0.947081293898059\n",
      "Train  ALL Spearman: 0.9378974573560248\n",
      "31.963298797607422\n",
      "Test Epoch:120 [(0%)]\t Loss: 0.1973  Pearson:0.9344 Spearman:0.9192\n",
      "Test : Loss:0.2084 \n",
      "pearson： 0.9288668888593855 ALL Pearson: 0.9245442167730943\n",
      "spearman： 0.9168184609578099 ALL Spearman: 0.9145848896218657\n",
      "time: 36.7177734375\n",
      "0.009\n",
      "Train Epoch:121 [(0%)]\t Loss: 0.1376  Pearson:0.9466 Spearman:0.9505\n",
      "Train ALL Pearson: 0.9475820028261374\n",
      "Train  ALL Spearman: 0.9368925134708591\n",
      "31.230961561203003\n",
      "Test Epoch:121 [(0%)]\t Loss: 0.2062  Pearson:0.9241 Spearman:0.9213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : Loss:0.2041 \n",
      "pearson： 0.9206351127221226 ALL Pearson: 0.9243800660476924\n",
      "spearman： 0.9166476611875958 ALL Spearman: 0.9162907086877645\n",
      "time: 35.78259086608887\n",
      "0.009\n",
      "Train Epoch:122 [(0%)]\t Loss: 0.1319  Pearson:0.9446 Spearman:0.9287\n",
      "Train ALL Pearson: 0.9471864956739136\n",
      "Train  ALL Spearman: 0.9380695635654221\n",
      "32.032732248306274\n",
      "Test Epoch:122 [(0%)]\t Loss: 0.1885  Pearson:0.9191 Spearman:0.9173\n",
      "Test : Loss:0.1997 \n",
      "pearson： 0.9241055336384952 ALL Pearson: 0.9260294368800035\n",
      "spearman： 0.9157273342585494 ALL Spearman: 0.9168174672903433\n",
      "time: 36.96764922142029\n",
      "0.009\n",
      "Train Epoch:123 [(0%)]\t Loss: 0.1417  Pearson:0.9421 Spearman:0.9167\n",
      "Train ALL Pearson: 0.9484814266990297\n",
      "Train  ALL Spearman: 0.9397409236104033\n",
      "31.60268807411194\n",
      "Test Epoch:123 [(0%)]\t Loss: 0.2060  Pearson:0.9305 Spearman:0.9382\n",
      "Test : Loss:0.1945 \n",
      "pearson： 0.925198675423513 ALL Pearson: 0.9257971951845361\n",
      "spearman： 0.9217622027527047 ALL Spearman: 0.91640157861006\n",
      "time: 36.321173906326294\n",
      "0.009\n",
      "Train Epoch:124 [(0%)]\t Loss: 0.1343  Pearson:0.9525 Spearman:0.9481\n",
      "Train ALL Pearson: 0.9498943539761484\n",
      "Train  ALL Spearman: 0.9406736162559812\n",
      "32.00875806808472\n",
      "Test Epoch:124 [(0%)]\t Loss: 0.1973  Pearson:0.9292 Spearman:0.8968\n",
      "Test : Loss:0.1850 \n",
      "pearson： 0.925270214216495 ALL Pearson: 0.9265389777862849\n",
      "spearman： 0.9117428394888255 ALL Spearman: 0.9159237199560347\n",
      "time: 36.543304204940796\n",
      "0.009\n",
      "Train Epoch:125 [(0%)]\t Loss: 0.1304  Pearson:0.9576 Spearman:0.9593\n",
      "Train ALL Pearson: 0.9491943672180543\n",
      "Train  ALL Spearman: 0.9397213370026655\n",
      "31.900002241134644\n",
      "Test Epoch:125 [(0%)]\t Loss: 0.1840  Pearson:0.9252 Spearman:0.9116\n",
      "Test : Loss:0.2017 \n",
      "pearson： 0.9280185315835703 ALL Pearson: 0.9263529046490443\n",
      "spearman： 0.9169222622160931 ALL Spearman: 0.9161180351473375\n",
      "time: 36.731555461883545\n",
      "0.009\n",
      "Train Epoch:126 [(0%)]\t Loss: 0.1278  Pearson:0.9477 Spearman:0.9265\n",
      "Train ALL Pearson: 0.9493916316057788\n",
      "Train  ALL Spearman: 0.9393380449466833\n",
      "31.74219584465027\n",
      "Test Epoch:126 [(0%)]\t Loss: 0.1848  Pearson:0.9364 Spearman:0.9226\n",
      "Test : Loss:0.1935 \n",
      "pearson： 0.9302104783534997 ALL Pearson: 0.9259608310175848\n",
      "spearman： 0.9186203591354256 ALL Spearman: 0.9166388003283905\n",
      "time: 36.333723068237305\n",
      "0.009\n",
      "Train Epoch:127 [(0%)]\t Loss: 0.1179  Pearson:0.9514 Spearman:0.9514\n",
      "Train ALL Pearson: 0.9509966982256315\n",
      "Train  ALL Spearman: 0.9413045832671618\n",
      "31.264280080795288\n",
      "Test Epoch:127 [(0%)]\t Loss: 0.1696  Pearson:0.9315 Spearman:0.9083\n",
      "Test : Loss:0.1863 \n",
      "pearson： 0.928347337767181 ALL Pearson: 0.9271623494768811\n",
      "spearman： 0.9166520857580672 ALL Spearman: 0.9170387201442298\n",
      "time: 36.02621603012085\n",
      "0.009\n",
      "Train Epoch:128 [(0%)]\t Loss: 0.1237  Pearson:0.9548 Spearman:0.9511\n",
      "Train ALL Pearson: 0.9511128751962674\n",
      "Train  ALL Spearman: 0.9415442460045036\n",
      "31.777309894561768\n",
      "Test Epoch:128 [(0%)]\t Loss: 0.1787  Pearson:0.9354 Spearman:0.9197\n",
      "Test : Loss:0.1863 \n",
      "pearson： 0.9279574093012056 ALL Pearson: 0.9279732829495402\n",
      "spearman： 0.9179079154009716 ALL Spearman: 0.91708163466474\n",
      "time: 36.43781518936157\n",
      "0.009\n",
      "Train Epoch:129 [(0%)]\t Loss: 0.1293  Pearson:0.9541 Spearman:0.9361\n",
      "Train ALL Pearson: 0.9506112293525822\n",
      "Train  ALL Spearman: 0.9412108220010388\n",
      "31.431236267089844\n",
      "Test Epoch:129 [(0%)]\t Loss: 0.2157  Pearson:0.9134 Spearman:0.9027\n",
      "Test : Loss:0.2017 \n",
      "pearson： 0.9230584663291995 ALL Pearson: 0.9271863586027088\n",
      "spearman： 0.9133852308714604 ALL Spearman: 0.9163142685716654\n",
      "time: 36.033761501312256\n",
      "0.009\n",
      "Train Epoch:130 [(0%)]\t Loss: 0.1306  Pearson:0.9502 Spearman:0.9467\n",
      "Train ALL Pearson: 0.951395845407615\n",
      "Train  ALL Spearman: 0.9424458169496545\n",
      "31.77135419845581\n",
      "Test Epoch:130 [(0%)]\t Loss: 0.1962  Pearson:0.9123 Spearman:0.9147\n",
      "Test : Loss:0.1882 \n",
      "pearson： 0.9246181710769522 ALL Pearson: 0.9279177848923643\n",
      "spearman： 0.9173425822848812 ALL Spearman: 0.9177213456100816\n",
      "time: 36.70677304267883\n",
      "0.009\n",
      "Train Epoch:131 [(0%)]\t Loss: 0.1357  Pearson:0.9340 Spearman:0.9305\n",
      "Train ALL Pearson: 0.950799452883335\n",
      "Train  ALL Spearman: 0.9405367819782806\n",
      "32.0748176574707\n",
      "Test Epoch:131 [(0%)]\t Loss: 0.1657  Pearson:0.9497 Spearman:0.9391\n",
      "Test : Loss:0.1869 \n",
      "pearson： 0.9322163954525255 ALL Pearson: 0.9281843258543186\n",
      "spearman： 0.921770332227778 ALL Spearman: 0.9170463218488658\n",
      "time: 36.59336805343628\n",
      "0.009\n",
      "Train Epoch:132 [(0%)]\t Loss: 0.1237  Pearson:0.9470 Spearman:0.9391\n",
      "Train ALL Pearson: 0.9517312329735715\n",
      "Train  ALL Spearman: 0.9419198336507594\n",
      "31.199946403503418\n",
      "Test Epoch:132 [(0%)]\t Loss: 0.1747  Pearson:0.9377 Spearman:0.9164\n",
      "Test : Loss:0.1805 \n",
      "pearson： 0.932471473617499 ALL Pearson: 0.9286091682740581\n",
      "spearman： 0.920413675296103 ALL Spearman: 0.9176640066934515\n",
      "time: 35.89237308502197\n",
      "0.009\n",
      "Train Epoch:133 [(0%)]\t Loss: 0.1139  Pearson:0.9555 Spearman:0.9543\n",
      "Train ALL Pearson: 0.9514386969373915\n",
      "Train  ALL Spearman: 0.9412730555385144\n",
      "31.365541219711304\n",
      "Test Epoch:133 [(0%)]\t Loss: 0.2120  Pearson:0.9186 Spearman:0.9134\n",
      "Test : Loss:0.1951 \n",
      "pearson： 0.9271441341464435 ALL Pearson: 0.929093918567522\n",
      "spearman： 0.9196730538803674 ALL Spearman: 0.9176266390742792\n",
      "time: 36.29681348800659\n",
      "0.009\n",
      "Train Epoch:134 [(0%)]\t Loss: 0.1380  Pearson:0.9514 Spearman:0.9357\n",
      "Train ALL Pearson: 0.9514728944131539\n",
      "Train  ALL Spearman: 0.9415075542674044\n",
      "31.589488744735718\n",
      "Test Epoch:134 [(0%)]\t Loss: 0.1802  Pearson:0.9363 Spearman:0.8994\n",
      "Test : Loss:0.1831 \n",
      "pearson： 0.9289514017894818 ALL Pearson: 0.9290010794618837\n",
      "spearman： 0.9134521689677025 ALL Spearman: 0.9179331569557198\n",
      "time: 36.33296823501587\n",
      "0.009\n",
      "Train Epoch:135 [(0%)]\t Loss: 0.1318  Pearson:0.9368 Spearman:0.9250\n",
      "Train ALL Pearson: 0.9524864969157274\n",
      "Train  ALL Spearman: 0.9424091555295963\n",
      "31.706332683563232\n",
      "Test Epoch:135 [(0%)]\t Loss: 0.1865  Pearson:0.9179 Spearman:0.9089\n",
      "Test : Loss:0.1820 \n",
      "pearson： 0.9284933370270565 ALL Pearson: 0.9283600601107795\n",
      "spearman： 0.9161496515560272 ALL Spearman: 0.9180962433099064\n",
      "time: 36.37083697319031\n",
      "0.009\n",
      "Train Epoch:136 [(0%)]\t Loss: 0.1287  Pearson:0.9466 Spearman:0.9289\n",
      "Train ALL Pearson: 0.9522866612191714\n",
      "Train  ALL Spearman: 0.9424623359347662\n",
      "31.561245918273926\n",
      "Test Epoch:136 [(0%)]\t Loss: 0.1754  Pearson:0.9226 Spearman:0.9120\n",
      "Test : Loss:0.1767 \n",
      "pearson： 0.9275947173991576 ALL Pearson: 0.9293061400820458\n",
      "spearman： 0.914664767628869 ALL Spearman: 0.917763883250122\n",
      "time: 36.23774695396423\n",
      "0.009\n",
      "Train Epoch:137 [(0%)]\t Loss: 0.1342  Pearson:0.9512 Spearman:0.9513\n",
      "Train ALL Pearson: 0.9522985129582867\n",
      "Train  ALL Spearman: 0.9426027077427683\n",
      "31.822446584701538\n",
      "Test Epoch:137 [(0%)]\t Loss: 0.1769  Pearson:0.9145 Spearman:0.8903\n",
      "Test : Loss:0.1788 \n",
      "pearson： 0.9251570996838971 ALL Pearson: 0.9295123684974523\n",
      "spearman： 0.9109597925426779 ALL Spearman: 0.9182466689622337\n",
      "time: 36.526936054229736\n",
      "0.009\n",
      "Train Epoch:138 [(0%)]\t Loss: 0.1269  Pearson:0.9334 Spearman:0.9274\n",
      "Train ALL Pearson: 0.9531623280122871\n",
      "Train  ALL Spearman: 0.9434534613900213\n",
      "31.847704648971558\n",
      "Test Epoch:138 [(0%)]\t Loss: 0.1948  Pearson:0.9309 Spearman:0.9208\n",
      "Test : Loss:0.1851 \n",
      "pearson： 0.9324586243690903 ALL Pearson: 0.9294289427502131\n",
      "spearman： 0.92011675220658 ALL Spearman: 0.9190326466906031\n",
      "time: 36.45092296600342\n",
      "0.009\n",
      "Train Epoch:139 [(0%)]\t Loss: 0.1239  Pearson:0.9502 Spearman:0.9338\n",
      "Train ALL Pearson: 0.9527979063571289\n",
      "Train  ALL Spearman: 0.9426777262326961\n",
      "31.869813203811646\n",
      "Test Epoch:139 [(0%)]\t Loss: 0.1990  Pearson:0.9347 Spearman:0.9296\n",
      "Test : Loss:0.1947 \n",
      "pearson： 0.9324249968891724 ALL Pearson: 0.9297451859241304\n",
      "spearman： 0.9232659448650075 ALL Spearman: 0.9185254110229589\n",
      "time: 36.65608882904053\n",
      "0.009\n",
      "Train Epoch:140 [(0%)]\t Loss: 0.1225  Pearson:0.9474 Spearman:0.9390\n",
      "Train ALL Pearson: 0.95485070064443\n",
      "Train  ALL Spearman: 0.9460889584731872\n",
      "31.511582136154175\n",
      "Test Epoch:140 [(0%)]\t Loss: 0.2003  Pearson:0.9347 Spearman:0.9280\n",
      "Test : Loss:0.1946 \n",
      "pearson： 0.9306700768638514 ALL Pearson: 0.9306492760565597\n",
      "spearman： 0.9221241552488858 ALL Spearman: 0.9192206824965077\n",
      "time: 36.41095018386841\n",
      "0.009\n",
      "Train Epoch:141 [(0%)]\t Loss: 0.1253  Pearson:0.9593 Spearman:0.9455\n",
      "Train ALL Pearson: 0.9542575113515023\n",
      "Train  ALL Spearman: 0.9462038936888725\n",
      "31.51381492614746\n",
      "Test Epoch:141 [(0%)]\t Loss: 0.1971  Pearson:0.9215 Spearman:0.9072\n",
      "Test : Loss:0.2013 \n",
      "pearson： 0.9283375109348515 ALL Pearson: 0.9301240401999255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9156538095252234 ALL Spearman: 0.9190751192118325\n",
      "time: 36.299280405044556\n",
      "0.009\n",
      "Train Epoch:142 [(0%)]\t Loss: 0.1255  Pearson:0.9608 Spearman:0.9358\n",
      "Train ALL Pearson: 0.9543032122864802\n",
      "Train  ALL Spearman: 0.9450939724930458\n",
      "32.046710729599\n",
      "Test Epoch:142 [(0%)]\t Loss: 0.1740  Pearson:0.9397 Spearman:0.9342\n",
      "Test : Loss:0.1883 \n",
      "pearson： 0.9316069885619018 ALL Pearson: 0.9297872767991676\n",
      "spearman： 0.9187242635045231 ALL Spearman: 0.9191883035828792\n",
      "time: 36.930108308792114\n",
      "0.009\n",
      "Train Epoch:143 [(0%)]\t Loss: 0.1253  Pearson:0.9527 Spearman:0.9517\n",
      "Train ALL Pearson: 0.9537448599425273\n",
      "Train  ALL Spearman: 0.9435259562391866\n",
      "32.04530429840088\n",
      "Test Epoch:143 [(0%)]\t Loss: 0.1661  Pearson:0.9264 Spearman:0.9153\n",
      "Test : Loss:0.1801 \n",
      "pearson： 0.9326334132036131 ALL Pearson: 0.9297363633949256\n",
      "spearman： 0.9215974538179684 ALL Spearman: 0.9188646791831545\n",
      "time: 36.68437838554382\n",
      "0.009\n",
      "Train Epoch:144 [(0%)]\t Loss: 0.1307  Pearson:0.9592 Spearman:0.9448\n",
      "Train ALL Pearson: 0.9551407347263169\n",
      "Train  ALL Spearman: 0.945761840123224\n",
      "31.927117824554443\n",
      "Test Epoch:144 [(0%)]\t Loss: 0.1667  Pearson:0.9305 Spearman:0.9118\n",
      "Test : Loss:0.1725 \n",
      "pearson： 0.9304544214808653 ALL Pearson: 0.930718692495513\n",
      "spearman： 0.9188255827670653 ALL Spearman: 0.918940603039242\n",
      "time: 36.61217498779297\n",
      "0.009\n",
      "Train Epoch:145 [(0%)]\t Loss: 0.1269  Pearson:0.9544 Spearman:0.9406\n",
      "Train ALL Pearson: 0.9545131772493527\n",
      "Train  ALL Spearman: 0.9444260610712525\n",
      "31.87958312034607\n",
      "Test Epoch:145 [(0%)]\t Loss: 0.1965  Pearson:0.9103 Spearman:0.9040\n",
      "Test : Loss:0.1817 \n",
      "pearson： 0.9278749938078878 ALL Pearson: 0.9303256653537338\n",
      "spearman： 0.9158023517893401 ALL Spearman: 0.918490772115896\n",
      "time: 36.818578243255615\n",
      "0.009\n",
      "Train Epoch:146 [(0%)]\t Loss: 0.1097  Pearson:0.9556 Spearman:0.9453\n",
      "Train ALL Pearson: 0.9551174401655502\n",
      "Train  ALL Spearman: 0.9454851737217117\n",
      "31.49411368370056\n",
      "Test Epoch:146 [(0%)]\t Loss: 0.1773  Pearson:0.9248 Spearman:0.9012\n",
      "Test : Loss:0.1843 \n",
      "pearson： 0.9271271064764335 ALL Pearson: 0.9294830442920631\n",
      "spearman： 0.9132222085137139 ALL Spearman: 0.9189518116978461\n",
      "time: 36.12962794303894\n",
      "0.009\n",
      "Train Epoch:147 [(0%)]\t Loss: 0.1273  Pearson:0.9576 Spearman:0.9441\n",
      "Train ALL Pearson: 0.956480005297722\n",
      "Train  ALL Spearman: 0.9469338508595376\n",
      "32.42560291290283\n",
      "Test Epoch:147 [(0%)]\t Loss: 0.2033  Pearson:0.9105 Spearman:0.9007\n",
      "Test : Loss:0.1841 \n",
      "pearson： 0.9255007405808623 ALL Pearson: 0.9304447345548724\n",
      "spearman： 0.9144720420649048 ALL Spearman: 0.9192831627831884\n",
      "time: 37.23306226730347\n",
      "0.009\n",
      "Train Epoch:148 [(0%)]\t Loss: 0.1307  Pearson:0.9594 Spearman:0.9499\n",
      "Train ALL Pearson: 0.9562744092523737\n",
      "Train  ALL Spearman: 0.9465081736104942\n",
      "31.840356826782227\n",
      "Test Epoch:148 [(0%)]\t Loss: 0.1643  Pearson:0.9342 Spearman:0.9226\n",
      "Test : Loss:0.1757 \n",
      "pearson： 0.9317939754640752 ALL Pearson: 0.9306295537651382\n",
      "spearman： 0.9196249447335562 ALL Spearman: 0.9188463324326981\n",
      "time: 36.70079946517944\n",
      "0.009\n",
      "Train Epoch:149 [(0%)]\t Loss: 0.1191  Pearson:0.9546 Spearman:0.9368\n",
      "Train ALL Pearson: 0.9547297564680755\n",
      "Train  ALL Spearman: 0.9459668342798314\n",
      "31.68206810951233\n",
      "Test Epoch:149 [(0%)]\t Loss: 0.1952  Pearson:0.9292 Spearman:0.9329\n",
      "Test : Loss:0.1901 \n",
      "pearson： 0.9296482395348985 ALL Pearson: 0.9305691430063091\n",
      "spearman： 0.9229798474788855 ALL Spearman: 0.919417406255609\n",
      "time: 36.25160264968872\n",
      "0.009\n",
      "Train Epoch:150 [(0%)]\t Loss: 0.1288  Pearson:0.9491 Spearman:0.9289\n",
      "Train ALL Pearson: 0.9559061223684964\n",
      "Train  ALL Spearman: 0.9461875256361528\n",
      "31.617889165878296\n",
      "Test Epoch:150 [(0%)]\t Loss: 0.1927  Pearson:0.9368 Spearman:0.9207\n",
      "Test : Loss:0.1765 \n",
      "pearson： 0.9338135202424641 ALL Pearson: 0.9308217290430396\n",
      "spearman： 0.9211519439373063 ALL Spearman: 0.9190797948494418\n",
      "time: 36.615288496017456\n",
      "0.009\n",
      "Train Epoch:151 [(0%)]\t Loss: 0.1045  Pearson:0.9602 Spearman:0.9587\n",
      "Train ALL Pearson: 0.9556023214646162\n",
      "Train  ALL Spearman: 0.946073402563105\n",
      "31.646562099456787\n",
      "Test Epoch:151 [(0%)]\t Loss: 0.1957  Pearson:0.9333 Spearman:0.9194\n",
      "Test : Loss:0.2008 \n",
      "pearson： 0.9310752109644226 ALL Pearson: 0.9308711890609881\n",
      "spearman： 0.9212182219088956 ALL Spearman: 0.9197285490757565\n",
      "time: 36.260082721710205\n",
      "0.009\n",
      "Train Epoch:152 [(0%)]\t Loss: 0.1351  Pearson:0.9429 Spearman:0.9427\n",
      "Train ALL Pearson: 0.956324066289168\n",
      "Train  ALL Spearman: 0.9468449885310023\n",
      "31.79837393760681\n",
      "Test Epoch:152 [(0%)]\t Loss: 0.1799  Pearson:0.9399 Spearman:0.9061\n",
      "Test : Loss:0.1787 \n",
      "pearson： 0.9321761734172263 ALL Pearson: 0.9313254780259007\n",
      "spearman： 0.9191339517376537 ALL Spearman: 0.9195186523389212\n",
      "time: 36.40489745140076\n",
      "0.009\n",
      "Train Epoch:153 [(0%)]\t Loss: 0.1328  Pearson:0.9463 Spearman:0.9375\n",
      "Train ALL Pearson: 0.9566285852893502\n",
      "Train  ALL Spearman: 0.9463489189531414\n",
      "31.418068408966064\n",
      "Test Epoch:153 [(0%)]\t Loss: 0.1768  Pearson:0.9246 Spearman:0.8974\n",
      "Test : Loss:0.1737 \n",
      "pearson： 0.9300783314930839 ALL Pearson: 0.9317525803145422\n",
      "spearman： 0.9140103433625199 ALL Spearman: 0.9195690768164156\n",
      "time: 36.1835401058197\n",
      "0.009\n",
      "Train Epoch:154 [(0%)]\t Loss: 0.1170  Pearson:0.9623 Spearman:0.9424\n",
      "Train ALL Pearson: 0.9571041231343526\n",
      "Train  ALL Spearman: 0.9477694202932414\n",
      "31.60898995399475\n",
      "Test Epoch:154 [(0%)]\t Loss: 0.1741  Pearson:0.9443 Spearman:0.9256\n",
      "Test : Loss:0.1796 \n",
      "pearson： 0.9320141419415244 ALL Pearson: 0.9312618526769878\n",
      "spearman： 0.921487050271728 ALL Spearman: 0.9202082530558477\n",
      "time: 36.258498668670654\n",
      "0.009\n",
      "Train Epoch:155 [(0%)]\t Loss: 0.1196  Pearson:0.9535 Spearman:0.9328\n",
      "Train ALL Pearson: 0.9567322806409124\n",
      "Train  ALL Spearman: 0.9471969932283981\n",
      "31.98066544532776\n",
      "Test Epoch:155 [(0%)]\t Loss: 0.1908  Pearson:0.9310 Spearman:0.9202\n",
      "Test : Loss:0.1891 \n",
      "pearson： 0.9304007985115409 ALL Pearson: 0.93027559370942\n",
      "spearman： 0.9208005913313663 ALL Spearman: 0.9199820123027123\n",
      "time: 36.62317633628845\n",
      "0.009\n",
      "Train Epoch:156 [(0%)]\t Loss: 0.1108  Pearson:0.9588 Spearman:0.9527\n",
      "Train ALL Pearson: 0.9563243542986036\n",
      "Train  ALL Spearman: 0.9470922948532499\n",
      "31.830347776412964\n",
      "Test Epoch:156 [(0%)]\t Loss: 0.1851  Pearson:0.9078 Spearman:0.8703\n",
      "Test : Loss:0.1681 \n",
      "pearson： 0.9276170804240156 ALL Pearson: 0.9317762964821974\n",
      "spearman： 0.909191985076832 ALL Spearman: 0.9198337781790968\n",
      "time: 36.680792808532715\n",
      "0.009\n",
      "Train Epoch:157 [(0%)]\t Loss: 0.1194  Pearson:0.9596 Spearman:0.9490\n",
      "Train ALL Pearson: 0.9565083026275604\n",
      "Train  ALL Spearman: 0.9475028415099076\n",
      "31.574225664138794\n",
      "Test Epoch:157 [(0%)]\t Loss: 0.1631  Pearson:0.9360 Spearman:0.9311\n",
      "Test : Loss:0.1741 \n",
      "pearson： 0.9321326848250869 ALL Pearson: 0.9320674821840672\n",
      "spearman： 0.9217818336134175 ALL Spearman: 0.9197183718950731\n",
      "time: 36.1842885017395\n",
      "0.009\n",
      "Train Epoch:158 [(0%)]\t Loss: 0.1152  Pearson:0.9602 Spearman:0.9549\n",
      "Train ALL Pearson: 0.9585885814885103\n",
      "Train  ALL Spearman: 0.9497633961337879\n",
      "31.84816312789917\n",
      "Test Epoch:158 [(0%)]\t Loss: 0.1977  Pearson:0.9313 Spearman:0.8991\n",
      "Test : Loss:0.1984 \n",
      "pearson： 0.9302028616748337 ALL Pearson: 0.9296656729108477\n",
      "spearman： 0.913977249864026 ALL Spearman: 0.920109085016069\n",
      "time: 36.57175421714783\n",
      "0.009\n",
      "Train Epoch:159 [(0%)]\t Loss: 0.1065  Pearson:0.9674 Spearman:0.9591\n",
      "Train ALL Pearson: 0.9575241268482882\n",
      "Train  ALL Spearman: 0.9477198879929855\n",
      "31.559834718704224\n",
      "Test Epoch:159 [(0%)]\t Loss: 0.1974  Pearson:0.9160 Spearman:0.8713\n",
      "Test : Loss:0.1897 \n",
      "pearson： 0.9286890908262785 ALL Pearson: 0.931126234613338\n",
      "spearman： 0.9109187209983515 ALL Spearman: 0.9195786263120194\n",
      "time: 36.2906813621521\n",
      "0.009\n",
      "Train Epoch:160 [(0%)]\t Loss: 0.1133  Pearson:0.9530 Spearman:0.9472\n",
      "Train ALL Pearson: 0.9581511091028388\n",
      "Train  ALL Spearman: 0.9485332291393526\n",
      "31.804670095443726\n",
      "Test Epoch:160 [(0%)]\t Loss: 0.1947  Pearson:0.9374 Spearman:0.9342\n",
      "Test : Loss:0.1979 \n",
      "pearson： 0.9342160551411446 ALL Pearson: 0.9313345832132771\n",
      "spearman： 0.9261958935556719 ALL Spearman: 0.9203547379777963\n",
      "time: 36.75808238983154\n",
      "0.009\n",
      "Train Epoch:161 [(0%)]\t Loss: 0.1019  Pearson:0.9679 Spearman:0.9575\n",
      "Train ALL Pearson: 0.9578450309326175\n",
      "Train  ALL Spearman: 0.9486870006412192\n",
      "31.67139172554016\n",
      "Test Epoch:161 [(0%)]\t Loss: 0.1897  Pearson:0.9425 Spearman:0.9422\n",
      "Test : Loss:0.2026 \n",
      "pearson： 0.9354606282106958 ALL Pearson: 0.9307157278119246\n",
      "spearman： 0.9289368454433624 ALL Spearman: 0.9206990547388866\n",
      "time: 36.42340850830078\n",
      "0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:162 [(0%)]\t Loss: 0.1339  Pearson:0.9578 Spearman:0.9590\n",
      "Train ALL Pearson: 0.9568980147065839\n",
      "Train  ALL Spearman: 0.9472830471664526\n",
      "32.06646370887756\n",
      "Test Epoch:162 [(0%)]\t Loss: 0.1811  Pearson:0.9279 Spearman:0.9261\n",
      "Test : Loss:0.1797 \n",
      "pearson： 0.9319391540098937 ALL Pearson: 0.9314998432736901\n",
      "spearman： 0.9210115676751793 ALL Spearman: 0.9209343578876144\n",
      "time: 36.6935350894928\n",
      "0.009\n",
      "Train Epoch:163 [(0%)]\t Loss: 0.1038  Pearson:0.9697 Spearman:0.9624\n",
      "Train ALL Pearson: 0.9588030317201914\n",
      "Train  ALL Spearman: 0.9498802306059237\n",
      "31.620863437652588\n",
      "Test Epoch:163 [(0%)]\t Loss: 0.1628  Pearson:0.9278 Spearman:0.9170\n",
      "Test : Loss:0.1672 \n",
      "pearson： 0.9310719920263896 ALL Pearson: 0.9323009470505879\n",
      "spearman： 0.9188663373565951 ALL Spearman: 0.9208566993117563\n",
      "time: 36.279805183410645\n",
      "0.009\n",
      "Train Epoch:164 [(0%)]\t Loss: 0.1009  Pearson:0.9711 Spearman:0.9535\n",
      "Train ALL Pearson: 0.9582787138524018\n",
      "Train  ALL Spearman: 0.9488857886960727\n",
      "32.077746629714966\n",
      "Test Epoch:164 [(0%)]\t Loss: 0.1889  Pearson:0.9343 Spearman:0.9083\n",
      "Test : Loss:0.1859 \n",
      "pearson： 0.9322997525727338 ALL Pearson: 0.9313667929935531\n",
      "spearman： 0.9189220936151542 ALL Spearman: 0.9207669798658151\n",
      "time: 36.8102285861969\n",
      "0.009\n",
      "Train Epoch:165 [(0%)]\t Loss: 0.1357  Pearson:0.9505 Spearman:0.9356\n",
      "Train ALL Pearson: 0.9590221939844192\n",
      "Train  ALL Spearman: 0.9490739300732509\n",
      "31.49574565887451\n",
      "Test Epoch:165 [(0%)]\t Loss: 0.1614  Pearson:0.9183 Spearman:0.9153\n",
      "Test : Loss:0.1668 \n",
      "pearson： 0.9290890579830109 ALL Pearson: 0.9316671715923808\n",
      "spearman： 0.9164016353828692 ALL Spearman: 0.9198640702875067\n",
      "time: 36.471149921417236\n",
      "0.009\n",
      "Train Epoch:166 [(0%)]\t Loss: 0.1153  Pearson:0.9578 Spearman:0.9542\n",
      "Train ALL Pearson: 0.9592814795835023\n",
      "Train  ALL Spearman: 0.9510370522694634\n",
      "31.561590909957886\n",
      "Test Epoch:166 [(0%)]\t Loss: 0.2291  Pearson:0.9168 Spearman:0.8885\n",
      "Test : Loss:0.2069 \n",
      "pearson： 0.929303345091573 ALL Pearson: 0.930825311104602\n",
      "spearman： 0.9135971127552325 ALL Spearman: 0.9204399780598586\n",
      "time: 36.065147399902344\n",
      "0.009\n",
      "Train Epoch:167 [(0%)]\t Loss: 0.1162  Pearson:0.9669 Spearman:0.9646\n",
      "Train ALL Pearson: 0.9586634714512581\n",
      "Train  ALL Spearman: 0.9486026494512839\n",
      "32.158721685409546\n",
      "Test Epoch:167 [(0%)]\t Loss: 0.2092  Pearson:0.9181 Spearman:0.9118\n",
      "Test : Loss:0.1849 \n",
      "pearson： 0.9315951991214342 ALL Pearson: 0.9312491072406556\n",
      "spearman： 0.9213754028206153 ALL Spearman: 0.9203880396207517\n",
      "time: 37.000168800354004\n",
      "0.009\n",
      "Train Epoch:168 [(0%)]\t Loss: 0.1100  Pearson:0.9584 Spearman:0.9454\n",
      "Train ALL Pearson: 0.9590547632545329\n",
      "Train  ALL Spearman: 0.9501581845404177\n",
      "31.7802152633667\n",
      "Test Epoch:168 [(0%)]\t Loss: 0.1717  Pearson:0.9455 Spearman:0.9320\n",
      "Test : Loss:0.1687 \n",
      "pearson： 0.9334806781799542 ALL Pearson: 0.9313894287774446\n",
      "spearman： 0.923283247753741 ALL Spearman: 0.9194710101090978\n",
      "time: 36.42055559158325\n",
      "0.009\n",
      "Train Epoch:169 [(0%)]\t Loss: 0.1064  Pearson:0.9702 Spearman:0.9539\n",
      "Train ALL Pearson: 0.9592431311112779\n",
      "Train  ALL Spearman: 0.9501397812713527\n",
      "31.82917833328247\n",
      "Test Epoch:169 [(0%)]\t Loss: 0.1840  Pearson:0.9392 Spearman:0.9175\n",
      "Test : Loss:0.1736 \n",
      "pearson： 0.9351927213424 ALL Pearson: 0.9319470573951941\n",
      "spearman： 0.9200847269079738 ALL Spearman: 0.9188303680451905\n",
      "time: 36.6346390247345\n",
      "0.009\n",
      "Train Epoch:170 [(0%)]\t Loss: 0.1104  Pearson:0.9628 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9586413957267259\n",
      "Train  ALL Spearman: 0.9493681337916613\n",
      "31.500358819961548\n",
      "Test Epoch:170 [(0%)]\t Loss: 0.1871  Pearson:0.9295 Spearman:0.9152\n",
      "Test : Loss:0.1853 \n",
      "pearson： 0.9336350265399015 ALL Pearson: 0.9311699381091583\n",
      "spearman： 0.9208052477258456 ALL Spearman: 0.9193370965829787\n",
      "time: 36.19585347175598\n",
      "0.009\n",
      "Train Epoch:171 [(0%)]\t Loss: 0.1224  Pearson:0.9438 Spearman:0.9446\n",
      "Train ALL Pearson: 0.9590016878838522\n",
      "Train  ALL Spearman: 0.9483286289907465\n",
      "32.002336263656616\n",
      "Test Epoch:171 [(0%)]\t Loss: 0.1804  Pearson:0.9233 Spearman:0.9165\n",
      "Test : Loss:0.1678 \n",
      "pearson： 0.9307451267322947 ALL Pearson: 0.932277142620379\n",
      "spearman： 0.9197311890840936 ALL Spearman: 0.9198485044552244\n",
      "time: 36.68683409690857\n",
      "0.009\n",
      "Train Epoch:172 [(0%)]\t Loss: 0.1183  Pearson:0.9653 Spearman:0.9578\n",
      "Train ALL Pearson: 0.9600844162913381\n",
      "Train  ALL Spearman: 0.9509801710774632\n",
      "31.765348196029663\n",
      "Test Epoch:172 [(0%)]\t Loss: 0.1746  Pearson:0.9250 Spearman:0.9199\n",
      "Test : Loss:0.1792 \n",
      "pearson： 0.9298018185793943 ALL Pearson: 0.9320723136964659\n",
      "spearman： 0.9180410118263189 ALL Spearman: 0.9198737713878643\n",
      "time: 36.61879205703735\n",
      "0.009\n",
      "Train Epoch:173 [(0%)]\t Loss: 0.1140  Pearson:0.9614 Spearman:0.9477\n",
      "Train ALL Pearson: 0.9605492413366842\n",
      "Train  ALL Spearman: 0.9510334619550593\n",
      "31.555835962295532\n",
      "Test Epoch:173 [(0%)]\t Loss: 0.2112  Pearson:0.9242 Spearman:0.9094\n",
      "Test : Loss:0.1972 \n",
      "pearson： 0.9295100647908822 ALL Pearson: 0.9313356234324309\n",
      "spearman： 0.9167903615691937 ALL Spearman: 0.9200691918276022\n",
      "time: 36.21034336090088\n",
      "0.009\n",
      "Train Epoch:174 [(0%)]\t Loss: 0.1168  Pearson:0.9507 Spearman:0.9268\n",
      "Train ALL Pearson: 0.9595826924212474\n",
      "Train  ALL Spearman: 0.9499674116982624\n",
      "32.37472176551819\n",
      "Test Epoch:174 [(0%)]\t Loss: 0.1771  Pearson:0.9472 Spearman:0.9253\n",
      "Test : Loss:0.1869 \n",
      "pearson： 0.9344236974110731 ALL Pearson: 0.9326358182432181\n",
      "spearman： 0.9206500125754183 ALL Spearman: 0.9206142117354664\n",
      "time: 36.987242698669434\n",
      "0.009\n",
      "Train Epoch:175 [(0%)]\t Loss: 0.1163  Pearson:0.9616 Spearman:0.9478\n",
      "Train ALL Pearson: 0.9606610250879791\n",
      "Train  ALL Spearman: 0.9513392794290718\n",
      "32.06247305870056\n",
      "Test Epoch:175 [(0%)]\t Loss: 0.1838  Pearson:0.9342 Spearman:0.9273\n",
      "Test : Loss:0.1888 \n",
      "pearson： 0.933855828296209 ALL Pearson: 0.9325112726116994\n",
      "spearman： 0.9224629924849872 ALL Spearman: 0.9208904577955068\n",
      "time: 37.138845682144165\n",
      "0.009\n",
      "Train Epoch:176 [(0%)]\t Loss: 0.1157  Pearson:0.9506 Spearman:0.9466\n",
      "Train ALL Pearson: 0.9589493283658644\n",
      "Train  ALL Spearman: 0.9488871875165011\n",
      "31.797539949417114\n",
      "Test Epoch:176 [(0%)]\t Loss: 0.1566  Pearson:0.9319 Spearman:0.9130\n",
      "Test : Loss:0.1592 \n",
      "pearson： 0.9290653061266482 ALL Pearson: 0.9330284009212463\n",
      "spearman： 0.9178252419587177 ALL Spearman: 0.919969226530493\n",
      "time: 36.45365929603577\n",
      "0.009\n",
      "Train Epoch:177 [(0%)]\t Loss: 0.1106  Pearson:0.9730 Spearman:0.9616\n",
      "Train ALL Pearson: 0.9592965511708493\n",
      "Train  ALL Spearman: 0.9483646834836206\n",
      "31.625141620635986\n",
      "Test Epoch:177 [(0%)]\t Loss: 0.1777  Pearson:0.9349 Spearman:0.9129\n",
      "Test : Loss:0.1795 \n",
      "pearson： 0.9305790946747795 ALL Pearson: 0.9316070780313405\n",
      "spearman： 0.9167580264138201 ALL Spearman: 0.9197170571343459\n",
      "time: 36.216670513153076\n",
      "0.009\n",
      "Train Epoch:178 [(0%)]\t Loss: 0.1118  Pearson:0.9643 Spearman:0.9542\n",
      "Train ALL Pearson: 0.9605239725315009\n",
      "Train  ALL Spearman: 0.9512073243008298\n",
      "31.97148633003235\n",
      "Test Epoch:178 [(0%)]\t Loss: 0.2255  Pearson:0.9316 Spearman:0.9255\n",
      "Test : Loss:0.2106 \n",
      "pearson： 0.9331012897514348 ALL Pearson: 0.9314313120561506\n",
      "spearman： 0.9260813857952891 ALL Spearman: 0.9214257757907262\n",
      "time: 36.63475751876831\n",
      "0.009\n",
      "Train Epoch:179 [(0%)]\t Loss: 0.1225  Pearson:0.9597 Spearman:0.9491\n",
      "Train ALL Pearson: 0.9595977588567379\n",
      "Train  ALL Spearman: 0.9493186240377256\n",
      "31.361064910888672\n",
      "Test Epoch:179 [(0%)]\t Loss: 0.1885  Pearson:0.9272 Spearman:0.9123\n",
      "Test : Loss:0.1800 \n",
      "pearson： 0.9330005026655979 ALL Pearson: 0.9325088864981688\n",
      "spearman： 0.920552803406419 ALL Spearman: 0.9199783535954803\n",
      "time: 36.324472188949585\n",
      "0.009\n",
      "Train Epoch:180 [(0%)]\t Loss: 0.1182  Pearson:0.9707 Spearman:0.9681\n",
      "Train ALL Pearson: 0.960025159902025\n",
      "Train  ALL Spearman: 0.9501453210748682\n",
      "31.624687433242798\n",
      "Test Epoch:180 [(0%)]\t Loss: 0.1807  Pearson:0.9184 Spearman:0.8970\n",
      "Test : Loss:0.1850 \n",
      "pearson： 0.928677969514272 ALL Pearson: 0.9319226754793826\n",
      "spearman： 0.912182178259608 ALL Spearman: 0.9200989790384407\n",
      "time: 36.453258752822876\n",
      "0.009\n",
      "Train Epoch:181 [(0%)]\t Loss: 0.1089  Pearson:0.9684 Spearman:0.9537\n",
      "Train ALL Pearson: 0.9603853840315805\n",
      "Train  ALL Spearman: 0.9513288338232389\n",
      "31.737825632095337\n",
      "Test Epoch:181 [(0%)]\t Loss: 0.1848  Pearson:0.9409 Spearman:0.9297\n",
      "Test : Loss:0.1854 \n",
      "pearson： 0.9357797832293243 ALL Pearson: 0.9318893460009318\n",
      "spearman： 0.9244994203947527 ALL Spearman: 0.9194140453272884\n",
      "time: 36.22597670555115\n",
      "0.009\n",
      "Train Epoch:182 [(0%)]\t Loss: 0.1124  Pearson:0.9608 Spearman:0.9564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson: 0.961481704848384\n",
      "Train  ALL Spearman: 0.95200868810468\n",
      "31.895030736923218\n",
      "Test Epoch:182 [(0%)]\t Loss: 0.1916  Pearson:0.9182 Spearman:0.9142\n",
      "Test : Loss:0.1971 \n",
      "pearson： 0.9264602768871456 ALL Pearson: 0.9315325733049262\n",
      "spearman： 0.9183848763330382 ALL Spearman: 0.9197157015006319\n",
      "time: 36.66749858856201\n",
      "0.009\n",
      "Train Epoch:183 [(0%)]\t Loss: 0.1110  Pearson:0.9596 Spearman:0.9521\n",
      "Train ALL Pearson: 0.9610535096955608\n",
      "Train  ALL Spearman: 0.9517096357231841\n",
      "31.57908821105957\n",
      "Test Epoch:183 [(0%)]\t Loss: 0.2024  Pearson:0.9241 Spearman:0.9108\n",
      "Test : Loss:0.1884 \n",
      "pearson： 0.9302951223163273 ALL Pearson: 0.9316989494923371\n",
      "spearman： 0.9160321236488307 ALL Spearman: 0.9202264989852963\n",
      "time: 36.36255598068237\n",
      "0.009\n",
      "Train Epoch:184 [(0%)]\t Loss: 0.1185  Pearson:0.9693 Spearman:0.9562\n",
      "Train ALL Pearson: 0.95990541004552\n",
      "Train  ALL Spearman: 0.9492962941330373\n",
      "31.37145709991455\n",
      "Test Epoch:184 [(0%)]\t Loss: 0.1837  Pearson:0.9392 Spearman:0.9188\n",
      "Test : Loss:0.2011 \n",
      "pearson： 0.9349386457191913 ALL Pearson: 0.9311226191501188\n",
      "spearman： 0.9245316466293171 ALL Spearman: 0.9197096430724434\n",
      "time: 36.1362202167511\n",
      "0.009\n",
      "Train Epoch:185 [(0%)]\t Loss: 0.1066  Pearson:0.9641 Spearman:0.9656\n",
      "Train ALL Pearson: 0.9621918353954638\n",
      "Train  ALL Spearman: 0.9536741254605132\n",
      "31.930838346481323\n",
      "Test Epoch:185 [(0%)]\t Loss: 0.1831  Pearson:0.9347 Spearman:0.9182\n",
      "Test : Loss:0.1933 \n",
      "pearson： 0.934104503812523 ALL Pearson: 0.9318501753837208\n",
      "spearman： 0.9209909876949026 ALL Spearman: 0.9203927242332713\n",
      "time: 36.85479998588562\n",
      "0.009\n",
      "Train Epoch:186 [(0%)]\t Loss: 0.1382  Pearson:0.9466 Spearman:0.9271\n",
      "Train ALL Pearson: 0.9610512125781239\n",
      "Train  ALL Spearman: 0.9518773179003507\n",
      "32.16252875328064\n",
      "Test Epoch:186 [(0%)]\t Loss: 0.2295  Pearson:0.9325 Spearman:0.9209\n",
      "Test : Loss:0.2332 \n",
      "pearson： 0.9327896991413255 ALL Pearson: 0.930594855521171\n",
      "spearman： 0.9232214275630319 ALL Spearman: 0.9191978103967567\n",
      "time: 37.03473353385925\n",
      "0.009\n",
      "Train Epoch:187 [(0%)]\t Loss: 0.1402  Pearson:0.9582 Spearman:0.9550\n",
      "Train ALL Pearson: 0.960986718112715\n",
      "Train  ALL Spearman: 0.9516486394793089\n",
      "31.769867420196533\n",
      "Test Epoch:187 [(0%)]\t Loss: 0.1902  Pearson:0.9384 Spearman:0.9222\n",
      "Test : Loss:0.1909 \n",
      "pearson： 0.9345518915827232 ALL Pearson: 0.9315467526168892\n",
      "spearman： 0.9195041965101378 ALL Spearman: 0.9200302715557395\n",
      "time: 36.70097517967224\n",
      "0.009\n",
      "Train Epoch:188 [(0%)]\t Loss: 0.0991  Pearson:0.9758 Spearman:0.9666\n",
      "Train ALL Pearson: 0.961245630947267\n",
      "Train  ALL Spearman: 0.9515903263369521\n",
      "31.97416114807129\n",
      "Test Epoch:188 [(0%)]\t Loss: 0.1832  Pearson:0.9100 Spearman:0.9157\n",
      "Test : Loss:0.1756 \n",
      "pearson： 0.928466833179744 ALL Pearson: 0.9317320535505726\n",
      "spearman： 0.9213505450496752 ALL Spearman: 0.9198024636191369\n",
      "time: 36.592681646347046\n",
      "0.009\n",
      "Train Epoch:189 [(0%)]\t Loss: 0.1043  Pearson:0.9635 Spearman:0.9524\n",
      "Train ALL Pearson: 0.9605529919855595\n",
      "Train  ALL Spearman: 0.9512968669189444\n",
      "31.704554796218872\n",
      "Test Epoch:189 [(0%)]\t Loss: 0.1698  Pearson:0.9382 Spearman:0.9246\n",
      "Test : Loss:0.1745 \n",
      "pearson： 0.9344050225241833 ALL Pearson: 0.932609548415984\n",
      "spearman： 0.9199030299643038 ALL Spearman: 0.9203574726781569\n",
      "time: 36.384052753448486\n",
      "0.009\n",
      "Train Epoch:190 [(0%)]\t Loss: 0.1035  Pearson:0.9666 Spearman:0.9536\n",
      "Train ALL Pearson: 0.9601617892680578\n",
      "Train  ALL Spearman: 0.9504889145020269\n",
      "31.98969078063965\n",
      "Test Epoch:190 [(0%)]\t Loss: 0.1893  Pearson:0.9309 Spearman:0.9002\n",
      "Test : Loss:0.1924 \n",
      "pearson： 0.9329037771254319 ALL Pearson: 0.9325207939740356\n",
      "spearman： 0.9160280183568644 ALL Spearman: 0.9207139227690901\n",
      "time: 36.78115463256836\n",
      "0.009\n",
      "Train Epoch:191 [(0%)]\t Loss: 0.1141  Pearson:0.9599 Spearman:0.9652\n",
      "Train ALL Pearson: 0.9616749075088826\n",
      "Train  ALL Spearman: 0.9525027127794867\n",
      "32.082496643066406\n",
      "Test Epoch:191 [(0%)]\t Loss: 0.1381  Pearson:0.9503 Spearman:0.9403\n",
      "Test : Loss:0.1540 \n",
      "pearson： 0.9365052089956204 ALL Pearson: 0.9333994268588423\n",
      "spearman： 0.9232027309924082 ALL Spearman: 0.9185469034467663\n",
      "time: 36.65107250213623\n",
      "0.009\n",
      "Train Epoch:192 [(0%)]\t Loss: 0.1354  Pearson:0.9669 Spearman:0.9620\n",
      "Train ALL Pearson: 0.9607794780577338\n",
      "Train  ALL Spearman: 0.9506098227300631\n",
      "32.11064291000366\n",
      "Test Epoch:192 [(0%)]\t Loss: 0.1801  Pearson:0.9394 Spearman:0.9176\n",
      "Test : Loss:0.1835 \n",
      "pearson： 0.9329175791622085 ALL Pearson: 0.9327550378036465\n",
      "spearman： 0.9178498278539753 ALL Spearman: 0.9197384963958332\n",
      "time: 36.856313705444336\n",
      "0.009\n",
      "Train Epoch:193 [(0%)]\t Loss: 0.1138  Pearson:0.9702 Spearman:0.9615\n",
      "Train ALL Pearson: 0.9606411332272735\n",
      "Train  ALL Spearman: 0.9508015273485362\n",
      "32.09925436973572\n",
      "Test Epoch:193 [(0%)]\t Loss: 0.1954  Pearson:0.9469 Spearman:0.9250\n",
      "Test : Loss:0.2043 \n",
      "pearson： 0.9359546938495886 ALL Pearson: 0.9323808578991277\n",
      "spearman： 0.9201992123902322 ALL Spearman: 0.918901174774022\n",
      "time: 37.02495765686035\n",
      "0.009\n",
      "Train Epoch:194 [(0%)]\t Loss: 0.1251  Pearson:0.9692 Spearman:0.9648\n",
      "Train ALL Pearson: 0.9608010780882833\n",
      "Train  ALL Spearman: 0.9509733217041163\n",
      "31.46546769142151\n",
      "Test Epoch:194 [(0%)]\t Loss: 0.1641  Pearson:0.9464 Spearman:0.9145\n",
      "Test : Loss:0.1676 \n",
      "pearson： 0.9353887569088456 ALL Pearson: 0.9324098746542464\n",
      "spearman： 0.917998534332305 ALL Spearman: 0.918759384774264\n",
      "time: 36.067992210388184\n",
      "0.009\n",
      "Train Epoch:195 [(0%)]\t Loss: 0.1045  Pearson:0.9639 Spearman:0.9475\n",
      "Train ALL Pearson: 0.9616855755468765\n",
      "Train  ALL Spearman: 0.9531064152524171\n",
      "31.96515440940857\n",
      "Test Epoch:195 [(0%)]\t Loss: 0.1891  Pearson:0.9305 Spearman:0.9156\n",
      "Test : Loss:0.1655 \n",
      "pearson： 0.9336442863838692 ALL Pearson: 0.9324334034256513\n",
      "spearman： 0.9185760627823941 ALL Spearman: 0.9189143685941378\n",
      "time: 36.5956711769104\n",
      "0.009\n",
      "Train Epoch:196 [(0%)]\t Loss: 0.1169  Pearson:0.9586 Spearman:0.9575\n",
      "Train ALL Pearson: 0.9605089899200727\n",
      "Train  ALL Spearman: 0.9496182917309194\n",
      "31.588178157806396\n",
      "Test Epoch:196 [(0%)]\t Loss: 0.1628  Pearson:0.9327 Spearman:0.9068\n",
      "Test : Loss:0.1619 \n",
      "pearson： 0.9350187553956655 ALL Pearson: 0.9326765415582763\n",
      "spearman： 0.9165900108425906 ALL Spearman: 0.9188148776042869\n",
      "time: 36.238043546676636\n",
      "0.009\n",
      "Train Epoch:197 [(0%)]\t Loss: 0.1072  Pearson:0.9718 Spearman:0.9640\n",
      "Train ALL Pearson: 0.9617878992098999\n",
      "Train  ALL Spearman: 0.9525064667821388\n",
      "31.773410320281982\n",
      "Test Epoch:197 [(0%)]\t Loss: 0.1615  Pearson:0.9398 Spearman:0.9307\n",
      "Test : Loss:0.1558 \n",
      "pearson： 0.9350194763113963 ALL Pearson: 0.9328395278853753\n",
      "spearman： 0.9233443087583851 ALL Spearman: 0.9192602964058899\n",
      "time: 36.43891429901123\n",
      "0.009\n",
      "Train Epoch:198 [(0%)]\t Loss: 0.1151  Pearson:0.9754 Spearman:0.9608\n",
      "Train ALL Pearson: 0.9627385397432722\n",
      "Train  ALL Spearman: 0.9532871168981208\n",
      "31.632791757583618\n",
      "Test Epoch:198 [(0%)]\t Loss: 0.1851  Pearson:0.9377 Spearman:0.9190\n",
      "Test : Loss:0.1833 \n",
      "pearson： 0.9319861956615122 ALL Pearson: 0.9325542258948443\n",
      "spearman： 0.9199090567267489 ALL Spearman: 0.9193829578702628\n",
      "time: 36.27230429649353\n",
      "0.009\n",
      "Train Epoch:199 [(0%)]\t Loss: 0.0997  Pearson:0.9722 Spearman:0.9671\n",
      "Train ALL Pearson: 0.9622499369605264\n",
      "Train  ALL Spearman: 0.9523449215811581\n",
      "31.95950722694397\n",
      "Test Epoch:199 [(0%)]\t Loss: 0.1608  Pearson:0.9404 Spearman:0.9342\n",
      "Test : Loss:0.1573 \n",
      "pearson： 0.9386200542037859 ALL Pearson: 0.9323520719814998\n",
      "spearman： 0.927043715847763 ALL Spearman: 0.9189666662815034\n",
      "time: 36.7613251209259\n",
      "0.009\n",
      "Train Epoch:200 [(0%)]\t Loss: 0.1168  Pearson:0.9643 Spearman:0.9459\n",
      "Train ALL Pearson: 0.9596742639494666\n",
      "Train  ALL Spearman: 0.9470950097598881\n",
      "31.785366773605347\n",
      "Test Epoch:200 [(0%)]\t Loss: 0.1779  Pearson:0.9464 Spearman:0.9338\n",
      "Test : Loss:0.1823 \n",
      "pearson： 0.9369504534148786 ALL Pearson: 0.9314208524011688\n",
      "spearman： 0.9258001507335613 ALL Spearman: 0.9189752879079869\n",
      "time: 36.454869747161865\n",
      "0.009\n",
      "Train Epoch:201 [(0%)]\t Loss: 0.1152  Pearson:0.9660 Spearman:0.9582\n",
      "Train ALL Pearson: 0.9624838054928201\n",
      "Train  ALL Spearman: 0.9522775110936033\n",
      "31.692911386489868\n",
      "Test Epoch:201 [(0%)]\t Loss: 0.1943  Pearson:0.9492 Spearman:0.9482\n",
      "Test : Loss:0.1901 \n",
      "pearson： 0.9339269219123035 ALL Pearson: 0.9317297558696102\n",
      "spearman： 0.926020172748006 ALL Spearman: 0.9197668113120728\n",
      "time: 36.22817325592041\n",
      "0.009\n",
      "Train Epoch:202 [(0%)]\t Loss: 0.1102  Pearson:0.9618 Spearman:0.9621\n",
      "Train ALL Pearson: 0.9632257946045736\n",
      "Train  ALL Spearman: 0.9539453694618225\n",
      "32.17106342315674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:202 [(0%)]\t Loss: 0.1727  Pearson:0.9218 Spearman:0.9210\n",
      "Test : Loss:0.1676 \n",
      "pearson： 0.9294762267746343 ALL Pearson: 0.9322588775046856\n",
      "spearman： 0.9146008050990801 ALL Spearman: 0.9196207512805598\n",
      "time: 36.92853760719299\n",
      "0.009\n",
      "Train Epoch:203 [(0%)]\t Loss: 0.1084  Pearson:0.9695 Spearman:0.9445\n",
      "Train ALL Pearson: 0.9642351197809848\n",
      "Train  ALL Spearman: 0.9548927795714668\n",
      "31.376333713531494\n",
      "Test Epoch:203 [(0%)]\t Loss: 0.1630  Pearson:0.9327 Spearman:0.9129\n",
      "Test : Loss:0.1605 \n",
      "pearson： 0.9355149889684647 ALL Pearson: 0.9325203039577981\n",
      "spearman： 0.9202528404523995 ALL Spearman: 0.9199335555838241\n",
      "time: 36.19639539718628\n",
      "0.009\n",
      "Train Epoch:204 [(0%)]\t Loss: 0.1314  Pearson:0.9633 Spearman:0.9504\n",
      "Train ALL Pearson: 0.9627498573403585\n",
      "Train  ALL Spearman: 0.9521009950599568\n",
      "31.80580449104309\n",
      "Test Epoch:204 [(0%)]\t Loss: 0.1972  Pearson:0.9290 Spearman:0.9253\n",
      "Test : Loss:0.1971 \n",
      "pearson： 0.9333145173722016 ALL Pearson: 0.9323339500239319\n",
      "spearman： 0.9219525191356542 ALL Spearman: 0.9204471191786467\n",
      "time: 36.51693558692932\n",
      "0.009\n",
      "Train Epoch:205 [(0%)]\t Loss: 0.1161  Pearson:0.9583 Spearman:0.9486\n",
      "Train ALL Pearson: 0.9649699086987967\n",
      "Train  ALL Spearman: 0.9560805354786853\n",
      "31.261730194091797\n",
      "Test Epoch:205 [(0%)]\t Loss: 0.1840  Pearson:0.9513 Spearman:0.9478\n",
      "Test : Loss:0.2086 \n",
      "pearson： 0.9347108169262787 ALL Pearson: 0.9318847516995358\n",
      "spearman： 0.9243030416072308 ALL Spearman: 0.9199918289510443\n",
      "time: 35.993213176727295\n",
      "0.009\n",
      "Train Epoch:206 [(0%)]\t Loss: 0.1092  Pearson:0.9598 Spearman:0.9467\n",
      "Train ALL Pearson: 0.9615007733352816\n",
      "Train  ALL Spearman: 0.9516288419034261\n",
      "32.08943748474121\n",
      "Test Epoch:206 [(0%)]\t Loss: 0.2515  Pearson:0.9312 Spearman:0.9068\n",
      "Test : Loss:0.2458 \n",
      "pearson： 0.9322887193952517 ALL Pearson: 0.9301497785283019\n",
      "spearman： 0.9174570404170584 ALL Spearman: 0.9199778997620728\n",
      "time: 36.99686408042908\n",
      "0.009\n",
      "Train Epoch:207 [(0%)]\t Loss: 0.1441  Pearson:0.9687 Spearman:0.9664\n",
      "Train ALL Pearson: 0.9641664849720193\n",
      "Train  ALL Spearman: 0.9543218460132739\n",
      "31.82235598564148\n",
      "Test Epoch:207 [(0%)]\t Loss: 0.1706  Pearson:0.9257 Spearman:0.9121\n",
      "Test : Loss:0.1863 \n",
      "pearson： 0.9310799579688254 ALL Pearson: 0.9309297607008844\n",
      "spearman： 0.9187517758736942 ALL Spearman: 0.9195884459069679\n",
      "time: 36.50204396247864\n",
      "0.009\n",
      "Train Epoch:208 [(0%)]\t Loss: 0.1227  Pearson:0.9619 Spearman:0.9568\n",
      "Train ALL Pearson: 0.9617727798273927\n",
      "Train  ALL Spearman: 0.9517546017245498\n",
      "32.02455759048462\n",
      "Test Epoch:208 [(0%)]\t Loss: 0.1548  Pearson:0.9371 Spearman:0.9325\n",
      "Test : Loss:0.1578 \n",
      "pearson： 0.9333105785248077 ALL Pearson: 0.9324718312945048\n",
      "spearman： 0.9246878397148908 ALL Spearman: 0.9194850781755711\n",
      "time: 36.91798973083496\n",
      "0.009\n",
      "Train Epoch:209 [(0%)]\t Loss: 0.1250  Pearson:0.9597 Spearman:0.9523\n",
      "Train ALL Pearson: 0.9631281365196133\n",
      "Train  ALL Spearman: 0.9538609368652974\n",
      "31.70941996574402\n",
      "Test Epoch:209 [(0%)]\t Loss: 0.1907  Pearson:0.9309 Spearman:0.9350\n",
      "Test : Loss:0.1896 \n",
      "pearson： 0.9334068276653912 ALL Pearson: 0.9323664106123588\n",
      "spearman： 0.9245304940674522 ALL Spearman: 0.9206086456547407\n",
      "time: 36.64283990859985\n",
      "0.009\n",
      "Train Epoch:210 [(0%)]\t Loss: 0.1023  Pearson:0.9756 Spearman:0.9700\n",
      "Train ALL Pearson: 0.9592966915588491\n",
      "Train  ALL Spearman: 0.9477915160353954\n",
      "31.81203818321228\n",
      "Test Epoch:210 [(0%)]\t Loss: 0.2114  Pearson:0.9394 Spearman:0.9183\n",
      "Test : Loss:0.2154 \n",
      "pearson： 0.9308227619573572 ALL Pearson: 0.9310282902817519\n",
      "spearman： 0.9204923141338992 ALL Spearman: 0.9199959446901201\n",
      "time: 36.53952121734619\n",
      "0.009\n",
      "Train Epoch:211 [(0%)]\t Loss: 0.1199  Pearson:0.9655 Spearman:0.9548\n",
      "Train ALL Pearson: 0.9621374042282383\n",
      "Train  ALL Spearman: 0.9523403568773728\n",
      "32.21310114860535\n",
      "Test Epoch:211 [(0%)]\t Loss: 0.1921  Pearson:0.9379 Spearman:0.9367\n",
      "Test : Loss:0.1997 \n",
      "pearson： 0.9349273324953148 ALL Pearson: 0.931365552592087\n",
      "spearman： 0.9240864922541058 ALL Spearman: 0.9195691608272907\n",
      "time: 36.95558285713196\n",
      "0.009\n",
      "Train Epoch:212 [(0%)]\t Loss: 0.1214  Pearson:0.9590 Spearman:0.9412\n",
      "Train ALL Pearson: 0.9626235469945429\n",
      "Train  ALL Spearman: 0.9530587389253501\n",
      "31.865730047225952\n",
      "Test Epoch:212 [(0%)]\t Loss: 0.1593  Pearson:0.9190 Spearman:0.9005\n",
      "Test : Loss:0.1600 \n",
      "pearson： 0.9297723783095198 ALL Pearson: 0.9325648285238692\n",
      "spearman： 0.9169074695416765 ALL Spearman: 0.9196026137413638\n",
      "time: 36.5772168636322\n",
      "0.0026999999999999997\n",
      "Train Epoch:213 [(0%)]\t Loss: 0.1289  Pearson:0.9544 Spearman:0.9406\n",
      "Train ALL Pearson: 0.9650453297909655\n",
      "Train  ALL Spearman: 0.9574111346708465\n",
      "31.804189920425415\n",
      "Test Epoch:213 [(0%)]\t Loss: 0.1772  Pearson:0.9400 Spearman:0.9140\n",
      "Test : Loss:0.1761 \n",
      "pearson： 0.9342361439158509 ALL Pearson: 0.9325620571052687\n",
      "spearman： 0.9204403472380599 ALL Spearman: 0.9198226649980186\n",
      "time: 36.693623065948486\n",
      "0.0026999999999999997\n",
      "Train Epoch:214 [(0%)]\t Loss: 0.1144  Pearson:0.9605 Spearman:0.9549\n",
      "Train ALL Pearson: 0.9645568092923041\n",
      "Train  ALL Spearman: 0.9560781246046296\n",
      "31.605067014694214\n",
      "Test Epoch:214 [(0%)]\t Loss: 0.1751  Pearson:0.9425 Spearman:0.9086\n",
      "Test : Loss:0.1819 \n",
      "pearson： 0.9336584814571651 ALL Pearson: 0.9323315140836079\n",
      "spearman： 0.917556780188468 ALL Spearman: 0.9197494117255998\n",
      "time: 36.087629318237305\n",
      "0.0026999999999999997\n",
      "Train Epoch:215 [(0%)]\t Loss: 0.1106  Pearson:0.9707 Spearman:0.9600\n",
      "Train ALL Pearson: 0.9648767588566055\n",
      "Train  ALL Spearman: 0.9557660543319297\n",
      "31.77867865562439\n",
      "Test Epoch:215 [(0%)]\t Loss: 0.1729  Pearson:0.9362 Spearman:0.9261\n",
      "Test : Loss:0.1815 \n",
      "pearson： 0.9328362212236828 ALL Pearson: 0.9321551205771905\n",
      "spearman： 0.9162652676431225 ALL Spearman: 0.9198259828361234\n",
      "time: 36.630122661590576\n",
      "0.0026999999999999997\n",
      "Train Epoch:216 [(0%)]\t Loss: 0.1172  Pearson:0.9501 Spearman:0.9459\n",
      "Train ALL Pearson: 0.9642495798654904\n",
      "Train  ALL Spearman: 0.9564686717142232\n",
      "31.650642156600952\n",
      "Test Epoch:216 [(0%)]\t Loss: 0.1822  Pearson:0.9280 Spearman:0.9230\n",
      "Test : Loss:0.1827 \n",
      "pearson： 0.9316156533095633 ALL Pearson: 0.9321637942396236\n",
      "spearman： 0.9225482763642929 ALL Spearman: 0.919943299680671\n",
      "time: 36.23317074775696\n",
      "0.0026999999999999997\n",
      "Train Epoch:217 [(0%)]\t Loss: 0.1264  Pearson:0.9321 Spearman:0.9371\n",
      "Train ALL Pearson: 0.9652550933648055\n",
      "Train  ALL Spearman: 0.95729098877991\n",
      "31.931315898895264\n",
      "Test Epoch:217 [(0%)]\t Loss: 0.1612  Pearson:0.9386 Spearman:0.9214\n",
      "Test : Loss:0.1702 \n",
      "pearson： 0.9327799628701883 ALL Pearson: 0.932560504138192\n",
      "spearman： 0.9218992092395886 ALL Spearman: 0.9195814413274201\n",
      "time: 36.75177025794983\n",
      "0.0026999999999999997\n",
      "Train Epoch:218 [(0%)]\t Loss: 0.1081  Pearson:0.9605 Spearman:0.9364\n",
      "Train ALL Pearson: 0.9643255261735036\n",
      "Train  ALL Spearman: 0.9552892111927496\n",
      "31.213812351226807\n",
      "Test Epoch:218 [(0%)]\t Loss: 0.1626  Pearson:0.9435 Spearman:0.9333\n",
      "Test : Loss:0.1734 \n",
      "pearson： 0.9358750830833847 ALL Pearson: 0.9326443049901872\n",
      "spearman： 0.9238979694084232 ALL Spearman: 0.9200860202029758\n",
      "time: 36.04826259613037\n",
      "0.0026999999999999997\n",
      "Train Epoch:219 [(0%)]\t Loss: 0.0975  Pearson:0.9760 Spearman:0.9659\n",
      "Train ALL Pearson: 0.9649109166317826\n",
      "Train  ALL Spearman: 0.9563131874259205\n",
      "31.509957790374756\n",
      "Test Epoch:219 [(0%)]\t Loss: 0.2010  Pearson:0.9228 Spearman:0.9034\n",
      "Test : Loss:0.1756 \n",
      "pearson： 0.9297344883426195 ALL Pearson: 0.9323514838571316\n",
      "spearman： 0.9157198239006703 ALL Spearman: 0.9198054970456615\n",
      "time: 36.17046284675598\n",
      "0.0026999999999999997\n",
      "Train Epoch:220 [(0%)]\t Loss: 0.0971  Pearson:0.9715 Spearman:0.9566\n",
      "Train ALL Pearson: 0.9641572481199587\n",
      "Train  ALL Spearman: 0.9550386759443632\n",
      "31.431257009506226\n",
      "Test Epoch:220 [(0%)]\t Loss: 0.1665  Pearson:0.9436 Spearman:0.9338\n",
      "Test : Loss:0.1706 \n",
      "pearson： 0.9351358166451306 ALL Pearson: 0.9327270553571334\n",
      "spearman： 0.9231265300438651 ALL Spearman: 0.9197805936059645\n",
      "time: 36.195730209350586\n",
      "0.0026999999999999997\n",
      "Train Epoch:221 [(0%)]\t Loss: 0.1011  Pearson:0.9607 Spearman:0.9321\n",
      "Train ALL Pearson: 0.9653232173751758\n",
      "Train  ALL Spearman: 0.957584331756732\n",
      "32.040122985839844\n",
      "Test Epoch:221 [(0%)]\t Loss: 0.1523  Pearson:0.9434 Spearman:0.9276\n",
      "Test : Loss:0.1746 \n",
      "pearson： 0.9362103569712296 ALL Pearson: 0.9329260443782904\n",
      "spearman： 0.9223035883476591 ALL Spearman: 0.9199301501050868\n",
      "time: 36.64964461326599\n",
      "0.0026999999999999997\n",
      "Train Epoch:222 [(0%)]\t Loss: 0.1022  Pearson:0.9644 Spearman:0.9363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson: 0.96672915168617\n",
      "Train  ALL Spearman: 0.9585146881685845\n",
      "31.806347131729126\n",
      "Test Epoch:222 [(0%)]\t Loss: 0.1831  Pearson:0.9259 Spearman:0.9122\n",
      "Test : Loss:0.1733 \n",
      "pearson： 0.9322848371263519 ALL Pearson: 0.932691281798736\n",
      "spearman： 0.9175535734344499 ALL Spearman: 0.9199522774986083\n",
      "time: 36.523815631866455\n",
      "0.0026999999999999997\n",
      "Train Epoch:223 [(0%)]\t Loss: 0.1210  Pearson:0.9630 Spearman:0.9385\n",
      "Train ALL Pearson: 0.965638692161774\n",
      "Train  ALL Spearman: 0.9570407435837833\n",
      "31.65994644165039\n",
      "Test Epoch:223 [(0%)]\t Loss: 0.1688  Pearson:0.9237 Spearman:0.9049\n",
      "Test : Loss:0.1729 \n",
      "pearson： 0.9293564340054133 ALL Pearson: 0.9326849517715313\n",
      "spearman： 0.9152964969655781 ALL Spearman: 0.9199511228863828\n",
      "time: 36.19049429893494\n",
      "0.0026999999999999997\n",
      "Train Epoch:224 [(0%)]\t Loss: 0.1090  Pearson:0.9665 Spearman:0.9523\n",
      "Train ALL Pearson: 0.9651237789391227\n",
      "Train  ALL Spearman: 0.9562592340411104\n",
      "32.00686836242676\n",
      "Test Epoch:224 [(0%)]\t Loss: 0.1633  Pearson:0.9383 Spearman:0.9201\n",
      "Test : Loss:0.1700 \n",
      "pearson： 0.9335619736961527 ALL Pearson: 0.9327138711198799\n",
      "spearman： 0.9212816001001916 ALL Spearman: 0.9200160815799457\n",
      "time: 36.48517394065857\n",
      "0.0026999999999999997\n",
      "Train Epoch:225 [(0%)]\t Loss: 0.1001  Pearson:0.9737 Spearman:0.9611\n",
      "Train ALL Pearson: 0.9660689350982702\n",
      "Train  ALL Spearman: 0.9581646856257439\n",
      "31.87633228302002\n",
      "Test Epoch:225 [(0%)]\t Loss: 0.1695  Pearson:0.9281 Spearman:0.9106\n",
      "Test : Loss:0.1790 \n",
      "pearson： 0.9307179990098735 ALL Pearson: 0.9326314865141128\n",
      "spearman： 0.9168998762578443 ALL Spearman: 0.9205000347207708\n",
      "time: 36.78731608390808\n",
      "0.0026999999999999997\n",
      "Train Epoch:226 [(0%)]\t Loss: 0.1050  Pearson:0.9715 Spearman:0.9592\n",
      "Train ALL Pearson: 0.9655365238911048\n",
      "Train  ALL Spearman: 0.9574724627163228\n",
      "31.73385214805603\n",
      "Test Epoch:226 [(0%)]\t Loss: 0.1722  Pearson:0.9424 Spearman:0.9340\n",
      "Test : Loss:0.1743 \n",
      "pearson： 0.9352995631596727 ALL Pearson: 0.9326904737773558\n",
      "spearman： 0.9252490024108426 ALL Spearman: 0.9202191630018272\n",
      "time: 36.524070501327515\n",
      "0.0026999999999999997\n",
      "Train Epoch:227 [(0%)]\t Loss: 0.1105  Pearson:0.9573 Spearman:0.9411\n",
      "Train ALL Pearson: 0.9650737896044507\n",
      "Train  ALL Spearman: 0.9571020368766858\n",
      "31.991344451904297\n",
      "Test Epoch:227 [(0%)]\t Loss: 0.1729  Pearson:0.9493 Spearman:0.9415\n",
      "Test : Loss:0.1763 \n",
      "pearson： 0.9358549774816882 ALL Pearson: 0.9326880787062153\n",
      "spearman： 0.9252386991512002 ALL Spearman: 0.9203475896597799\n",
      "time: 36.87463569641113\n",
      "0.0026999999999999997\n",
      "Train Epoch:228 [(0%)]\t Loss: 0.1095  Pearson:0.9655 Spearman:0.9564\n",
      "Train ALL Pearson: 0.9662031445464002\n",
      "Train  ALL Spearman: 0.9580966967279704\n",
      "31.550124645233154\n",
      "Test Epoch:228 [(0%)]\t Loss: 0.1618  Pearson:0.9515 Spearman:0.9261\n",
      "Test : Loss:0.1749 \n",
      "pearson： 0.9353995152277129 ALL Pearson: 0.9326136135831878\n",
      "spearman： 0.9181013270843643 ALL Spearman: 0.9205647823096202\n",
      "time: 36.24861764907837\n",
      "0.0026999999999999997\n",
      "Train Epoch:229 [(0%)]\t Loss: 0.1045  Pearson:0.9692 Spearman:0.9707\n",
      "Train ALL Pearson: 0.9659729730326383\n",
      "Train  ALL Spearman: 0.9578722994932474\n",
      "31.503295421600342\n",
      "Test Epoch:229 [(0%)]\t Loss: 0.1938  Pearson:0.9311 Spearman:0.9122\n",
      "Test : Loss:0.1775 \n",
      "pearson： 0.9351617813228479 ALL Pearson: 0.9328307920963893\n",
      "spearman： 0.9211884340297637 ALL Spearman: 0.9205598962806844\n",
      "time: 36.17679572105408\n",
      "0.0026999999999999997\n",
      "Train Epoch:230 [(0%)]\t Loss: 0.0960  Pearson:0.9694 Spearman:0.9606\n",
      "Train ALL Pearson: 0.9660436942690612\n",
      "Train  ALL Spearman: 0.9586835543270207\n",
      "31.89834213256836\n",
      "Test Epoch:230 [(0%)]\t Loss: 0.1900  Pearson:0.9256 Spearman:0.9202\n",
      "Test : Loss:0.1782 \n",
      "pearson： 0.930083795334273 ALL Pearson: 0.9325952317754741\n",
      "spearman： 0.9215385820526074 ALL Spearman: 0.9202269390197199\n",
      "time: 36.79331564903259\n",
      "0.0026999999999999997\n",
      "Train Epoch:231 [(0%)]\t Loss: 0.1011  Pearson:0.9728 Spearman:0.9713\n",
      "Train ALL Pearson: 0.9659042582579909\n",
      "Train  ALL Spearman: 0.9575444229152736\n",
      "31.714386224746704\n",
      "Test Epoch:231 [(0%)]\t Loss: 0.1713  Pearson:0.9363 Spearman:0.9353\n",
      "Test : Loss:0.1800 \n",
      "pearson： 0.9331283466581575 ALL Pearson: 0.9328847531394219\n",
      "spearman： 0.9226236164864868 ALL Spearman: 0.9201866974963333\n",
      "time: 36.29191732406616\n",
      "0.0026999999999999997\n",
      "Train Epoch:232 [(0%)]\t Loss: 0.1175  Pearson:0.9631 Spearman:0.9496\n",
      "Train ALL Pearson: 0.9660554177536853\n",
      "Train  ALL Spearman: 0.9579941710224389\n",
      "31.934294939041138\n",
      "Test Epoch:232 [(0%)]\t Loss: 0.1844  Pearson:0.9301 Spearman:0.9178\n",
      "Test : Loss:0.1758 \n",
      "pearson： 0.934370851554919 ALL Pearson: 0.932625066968019\n",
      "spearman： 0.9208661183368733 ALL Spearman: 0.9201341487455478\n",
      "time: 36.696768045425415\n",
      "0.0026999999999999997\n",
      "Train Epoch:233 [(0%)]\t Loss: 0.1036  Pearson:0.9679 Spearman:0.9657\n",
      "Train ALL Pearson: 0.9650142913637869\n",
      "Train  ALL Spearman: 0.9567526010597867\n",
      "31.737898111343384\n",
      "Test Epoch:233 [(0%)]\t Loss: 0.1772  Pearson:0.9431 Spearman:0.9427\n",
      "Test : Loss:0.1805 \n",
      "pearson： 0.9368671976715689 ALL Pearson: 0.9328242631158875\n",
      "spearman： 0.9285740710781286 ALL Spearman: 0.9201676554409616\n",
      "time: 36.3914053440094\n",
      "0.0008099999999999998\n",
      "Train Epoch:234 [(0%)]\t Loss: 0.1344  Pearson:0.9606 Spearman:0.9470\n",
      "Train ALL Pearson: 0.9643585436706398\n",
      "Train  ALL Spearman: 0.9548402448098909\n",
      "31.931482315063477\n",
      "Test Epoch:234 [(0%)]\t Loss: 0.1746  Pearson:0.9325 Spearman:0.9233\n",
      "Test : Loss:0.1778 \n",
      "pearson： 0.931316640782682 ALL Pearson: 0.9326535220378809\n",
      "spearman： 0.9219626328585075 ALL Spearman: 0.9198282895869728\n",
      "time: 36.421576738357544\n",
      "0.0008099999999999998\n",
      "Train Epoch:235 [(0%)]\t Loss: 0.1135  Pearson:0.9563 Spearman:0.9475\n",
      "Train ALL Pearson: 0.9646120259105573\n",
      "Train  ALL Spearman: 0.9562806728094595\n",
      "31.751898765563965\n",
      "Test Epoch:235 [(0%)]\t Loss: 0.1782  Pearson:0.9364 Spearman:0.9228\n",
      "Test : Loss:0.1759 \n",
      "pearson： 0.934426764650426 ALL Pearson: 0.9324834031760719\n",
      "spearman： 0.9220176432837491 ALL Spearman: 0.9198956905743694\n",
      "time: 36.611340045928955\n",
      "0.0008099999999999998\n",
      "Train Epoch:236 [(0%)]\t Loss: 0.1156  Pearson:0.9595 Spearman:0.9535\n",
      "Train ALL Pearson: 0.9653021644520668\n",
      "Train  ALL Spearman: 0.9571385685784425\n",
      "31.934104681015015\n",
      "Test Epoch:236 [(0%)]\t Loss: 0.1864  Pearson:0.9244 Spearman:0.8924\n",
      "Test : Loss:0.1801 \n",
      "pearson： 0.92726703434987 ALL Pearson: 0.9324605975906295\n",
      "spearman： 0.9125076354058266 ALL Spearman: 0.9199622115438443\n",
      "time: 36.66758680343628\n",
      "0.0008099999999999998\n",
      "Train Epoch:237 [(0%)]\t Loss: 0.1175  Pearson:0.9597 Spearman:0.9585\n",
      "Train ALL Pearson: 0.9642377610583873\n",
      "Train  ALL Spearman: 0.9556949317041269\n",
      "32.07419729232788\n",
      "Test Epoch:237 [(0%)]\t Loss: 0.1924  Pearson:0.9142 Spearman:0.8999\n",
      "Test : Loss:0.1771 \n",
      "pearson： 0.9279082924426945 ALL Pearson: 0.9327437771039229\n",
      "spearman： 0.9140279544599547 ALL Spearman: 0.9199356988266691\n",
      "time: 36.96263027191162\n",
      "0.0008099999999999998\n",
      "Train Epoch:238 [(0%)]\t Loss: 0.1276  Pearson:0.9587 Spearman:0.9529\n",
      "Train ALL Pearson: 0.9641601526497\n",
      "Train  ALL Spearman: 0.9552816590286134\n",
      "31.602492332458496\n",
      "Test Epoch:238 [(0%)]\t Loss: 0.1883  Pearson:0.9414 Spearman:0.9388\n",
      "Test : Loss:0.1759 \n",
      "pearson： 0.9338596355510613 ALL Pearson: 0.9324727225426789\n",
      "spearman： 0.9231594444465823 ALL Spearman: 0.9200151863049826\n",
      "time: 36.49643683433533\n",
      "0.0008099999999999998\n",
      "Train Epoch:239 [(0%)]\t Loss: 0.1035  Pearson:0.9706 Spearman:0.9521\n",
      "Train ALL Pearson: 0.9646553415383663\n",
      "Train  ALL Spearman: 0.9556987690063343\n",
      "31.37825584411621\n",
      "Test Epoch:239 [(0%)]\t Loss: 0.2032  Pearson:0.9334 Spearman:0.9258\n",
      "Test : Loss:0.1739 \n",
      "pearson： 0.9325871097142865 ALL Pearson: 0.9326809995440375\n",
      "spearman： 0.9200578115461259 ALL Spearman: 0.9198936033932232\n",
      "time: 36.00477147102356\n",
      "0.0008099999999999998\n",
      "Train Epoch:240 [(0%)]\t Loss: 0.1043  Pearson:0.9733 Spearman:0.9611\n",
      "Train ALL Pearson: 0.9648007864487538\n",
      "Train  ALL Spearman: 0.9561783460307446\n",
      "31.367950201034546\n",
      "Test Epoch:240 [(0%)]\t Loss: 0.1636  Pearson:0.9432 Spearman:0.9207\n",
      "Test : Loss:0.1774 \n",
      "pearson： 0.933966901478642 ALL Pearson: 0.9325620588318032\n",
      "spearman： 0.9198659052440529 ALL Spearman: 0.9199700946783984\n",
      "time: 36.14641809463501\n",
      "0.0008099999999999998\n",
      "Train Epoch:241 [(0%)]\t Loss: 0.1120  Pearson:0.9675 Spearman:0.9470\n",
      "Train ALL Pearson: 0.9647926406611543\n",
      "Train  ALL Spearman: 0.9566415988329363\n",
      "31.601274967193604\n",
      "Test Epoch:241 [(0%)]\t Loss: 0.1897  Pearson:0.9367 Spearman:0.9218\n",
      "Test : Loss:0.1781 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9346675177685673 ALL Pearson: 0.9326508872126874\n",
      "spearman： 0.9203910373876263 ALL Spearman: 0.9200362720772368\n",
      "time: 36.341755390167236\n",
      "0.0008099999999999998\n",
      "Train Epoch:242 [(0%)]\t Loss: 0.1056  Pearson:0.9706 Spearman:0.9576\n",
      "Train ALL Pearson: 0.9647773656148539\n",
      "Train  ALL Spearman: 0.9559541742998312\n",
      "31.88076877593994\n",
      "Test Epoch:242 [(0%)]\t Loss: 0.1735  Pearson:0.9420 Spearman:0.9288\n",
      "Test : Loss:0.1748 \n",
      "pearson： 0.9332089739661377 ALL Pearson: 0.9327896624364169\n",
      "spearman： 0.919670803635539 ALL Spearman: 0.9199815235399637\n",
      "time: 36.52827787399292\n",
      "0.0008099999999999998\n",
      "Train Epoch:243 [(0%)]\t Loss: 0.1104  Pearson:0.9631 Spearman:0.9554\n",
      "Train ALL Pearson: 0.9653482439959719\n",
      "Train  ALL Spearman: 0.9562761303967151\n",
      "31.910856246948242\n",
      "Test Epoch:243 [(0%)]\t Loss: 0.1801  Pearson:0.9368 Spearman:0.9132\n",
      "Test : Loss:0.1775 \n",
      "pearson： 0.9352900372925816 ALL Pearson: 0.9326160950344357\n",
      "spearman： 0.9217669531570579 ALL Spearman: 0.9199780289994143\n",
      "time: 36.50638246536255\n",
      "0.0008099999999999998\n",
      "Train Epoch:244 [(0%)]\t Loss: 0.1158  Pearson:0.9654 Spearman:0.9546\n",
      "Train ALL Pearson: 0.9650848825287147\n",
      "Train  ALL Spearman: 0.956721990114106\n",
      "31.62540078163147\n",
      "Test Epoch:244 [(0%)]\t Loss: 0.1768  Pearson:0.9305 Spearman:0.9155\n",
      "Test : Loss:0.1737 \n",
      "pearson： 0.9313434192680692 ALL Pearson: 0.9325843028294496\n",
      "spearman： 0.9181561934309105 ALL Spearman: 0.9200112947249437\n",
      "time: 36.21192955970764\n",
      "0.0008099999999999998\n",
      "Train Epoch:245 [(0%)]\t Loss: 0.1071  Pearson:0.9618 Spearman:0.9361\n",
      "Train ALL Pearson: 0.9647632206528955\n",
      "Train  ALL Spearman: 0.9559863307235027\n",
      "31.30108118057251\n",
      "Test Epoch:245 [(0%)]\t Loss: 0.1748  Pearson:0.9285 Spearman:0.9145\n",
      "Test : Loss:0.1741 \n",
      "pearson： 0.9320877592190766 ALL Pearson: 0.932589449635772\n",
      "spearman： 0.9198903108006956 ALL Spearman: 0.9199926668047241\n",
      "time: 36.00157403945923\n",
      "0.0008099999999999998\n",
      "Train Epoch:246 [(0%)]\t Loss: 0.1079  Pearson:0.9593 Spearman:0.9397\n",
      "Train ALL Pearson: 0.965463630693375\n",
      "Train  ALL Spearman: 0.9569752268537617\n",
      "31.87160873413086\n",
      "Test Epoch:246 [(0%)]\t Loss: 0.1759  Pearson:0.9426 Spearman:0.9210\n",
      "Test : Loss:0.1780 \n",
      "pearson： 0.9321532591035155 ALL Pearson: 0.9325492226597764\n",
      "spearman： 0.9172310841124347 ALL Spearman: 0.9200971154608921\n",
      "time: 36.552109241485596\n",
      "0.0008099999999999998\n",
      "Train Epoch:247 [(0%)]\t Loss: 0.1059  Pearson:0.9675 Spearman:0.9543\n",
      "Train ALL Pearson: 0.9653881337852943\n",
      "Train  ALL Spearman: 0.956601052608883\n",
      "31.673426866531372\n",
      "Test Epoch:247 [(0%)]\t Loss: 0.1858  Pearson:0.9307 Spearman:0.9234\n",
      "Test : Loss:0.1797 \n",
      "pearson： 0.9314129245167422 ALL Pearson: 0.9326733237728105\n",
      "spearman： 0.9190963130221079 ALL Spearman: 0.9202660403191703\n",
      "time: 36.398911237716675\n",
      "0.0008099999999999998\n",
      "Train Epoch:248 [(0%)]\t Loss: 0.1128  Pearson:0.9659 Spearman:0.9593\n",
      "Train ALL Pearson: 0.9647145748458771\n",
      "Train  ALL Spearman: 0.9563519774502869\n",
      "31.77689218521118\n",
      "Test Epoch:248 [(0%)]\t Loss: 0.1981  Pearson:0.9240 Spearman:0.9127\n",
      "Test : Loss:0.1757 \n",
      "pearson： 0.9301635767215377 ALL Pearson: 0.9326003218271249\n",
      "spearman： 0.9202680181399848 ALL Spearman: 0.9201561599661159\n",
      "time: 36.300440549850464\n",
      "0.0008099999999999998\n",
      "Train Epoch:249 [(0%)]\t Loss: 0.1111  Pearson:0.9525 Spearman:0.9522\n",
      "Train ALL Pearson: 0.9654698051806719\n",
      "Train  ALL Spearman: 0.9578554803018481\n",
      "31.77169179916382\n",
      "Test Epoch:249 [(0%)]\t Loss: 0.1896  Pearson:0.9134 Spearman:0.8974\n",
      "Test : Loss:0.1758 \n",
      "pearson： 0.9266386828790029 ALL Pearson: 0.9326331164405525\n",
      "spearman： 0.9112149611615028 ALL Spearman: 0.9202261443119247\n",
      "time: 36.48502159118652\n",
      "0.0008099999999999998\n",
      "Train Epoch:250 [(0%)]\t Loss: 0.1122  Pearson:0.9469 Spearman:0.9243\n",
      "Train ALL Pearson: 0.9650726858611988\n",
      "Train  ALL Spearman: 0.9566190829234628\n",
      "31.646399974822998\n",
      "Test Epoch:250 [(0%)]\t Loss: 0.1765  Pearson:0.9356 Spearman:0.9172\n",
      "Test : Loss:0.1737 \n",
      "pearson： 0.9318764276335118 ALL Pearson: 0.9327020543301364\n",
      "spearman： 0.9181898464383771 ALL Spearman: 0.9200420964977124\n",
      "time: 36.2700617313385\n",
      "0.0008099999999999998\n",
      "Train Epoch:251 [(0%)]\t Loss: 0.1100  Pearson:0.9669 Spearman:0.9484\n",
      "Train ALL Pearson: 0.9656912157538035\n",
      "Train  ALL Spearman: 0.9576436247874056\n",
      "32.20114326477051\n",
      "Test Epoch:251 [(0%)]\t Loss: 0.1664  Pearson:0.9312 Spearman:0.9219\n",
      "Test : Loss:0.1771 \n",
      "pearson： 0.9366900546734992 ALL Pearson: 0.9326726890192253\n",
      "spearman： 0.923779729426536 ALL Spearman: 0.9200839701068885\n",
      "time: 37.172550678253174\n",
      "0.0008099999999999998\n",
      "Train Epoch:252 [(0%)]\t Loss: 0.1098  Pearson:0.9711 Spearman:0.9672\n",
      "Train ALL Pearson: 0.9657905383621088\n",
      "Train  ALL Spearman: 0.9580190330631168\n",
      "31.90436363220215\n",
      "Test Epoch:252 [(0%)]\t Loss: 0.1649  Pearson:0.9364 Spearman:0.9243\n",
      "Test : Loss:0.1739 \n",
      "pearson： 0.9301017414307416 ALL Pearson: 0.9326881063171225\n",
      "spearman： 0.9175927044613036 ALL Spearman: 0.9199931450700493\n",
      "time: 36.496891498565674\n",
      "0.0008099999999999998\n",
      "Train Epoch:253 [(0%)]\t Loss: 0.1097  Pearson:0.9573 Spearman:0.9454\n",
      "Train ALL Pearson: 0.9656968201691184\n",
      "Train  ALL Spearman: 0.9575148504798872\n",
      "32.10045790672302\n",
      "Test Epoch:253 [(0%)]\t Loss: 0.1767  Pearson:0.9289 Spearman:0.9227\n",
      "Test : Loss:0.1777 \n",
      "pearson： 0.9285605080008739 ALL Pearson: 0.9324543472125874\n",
      "spearman： 0.9190618847553708 ALL Spearman: 0.9199518932536238\n",
      "time: 36.786954164505005\n",
      "0.0008099999999999998\n",
      "Train Epoch:254 [(0%)]\t Loss: 0.1075  Pearson:0.9617 Spearman:0.9563\n",
      "Train ALL Pearson: 0.9655827002402876\n",
      "Train  ALL Spearman: 0.9578616640653126\n",
      "32.14365792274475\n",
      "Test Epoch:254 [(0%)]\t Loss: 0.1842  Pearson:0.9518 Spearman:0.9483\n",
      "Test : Loss:0.1763 \n",
      "pearson： 0.9348746742042849 ALL Pearson: 0.932515451735503\n",
      "spearman： 0.9253152314330797 ALL Spearman: 0.9200855112101591\n",
      "time: 36.943718910217285\n",
      "0.00024299999999999994\n",
      "Train Epoch:255 [(0%)]\t Loss: 0.1366  Pearson:0.9638 Spearman:0.9561\n",
      "Train ALL Pearson: 0.9649899239103281\n",
      "Train  ALL Spearman: 0.9547030116974132\n",
      "31.773175477981567\n",
      "Test Epoch:255 [(0%)]\t Loss: 0.1628  Pearson:0.9315 Spearman:0.9246\n",
      "Test : Loss:0.1752 \n",
      "pearson： 0.9343614309244976 ALL Pearson: 0.9329999076787422\n",
      "spearman： 0.9205764013461003 ALL Spearman: 0.9194738597435751\n",
      "time: 36.76930236816406\n",
      "0.00024299999999999994\n",
      "Train Epoch:256 [(0%)]\t Loss: 0.1211  Pearson:0.9572 Spearman:0.9330\n",
      "Train ALL Pearson: 0.9639185260941696\n",
      "Train  ALL Spearman: 0.9533461432564934\n",
      "31.204340934753418\n",
      "Test Epoch:256 [(0%)]\t Loss: 0.1879  Pearson:0.9279 Spearman:0.9114\n",
      "Test : Loss:0.1772 \n",
      "pearson： 0.9305033861921235 ALL Pearson: 0.9328416195097684\n",
      "spearman： 0.9164744342593891 ALL Spearman: 0.9197209992162305\n",
      "time: 36.0447895526886\n",
      "0.00024299999999999994\n",
      "Train Epoch:257 [(0%)]\t Loss: 0.1092  Pearson:0.9710 Spearman:0.9678\n",
      "Train ALL Pearson: 0.9646637169425131\n",
      "Train  ALL Spearman: 0.9552342942449352\n",
      "31.457174062728882\n",
      "Test Epoch:257 [(0%)]\t Loss: 0.1709  Pearson:0.9299 Spearman:0.9289\n",
      "Test : Loss:0.1757 \n",
      "pearson： 0.9335701022202506 ALL Pearson: 0.932722641842956\n",
      "spearman： 0.9236738159229457 ALL Spearman: 0.9197682103332097\n",
      "time: 36.300620317459106\n",
      "0.00024299999999999994\n",
      "Train Epoch:258 [(0%)]\t Loss: 0.1134  Pearson:0.9667 Spearman:0.9524\n",
      "Train ALL Pearson: 0.9644237156938934\n",
      "Train  ALL Spearman: 0.9559856174899884\n",
      "31.92935061454773\n",
      "Test Epoch:258 [(0%)]\t Loss: 0.1846  Pearson:0.9389 Spearman:0.9257\n",
      "Test : Loss:0.1763 \n",
      "pearson： 0.9335734943278264 ALL Pearson: 0.9326829420528941\n",
      "spearman： 0.9218500343001839 ALL Spearman: 0.9198122078167998\n",
      "time: 36.643837451934814\n",
      "0.00024299999999999994\n",
      "Train Epoch:259 [(0%)]\t Loss: 0.1000  Pearson:0.9717 Spearman:0.9552\n",
      "Train ALL Pearson: 0.9640283078633773\n",
      "Train  ALL Spearman: 0.9546437210686654\n",
      "32.0991747379303\n",
      "Test Epoch:259 [(0%)]\t Loss: 0.1763  Pearson:0.9368 Spearman:0.9205\n",
      "Test : Loss:0.1768 \n",
      "pearson： 0.9342401225698357 ALL Pearson: 0.9326023944667716\n",
      "spearman： 0.9213942893181081 ALL Spearman: 0.919820844063808\n",
      "time: 36.9176287651062\n",
      "0.00024299999999999994\n",
      "Train Epoch:260 [(0%)]\t Loss: 0.1199  Pearson:0.9516 Spearman:0.9373\n",
      "Train ALL Pearson: 0.965278963427549\n",
      "Train  ALL Spearman: 0.9565773511660033\n",
      "31.800288200378418\n",
      "Test Epoch:260 [(0%)]\t Loss: 0.1761  Pearson:0.9398 Spearman:0.9316\n",
      "Test : Loss:0.1758 \n",
      "pearson： 0.9350780696714227 ALL Pearson: 0.9326235768622774\n",
      "spearman： 0.9216600424576139 ALL Spearman: 0.9198964508274844\n",
      "time: 36.75298762321472\n",
      "0.00024299999999999994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:261 [(0%)]\t Loss: 0.1056  Pearson:0.9681 Spearman:0.9589\n",
      "Train ALL Pearson: 0.9650710346783878\n",
      "Train  ALL Spearman: 0.957697714921708\n",
      "31.473873376846313\n",
      "Test Epoch:261 [(0%)]\t Loss: 0.1788  Pearson:0.9480 Spearman:0.9343\n",
      "Test : Loss:0.1781 \n",
      "pearson： 0.9361139272818939 ALL Pearson: 0.9326201790939487\n",
      "spearman： 0.922956673556135 ALL Spearman: 0.9199063532204345\n",
      "time: 36.13537883758545\n",
      "0.00024299999999999994\n",
      "Train Epoch:262 [(0%)]\t Loss: 0.1153  Pearson:0.9607 Spearman:0.9562\n",
      "Train ALL Pearson: 0.9647077439672219\n",
      "Train  ALL Spearman: 0.9565575968051726\n",
      "31.614537715911865\n",
      "Test Epoch:262 [(0%)]\t Loss: 0.1724  Pearson:0.9336 Spearman:0.9132\n",
      "Test : Loss:0.1766 \n",
      "pearson： 0.9349322498417916 ALL Pearson: 0.9326352815422904\n",
      "spearman： 0.9197062972362642 ALL Spearman: 0.9199282183537374\n",
      "time: 36.32060718536377\n",
      "0.00024299999999999994\n",
      "Train Epoch:263 [(0%)]\t Loss: 0.1120  Pearson:0.9661 Spearman:0.9432\n",
      "Train ALL Pearson: 0.9644561402003564\n",
      "Train  ALL Spearman: 0.9555181056460138\n",
      "32.10915493965149\n",
      "Test Epoch:263 [(0%)]\t Loss: 0.1764  Pearson:0.9306 Spearman:0.9233\n",
      "Test : Loss:0.1767 \n",
      "pearson： 0.9298294216989956 ALL Pearson: 0.9326344107683653\n",
      "spearman： 0.9170676944561689 ALL Spearman: 0.9199140755777994\n",
      "time: 36.82264494895935\n",
      "0.00024299999999999994\n",
      "Train Epoch:264 [(0%)]\t Loss: 0.1043  Pearson:0.9638 Spearman:0.9548\n",
      "Train ALL Pearson: 0.9650836851109046\n",
      "Train  ALL Spearman: 0.9568459071062402\n",
      "31.398309469223022\n",
      "Test Epoch:264 [(0%)]\t Loss: 0.1786  Pearson:0.9193 Spearman:0.9138\n",
      "Test : Loss:0.1771 \n",
      "pearson： 0.9283164467818404 ALL Pearson: 0.932640268217827\n",
      "spearman： 0.9141383468892988 ALL Spearman: 0.9199238639875877\n",
      "time: 36.20676803588867\n",
      "0.00024299999999999994\n",
      "Train Epoch:265 [(0%)]\t Loss: 0.1172  Pearson:0.9592 Spearman:0.9435\n",
      "Train ALL Pearson: 0.964383885941177\n",
      "Train  ALL Spearman: 0.9545580582936652\n",
      "31.837105751037598\n",
      "Test Epoch:265 [(0%)]\t Loss: 0.1740  Pearson:0.9273 Spearman:0.8971\n",
      "Test : Loss:0.1759 \n",
      "pearson： 0.9313236844676044 ALL Pearson: 0.9326764628133932\n",
      "spearman： 0.9154957721905881 ALL Spearman: 0.9199612793768637\n",
      "time: 36.586536169052124\n",
      "0.00024299999999999994\n",
      "Train Epoch:266 [(0%)]\t Loss: 0.1129  Pearson:0.9628 Spearman:0.9508\n",
      "Train ALL Pearson: 0.9651644448320685\n",
      "Train  ALL Spearman: 0.9563302091853683\n",
      "31.702543258666992\n",
      "Test Epoch:266 [(0%)]\t Loss: 0.1747  Pearson:0.9488 Spearman:0.9338\n",
      "Test : Loss:0.1758 \n",
      "pearson： 0.9356701809350383 ALL Pearson: 0.9325891002903897\n",
      "spearman： 0.9232465744525645 ALL Spearman: 0.9199697081060095\n",
      "time: 36.504005432128906\n",
      "0.00024299999999999994\n",
      "Train Epoch:267 [(0%)]\t Loss: 0.0999  Pearson:0.9663 Spearman:0.9517\n",
      "Train ALL Pearson: 0.9642542044306698\n",
      "Train  ALL Spearman: 0.9558744860510713\n",
      "32.029545545578\n",
      "Test Epoch:267 [(0%)]\t Loss: 0.1613  Pearson:0.9439 Spearman:0.9215\n",
      "Test : Loss:0.1778 \n",
      "pearson： 0.9349794468451311 ALL Pearson: 0.9325631557316763\n",
      "spearman： 0.9192261110027516 ALL Spearman: 0.9199517187600442\n",
      "time: 36.65106439590454\n",
      "0.00024299999999999994\n",
      "Train Epoch:268 [(0%)]\t Loss: 0.1156  Pearson:0.9621 Spearman:0.9479\n",
      "Train ALL Pearson: 0.96472044702311\n",
      "Train  ALL Spearman: 0.9560596293201652\n",
      "31.62841272354126\n",
      "Test Epoch:268 [(0%)]\t Loss: 0.1649  Pearson:0.9256 Spearman:0.9060\n",
      "Test : Loss:0.1761 \n",
      "pearson： 0.9321231121423722 ALL Pearson: 0.9325675542768097\n",
      "spearman： 0.9153258895594962 ALL Spearman: 0.9199384368890293\n",
      "time: 36.51371192932129\n",
      "0.00024299999999999994\n",
      "Train Epoch:269 [(0%)]\t Loss: 0.1013  Pearson:0.9699 Spearman:0.9616\n",
      "Train ALL Pearson: 0.96560265153927\n",
      "Train  ALL Spearman: 0.9572238967438953\n",
      "31.491751194000244\n",
      "Test Epoch:269 [(0%)]\t Loss: 0.1666  Pearson:0.9265 Spearman:0.9235\n",
      "Test : Loss:0.1769 \n",
      "pearson： 0.9337122406861713 ALL Pearson: 0.9325642199874404\n",
      "spearman： 0.9237250077908942 ALL Spearman: 0.9199219975435798\n",
      "time: 36.102272510528564\n",
      "0.00024299999999999994\n",
      "Train Epoch:270 [(0%)]\t Loss: 0.1133  Pearson:0.9735 Spearman:0.9635\n",
      "Train ALL Pearson: 0.9658423307629463\n",
      "Train  ALL Spearman: 0.9584124218763322\n",
      "31.919772148132324\n",
      "Test Epoch:270 [(0%)]\t Loss: 0.1690  Pearson:0.9285 Spearman:0.9091\n",
      "Test : Loss:0.1747 \n",
      "pearson： 0.9301810586671753 ALL Pearson: 0.9326212640086549\n",
      "spearman： 0.911909846587172 ALL Spearman: 0.9199452785907444\n",
      "time: 36.57272911071777\n",
      "0.00024299999999999994\n",
      "Train Epoch:271 [(0%)]\t Loss: 0.0917  Pearson:0.9724 Spearman:0.9619\n",
      "Train ALL Pearson: 0.9651814102465247\n",
      "Train  ALL Spearman: 0.9568382690537973\n",
      "32.03127408027649\n",
      "Test Epoch:271 [(0%)]\t Loss: 0.1806  Pearson:0.9303 Spearman:0.9114\n",
      "Test : Loss:0.1776 \n",
      "pearson： 0.9350338275087658 ALL Pearson: 0.9325611967719419\n",
      "spearman： 0.9195912485502028 ALL Spearman: 0.9199276448824244\n",
      "time: 36.69677686691284\n",
      "0.00024299999999999994\n",
      "Train Epoch:272 [(0%)]\t Loss: 0.1107  Pearson:0.9664 Spearman:0.9594\n",
      "Train ALL Pearson: 0.9653613415450293\n",
      "Train  ALL Spearman: 0.957832699185479\n",
      "32.07596206665039\n",
      "Test Epoch:272 [(0%)]\t Loss: 0.1798  Pearson:0.9281 Spearman:0.9302\n",
      "Test : Loss:0.1804 \n",
      "pearson： 0.9305436167819835 ALL Pearson: 0.93253163407533\n",
      "spearman： 0.9186661943247643 ALL Spearman: 0.9199302795092927\n",
      "time: 36.734007835388184\n",
      "0.00024299999999999994\n",
      "Train Epoch:273 [(0%)]\t Loss: 0.1117  Pearson:0.9657 Spearman:0.9580\n",
      "Train ALL Pearson: 0.9651006867567028\n",
      "Train  ALL Spearman: 0.9565330149270602\n",
      "31.934619665145874\n",
      "Test Epoch:273 [(0%)]\t Loss: 0.1733  Pearson:0.9368 Spearman:0.9174\n",
      "Test : Loss:0.1759 \n",
      "pearson： 0.9311082437530633 ALL Pearson: 0.93255364438889\n",
      "spearman： 0.9174427391268151 ALL Spearman: 0.9199479796175739\n",
      "time: 36.7243812084198\n",
      "0.00024299999999999994\n",
      "Train Epoch:274 [(0%)]\t Loss: 0.1024  Pearson:0.9655 Spearman:0.9552\n",
      "Train ALL Pearson: 0.9638527400631547\n",
      "Train  ALL Spearman: 0.9543385084551106\n",
      "31.97231149673462\n",
      "Test Epoch:274 [(0%)]\t Loss: 0.1667  Pearson:0.9322 Spearman:0.9029\n",
      "Test : Loss:0.1768 \n",
      "pearson： 0.9311665773996441 ALL Pearson: 0.9325429638314523\n",
      "spearman： 0.9172339308907368 ALL Spearman: 0.9199639272338745\n",
      "time: 36.75477719306946\n",
      "0.00024299999999999994\n",
      "Train Epoch:275 [(0%)]\t Loss: 0.1104  Pearson:0.9652 Spearman:0.9498\n",
      "Train ALL Pearson: 0.9649242567680605\n",
      "Train  ALL Spearman: 0.9571451751566555\n",
      "31.86901545524597\n",
      "Test Epoch:275 [(0%)]\t Loss: 0.1907  Pearson:0.9166 Spearman:0.9108\n",
      "Test : Loss:0.1776 \n",
      "pearson： 0.9275607598297047 ALL Pearson: 0.9325527126040303\n",
      "spearman： 0.9142014518522761 ALL Spearman: 0.9200386432976789\n",
      "time: 36.37356996536255\n",
      "7.289999999999998e-05\n",
      "Train Epoch:276 [(0%)]\t Loss: 0.1377  Pearson:0.9705 Spearman:0.9625\n",
      "Train ALL Pearson: 0.9640353463945133\n",
      "Train  ALL Spearman: 0.9536869475565888\n",
      "31.73748230934143\n",
      "Test Epoch:276 [(0%)]\t Loss: 0.1656  Pearson:0.9462 Spearman:0.9267\n",
      "Test : Loss:0.1608 \n",
      "pearson： 0.9316646500950069 ALL Pearson: 0.9333028446578684\n",
      "spearman： 0.9175117643578404 ALL Spearman: 0.9190616909042685\n",
      "time: 36.51249575614929\n",
      "7.289999999999998e-05\n",
      "Train Epoch:277 [(0%)]\t Loss: 0.1275  Pearson:0.9552 Spearman:0.9534\n",
      "Train ALL Pearson: 0.9647733647321937\n",
      "Train  ALL Spearman: 0.9550245269079455\n",
      "31.85401153564453\n",
      "Test Epoch:277 [(0%)]\t Loss: 0.1774  Pearson:0.9225 Spearman:0.9171\n",
      "Test : Loss:0.1668 \n",
      "pearson： 0.930276412429122 ALL Pearson: 0.9331704710933137\n",
      "spearman： 0.9164096107599679 ALL Spearman: 0.9193327096038616\n",
      "time: 36.865405559539795\n",
      "7.289999999999998e-05\n",
      "Train Epoch:278 [(0%)]\t Loss: 0.1191  Pearson:0.9644 Spearman:0.9561\n",
      "Train ALL Pearson: 0.9644667668441026\n",
      "Train  ALL Spearman: 0.9548060474036161\n",
      "31.817050218582153\n",
      "Test Epoch:278 [(0%)]\t Loss: 0.1782  Pearson:0.9260 Spearman:0.9028\n",
      "Test : Loss:0.1718 \n",
      "pearson： 0.932416706637513 ALL Pearson: 0.93306089326874\n",
      "spearman： 0.9166835105647336 ALL Spearman: 0.9194712401224372\n",
      "time: 36.44456601142883\n",
      "7.289999999999998e-05\n",
      "Train Epoch:279 [(0%)]\t Loss: 0.1219  Pearson:0.9671 Spearman:0.9211\n",
      "Train ALL Pearson: 0.9653919009489476\n",
      "Train  ALL Spearman: 0.9563233966113004\n",
      "31.548307418823242\n",
      "Test Epoch:279 [(0%)]\t Loss: 0.1743  Pearson:0.9426 Spearman:0.9142\n",
      "Test : Loss:0.1747 \n",
      "pearson： 0.9342011493053555 ALL Pearson: 0.932957459766717\n",
      "spearman： 0.9182442120460614 ALL Spearman: 0.9195791528190655\n",
      "time: 36.35876536369324\n",
      "7.289999999999998e-05\n",
      "Train Epoch:280 [(0%)]\t Loss: 0.1001  Pearson:0.9688 Spearman:0.9579\n",
      "Train ALL Pearson: 0.9646031066187065\n",
      "Train  ALL Spearman: 0.9550088636654751\n",
      "31.55902624130249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:280 [(0%)]\t Loss: 0.1864  Pearson:0.9181 Spearman:0.8767\n",
      "Test : Loss:0.1748 \n",
      "pearson： 0.9308617169539826 ALL Pearson: 0.9329035784265671\n",
      "spearman： 0.9130468481140592 ALL Spearman: 0.9196234132090735\n",
      "time: 36.178080797195435\n",
      "7.289999999999998e-05\n",
      "Train Epoch:281 [(0%)]\t Loss: 0.1138  Pearson:0.9638 Spearman:0.9369\n",
      "Train ALL Pearson: 0.9642069441757953\n",
      "Train  ALL Spearman: 0.9548807644118638\n",
      "31.418468952178955\n",
      "Test Epoch:281 [(0%)]\t Loss: 0.1904  Pearson:0.9243 Spearman:0.9182\n",
      "Test : Loss:0.1756 \n",
      "pearson： 0.9331430196531856 ALL Pearson: 0.9328432949063984\n",
      "spearman： 0.9206629565294763 ALL Spearman: 0.919679032497651\n",
      "time: 36.070977210998535\n",
      "7.289999999999998e-05\n",
      "Train Epoch:282 [(0%)]\t Loss: 0.1083  Pearson:0.9729 Spearman:0.9634\n",
      "Train ALL Pearson: 0.9652780606318733\n",
      "Train  ALL Spearman: 0.9565045426962313\n",
      "31.51319456100464\n",
      "Test Epoch:282 [(0%)]\t Loss: 0.1669  Pearson:0.9285 Spearman:0.9070\n",
      "Test : Loss:0.1758 \n",
      "pearson： 0.9332465456960514 ALL Pearson: 0.9328028522457573\n",
      "spearman： 0.9180790898422214 ALL Spearman: 0.9197001339453071\n",
      "time: 36.28066563606262\n",
      "7.289999999999998e-05\n",
      "Train Epoch:283 [(0%)]\t Loss: 0.1122  Pearson:0.9692 Spearman:0.9620\n",
      "Train ALL Pearson: 0.964716605315799\n",
      "Train  ALL Spearman: 0.9557245578573542\n",
      "31.838034868240356\n",
      "Test Epoch:283 [(0%)]\t Loss: 0.1570  Pearson:0.9483 Spearman:0.9306\n",
      "Test : Loss:0.1779 \n",
      "pearson： 0.9386655435225795 ALL Pearson: 0.9327610218724711\n",
      "spearman： 0.9244962696120979 ALL Spearman: 0.919718387203417\n",
      "time: 36.76845455169678\n",
      "7.289999999999998e-05\n",
      "Train Epoch:284 [(0%)]\t Loss: 0.1028  Pearson:0.9750 Spearman:0.9557\n",
      "Train ALL Pearson: 0.9647924980179782\n",
      "Train  ALL Spearman: 0.955951699041878\n",
      "31.54186773300171\n",
      "Test Epoch:284 [(0%)]\t Loss: 0.1858  Pearson:0.9400 Spearman:0.9275\n",
      "Test : Loss:0.1757 \n",
      "pearson： 0.934574627400388 ALL Pearson: 0.9327426611403482\n",
      "spearman： 0.9230628508321281 ALL Spearman: 0.9197183036353765\n",
      "time: 36.50228142738342\n",
      "7.289999999999998e-05\n",
      "Train Epoch:285 [(0%)]\t Loss: 0.1112  Pearson:0.9587 Spearman:0.9532\n",
      "Train ALL Pearson: 0.9636158488407982\n",
      "Train  ALL Spearman: 0.9543752058398913\n",
      "31.296143770217896\n",
      "Test Epoch:285 [(0%)]\t Loss: 0.1868  Pearson:0.9215 Spearman:0.9178\n",
      "Test : Loss:0.1758 \n",
      "pearson： 0.9287507691109294 ALL Pearson: 0.9327215569576776\n",
      "spearman： 0.9164978886854119 ALL Spearman: 0.9197513206619635\n",
      "time: 36.035000801086426\n",
      "7.289999999999998e-05\n",
      "Train Epoch:286 [(0%)]\t Loss: 0.1091  Pearson:0.9625 Spearman:0.9412\n",
      "Train ALL Pearson: 0.9651863561954314\n",
      "Train  ALL Spearman: 0.9567084724680102\n",
      "32.066391468048096\n",
      "Test Epoch:286 [(0%)]\t Loss: 0.1837  Pearson:0.9414 Spearman:0.9333\n",
      "Test : Loss:0.1767 \n",
      "pearson： 0.934998124363855 ALL Pearson: 0.9326934922436082\n",
      "spearman： 0.9211901761365113 ALL Spearman: 0.9197790012417083\n",
      "time: 36.75688695907593\n",
      "7.289999999999998e-05\n",
      "Train Epoch:287 [(0%)]\t Loss: 0.1014  Pearson:0.9692 Spearman:0.9648\n",
      "Train ALL Pearson: 0.9640181767056726\n",
      "Train  ALL Spearman: 0.9551427930014559\n",
      "31.911951303482056\n",
      "Test Epoch:287 [(0%)]\t Loss: 0.1675  Pearson:0.9449 Spearman:0.9345\n",
      "Test : Loss:0.1778 \n",
      "pearson： 0.9318052300512142 ALL Pearson: 0.932679533683993\n",
      "spearman： 0.9206463178127641 ALL Spearman: 0.9198108836753058\n",
      "time: 36.54846429824829\n",
      "7.289999999999998e-05\n",
      "Train Epoch:288 [(0%)]\t Loss: 0.1118  Pearson:0.9653 Spearman:0.9508\n",
      "Train ALL Pearson: 0.9645659572372196\n",
      "Train  ALL Spearman: 0.9557932908679349\n",
      "31.600483417510986\n",
      "Test Epoch:288 [(0%)]\t Loss: 0.1686  Pearson:0.9407 Spearman:0.9365\n",
      "Test : Loss:0.1766 \n",
      "pearson： 0.9349677764271324 ALL Pearson: 0.9326747334184299\n",
      "spearman： 0.9227422207449186 ALL Spearman: 0.9198284389155418\n",
      "time: 36.503910779953\n",
      "7.289999999999998e-05\n",
      "Train Epoch:289 [(0%)]\t Loss: 0.1040  Pearson:0.9649 Spearman:0.9628\n",
      "Train ALL Pearson: 0.9649510669775109\n",
      "Train  ALL Spearman: 0.9566166094544278\n",
      "31.671072721481323\n",
      "Test Epoch:289 [(0%)]\t Loss: 0.1745  Pearson:0.9312 Spearman:0.9252\n",
      "Test : Loss:0.1750 \n",
      "pearson： 0.9315691712998292 ALL Pearson: 0.9326633293012636\n",
      "spearman： 0.9196055617983597 ALL Spearman: 0.9198118964688794\n",
      "time: 36.30835223197937\n",
      "7.289999999999998e-05\n",
      "Train Epoch:290 [(0%)]\t Loss: 0.1179  Pearson:0.9521 Spearman:0.9273\n",
      "Train ALL Pearson: 0.9640752531676677\n",
      "Train  ALL Spearman: 0.9558837963619641\n",
      "31.75602889060974\n",
      "Test Epoch:290 [(0%)]\t Loss: 0.1650  Pearson:0.9360 Spearman:0.9156\n",
      "Test : Loss:0.1762 \n",
      "pearson： 0.9323939986411794 ALL Pearson: 0.9326571898360984\n",
      "spearman： 0.9153501393116437 ALL Spearman: 0.9198119225037613\n",
      "time: 36.33456110954285\n",
      "7.289999999999998e-05\n",
      "Train Epoch:291 [(0%)]\t Loss: 0.1103  Pearson:0.9663 Spearman:0.9649\n",
      "Train ALL Pearson: 0.9653730145909806\n",
      "Train  ALL Spearman: 0.9570283946110779\n",
      "31.755001068115234\n",
      "Test Epoch:291 [(0%)]\t Loss: 0.1645  Pearson:0.9414 Spearman:0.9158\n",
      "Test : Loss:0.1771 \n",
      "pearson： 0.9368813658439245 ALL Pearson: 0.9326386274950058\n",
      "spearman： 0.9182688689477302 ALL Spearman: 0.9198266824777372\n",
      "time: 36.52976989746094\n",
      "7.289999999999998e-05\n",
      "Train Epoch:292 [(0%)]\t Loss: 0.1040  Pearson:0.9698 Spearman:0.9482\n",
      "Train ALL Pearson: 0.9647130101217494\n",
      "Train  ALL Spearman: 0.9559642362375005\n",
      "32.01910352706909\n",
      "Test Epoch:292 [(0%)]\t Loss: 0.1700  Pearson:0.9362 Spearman:0.9276\n",
      "Test : Loss:0.1766 \n",
      "pearson： 0.9331039941993253 ALL Pearson: 0.9326230592239935\n",
      "spearman： 0.9200627012298525 ALL Spearman: 0.9198165662760364\n",
      "time: 36.547651290893555\n",
      "7.289999999999998e-05\n",
      "Train Epoch:293 [(0%)]\t Loss: 0.1217  Pearson:0.9692 Spearman:0.9623\n",
      "Train ALL Pearson: 0.964684069363389\n",
      "Train  ALL Spearman: 0.9564248144958754\n",
      "31.66685938835144\n",
      "Test Epoch:293 [(0%)]\t Loss: 0.1943  Pearson:0.9248 Spearman:0.9004\n",
      "Test : Loss:0.1764 \n",
      "pearson： 0.931650723414895 ALL Pearson: 0.9326191543320914\n",
      "spearman： 0.9144954101627589 ALL Spearman: 0.9198360337672754\n",
      "time: 36.31340146064758\n",
      "7.289999999999998e-05\n",
      "Train Epoch:294 [(0%)]\t Loss: 0.1218  Pearson:0.9538 Spearman:0.9360\n",
      "Train ALL Pearson: 0.9648621378221925\n",
      "Train  ALL Spearman: 0.9565825053526117\n",
      "31.923946619033813\n",
      "Test Epoch:294 [(0%)]\t Loss: 0.1755  Pearson:0.9405 Spearman:0.9368\n",
      "Test : Loss:0.1781 \n",
      "pearson： 0.9361964216360881 ALL Pearson: 0.9326225149011653\n",
      "spearman： 0.9273641250799763 ALL Spearman: 0.9198339231084023\n",
      "time: 36.73340344429016\n",
      "7.289999999999998e-05\n",
      "Train Epoch:295 [(0%)]\t Loss: 0.1070  Pearson:0.9668 Spearman:0.9605\n",
      "Train ALL Pearson: 0.9650921836387263\n",
      "Train  ALL Spearman: 0.9568987759381696\n",
      "31.61163330078125\n",
      "Test Epoch:295 [(0%)]\t Loss: 0.1708  Pearson:0.9488 Spearman:0.9228\n",
      "Test : Loss:0.1771 \n",
      "pearson： 0.9342923229038476 ALL Pearson: 0.9326264624186678\n",
      "spearman： 0.9187995878726783 ALL Spearman: 0.9198322326451277\n",
      "time: 36.528626918792725\n",
      "7.289999999999998e-05\n",
      "Train Epoch:296 [(0%)]\t Loss: 0.1232  Pearson:0.9594 Spearman:0.9385\n",
      "Train ALL Pearson: 0.9648787288696312\n",
      "Train  ALL Spearman: 0.9564034548127945\n",
      "31.769527196884155\n",
      "Test Epoch:296 [(0%)]\t Loss: 0.1698  Pearson:0.9431 Spearman:0.9273\n",
      "Test : Loss:0.1774 \n",
      "pearson： 0.9350587382380248 ALL Pearson: 0.9326418585214745\n",
      "spearman： 0.9234851213410405 ALL Spearman: 0.9198260957761742\n",
      "time: 36.62548780441284\n",
      "Test Epoch:-1 [(0%)]\t Loss: 0.1547  Pearson:0.9215 Spearman:0.8975\n",
      "Test : Loss:0.1552 \n",
      "pearson： 0.9331340902129978 ALL Pearson: 0.9333994268588421\n",
      "spearman： 0.913772876116353 ALL Spearman: 0.9185469034467663\n",
      "PLCC: [0.9390283689521908, 0.9380595102191237, 0.9333994268588421] SRCC: [0.9231706745873007, 0.9249109784651682, 0.9185469034467663]\n",
      "Split: 2 Median PLCC: 0.9380595102191237 SRCC: 0.9231706745873007\n",
      "Test Epoch:-1 [(0%)]\t Loss: 3.3078  Pearson:0.0090 Spearman:0.0523\n",
      "Test : Loss:3.3429 \n",
      "pearson： -0.007708619021577965 ALL Pearson: -0.014646854752189183\n",
      "spearman： 0.01816599537863866 ALL Spearman: 0.00546300833518343\n",
      "0.01\n",
      "Train Epoch:0 [(0%)]\t Loss: 3.3700  Pearson:-0.0972 Spearman:-0.0949\n",
      "Train ALL Pearson: -0.025956043200040833\n",
      "Train  ALL Spearman: -0.01817015321002587\n",
      "19.60505175590515\n",
      "Test Epoch:0 [(0%)]\t Loss: 1.7164  Pearson:-0.2463 Spearman:-0.1266\n",
      "Test : Loss:1.6933 \n",
      "pearson： -0.22224824912852228 ALL Pearson: -0.22391199710191434\n",
      "spearman： -0.16243478724020388 ALL Spearman: -0.1734171440359106\n",
      "time: 24.35852575302124\n",
      "0.01\n",
      "Train Epoch:1 [(0%)]\t Loss: 1.5232  Pearson:-0.1239 Spearman:-0.1250\n",
      "Train ALL Pearson: 0.05636762561219657\n",
      "Train  ALL Spearman: 0.05346886507012466\n",
      "19.98030376434326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:1 [(0%)]\t Loss: 0.6107  Pearson:0.4626 Spearman:0.4572\n",
      "Test : Loss:0.6268 \n",
      "pearson： 0.4818328341560843 ALL Pearson: 0.46055601628219417\n",
      "spearman： 0.4637789063128413 ALL Spearman: 0.439230381262191\n",
      "time: 24.814752340316772\n",
      "0.01\n",
      "Train Epoch:2 [(0%)]\t Loss: 0.5620  Pearson:0.2069 Spearman:0.1748\n",
      "Train ALL Pearson: 0.3889062811383713\n",
      "Train  ALL Spearman: 0.36315655500150623\n",
      "19.885923862457275\n",
      "Test Epoch:2 [(0%)]\t Loss: 0.5940  Pearson:0.7143 Spearman:0.6843\n",
      "Test : Loss:0.6054 \n",
      "pearson： 0.6462118405003155 ALL Pearson: 0.6207121497223749\n",
      "spearman： 0.6118696539762177 ALL Spearman: 0.5924385912020234\n",
      "time: 24.586198329925537\n",
      "0.01\n",
      "Train Epoch:3 [(0%)]\t Loss: 0.3842  Pearson:0.5633 Spearman:0.5500\n",
      "Train ALL Pearson: 0.5040782945363423\n",
      "Train  ALL Spearman: 0.4763592889859521\n",
      "20.102488040924072\n",
      "Test Epoch:3 [(0%)]\t Loss: 0.5490  Pearson:0.7116 Spearman:0.7237\n",
      "Test : Loss:0.5671 \n",
      "pearson： 0.672016677484003 ALL Pearson: 0.6716450790640656\n",
      "spearman： 0.6462706052140016 ALL Spearman: 0.6411977765407156\n",
      "time: 24.816977739334106\n",
      "0.01\n",
      "Train Epoch:4 [(0%)]\t Loss: 0.4353  Pearson:0.5596 Spearman:0.5371\n",
      "Train ALL Pearson: 0.5368744732485016\n",
      "Train  ALL Spearman: 0.5061457623482901\n",
      "20.082561254501343\n",
      "Test Epoch:4 [(0%)]\t Loss: 0.5836  Pearson:0.6774 Spearman:0.6768\n",
      "Test : Loss:0.6066 \n",
      "pearson： 0.6997066572226246 ALL Pearson: 0.7028781038221796\n",
      "spearman： 0.677738563790058 ALL Spearman: 0.6743502732138474\n",
      "time: 24.684086322784424\n",
      "0.01\n",
      "Train Epoch:5 [(0%)]\t Loss: 0.3974  Pearson:0.6417 Spearman:0.5928\n",
      "Train ALL Pearson: 0.5708499592852505\n",
      "Train  ALL Spearman: 0.5358006079124717\n",
      "19.809473514556885\n",
      "Test Epoch:5 [(0%)]\t Loss: 0.5942  Pearson:0.7349 Spearman:0.6915\n",
      "Test : Loss:0.6137 \n",
      "pearson： 0.7194774832912993 ALL Pearson: 0.7203224822348903\n",
      "spearman： 0.6875068343063282 ALL Spearman: 0.6954255884253792\n",
      "time: 24.691158056259155\n",
      "0.01\n",
      "Train Epoch:6 [(0%)]\t Loss: 0.4250  Pearson:0.5416 Spearman:0.5219\n",
      "Train ALL Pearson: 0.5981288486971668\n",
      "Train  ALL Spearman: 0.5673666952830194\n",
      "19.92869257926941\n",
      "Test Epoch:6 [(0%)]\t Loss: 0.5933  Pearson:0.7267 Spearman:0.6880\n",
      "Test : Loss:0.5853 \n",
      "pearson： 0.7284682885028886 ALL Pearson: 0.7310184296072574\n",
      "spearman： 0.6999424261996496 ALL Spearman: 0.7079354179217094\n",
      "time: 24.668171882629395\n",
      "0.01\n",
      "Train Epoch:7 [(0%)]\t Loss: 0.3445  Pearson:0.6356 Spearman:0.5986\n",
      "Train ALL Pearson: 0.6161643481424836\n",
      "Train  ALL Spearman: 0.5797598462223462\n",
      "20.05869221687317\n",
      "Test Epoch:7 [(0%)]\t Loss: 0.5669  Pearson:0.7103 Spearman:0.7083\n",
      "Test : Loss:0.5747 \n",
      "pearson： 0.7438571125750475 ALL Pearson: 0.7395389038316321\n",
      "spearman： 0.7292885201304101 ALL Spearman: 0.7167328029727111\n",
      "time: 24.860153675079346\n",
      "0.01\n",
      "Train Epoch:8 [(0%)]\t Loss: 0.3491  Pearson:0.7129 Spearman:0.6855\n",
      "Train ALL Pearson: 0.6293545417327989\n",
      "Train  ALL Spearman: 0.5998447248629537\n",
      "20.14521074295044\n",
      "Test Epoch:8 [(0%)]\t Loss: 0.5622  Pearson:0.6904 Spearman:0.6794\n",
      "Test : Loss:0.5515 \n",
      "pearson： 0.7372303995468035 ALL Pearson: 0.7420562133046339\n",
      "spearman： 0.715491463786064 ALL Spearman: 0.7184139116210096\n",
      "time: 24.81971001625061\n",
      "0.01\n",
      "Train Epoch:9 [(0%)]\t Loss: 0.3537  Pearson:0.6265 Spearman:0.6122\n",
      "Train ALL Pearson: 0.6318318782927193\n",
      "Train  ALL Spearman: 0.5964263151259208\n",
      "19.94860553741455\n",
      "Test Epoch:9 [(0%)]\t Loss: 0.5179  Pearson:0.7784 Spearman:0.7469\n",
      "Test : Loss:0.5399 \n",
      "pearson： 0.752659706137753 ALL Pearson: 0.7456467268013791\n",
      "spearman： 0.7288315171134658 ALL Spearman: 0.7224999136221953\n",
      "time: 24.852354764938354\n",
      "0.01\n",
      "Train Epoch:10 [(0%)]\t Loss: 0.3602  Pearson:0.6457 Spearman:0.5960\n",
      "Train ALL Pearson: 0.6355295824487004\n",
      "Train  ALL Spearman: 0.6016140972225598\n",
      "19.850205183029175\n",
      "Test Epoch:10 [(0%)]\t Loss: 0.5652  Pearson:0.7672 Spearman:0.7142\n",
      "Test : Loss:0.5612 \n",
      "pearson： 0.7516123626799935 ALL Pearson: 0.7481377013258483\n",
      "spearman： 0.7157962273155273 ALL Spearman: 0.7262616934408943\n",
      "time: 24.574261903762817\n",
      "0.001\n",
      "Train Epoch:11 [(0%)]\t Loss: 0.3631  Pearson:0.6238 Spearman:0.5574\n",
      "Train ALL Pearson: 0.6558131862573301\n",
      "Train  ALL Spearman: 0.625442948060771\n",
      "31.941251516342163\n",
      "Test Epoch:11 [(0%)]\t Loss: 0.5412  Pearson:0.7723 Spearman:0.7399\n",
      "Test : Loss:0.5491 \n",
      "pearson： 0.7598495705401955 ALL Pearson: 0.7573004862599831\n",
      "spearman： 0.7386233422940239 ALL Spearman: 0.7375644673985295\n",
      "time: 36.706629276275635\n",
      "0.001\n",
      "Train Epoch:12 [(0%)]\t Loss: 0.3083  Pearson:0.6979 Spearman:0.6655\n",
      "Train ALL Pearson: 0.6614487994654019\n",
      "Train  ALL Spearman: 0.6279230717840644\n",
      "31.826355695724487\n",
      "Test Epoch:12 [(0%)]\t Loss: 0.5496  Pearson:0.7331 Spearman:0.7107\n",
      "Test : Loss:0.5521 \n",
      "pearson： 0.7658101067733111 ALL Pearson: 0.7647371044349813\n",
      "spearman： 0.7422215807755012 ALL Spearman: 0.7466866736399908\n",
      "time: 36.66680312156677\n",
      "0.001\n",
      "Train Epoch:13 [(0%)]\t Loss: 0.3600  Pearson:0.6550 Spearman:0.6244\n",
      "Train ALL Pearson: 0.6782373795999027\n",
      "Train  ALL Spearman: 0.6516387719760947\n",
      "31.624842643737793\n",
      "Test Epoch:13 [(0%)]\t Loss: 0.5374  Pearson:0.7727 Spearman:0.7456\n",
      "Test : Loss:0.5352 \n",
      "pearson： 0.7696194240753205 ALL Pearson: 0.7714271265525323\n",
      "spearman： 0.7535284094542744 ALL Spearman: 0.7536408825784254\n",
      "time: 36.406312227249146\n",
      "0.001\n",
      "Train Epoch:14 [(0%)]\t Loss: 0.3530  Pearson:0.5950 Spearman:0.5872\n",
      "Train ALL Pearson: 0.684742464017508\n",
      "Train  ALL Spearman: 0.660333612983416\n",
      "31.498112440109253\n",
      "Test Epoch:14 [(0%)]\t Loss: 0.5264  Pearson:0.7634 Spearman:0.7345\n",
      "Test : Loss:0.5424 \n",
      "pearson： 0.7827052451517855 ALL Pearson: 0.7788882709792405\n",
      "spearman： 0.7653509527752417 ALL Spearman: 0.7629873957543578\n",
      "time: 36.12962746620178\n",
      "0.001\n",
      "Train Epoch:15 [(0%)]\t Loss: 0.3230  Pearson:0.5976 Spearman:0.5717\n",
      "Train ALL Pearson: 0.6934051672208857\n",
      "Train  ALL Spearman: 0.6606027425916459\n",
      "31.316641092300415\n",
      "Test Epoch:15 [(0%)]\t Loss: 0.5461  Pearson:0.8139 Spearman:0.8374\n",
      "Test : Loss:0.5313 \n",
      "pearson： 0.790913480480508 ALL Pearson: 0.785146487999211\n",
      "spearman： 0.7848368914857636 ALL Spearman: 0.7705695152238968\n",
      "time: 35.89517307281494\n",
      "0.001\n",
      "Train Epoch:16 [(0%)]\t Loss: 0.3148  Pearson:0.6385 Spearman:0.6582\n",
      "Train ALL Pearson: 0.700874451752139\n",
      "Train  ALL Spearman: 0.670515744140228\n",
      "32.01933407783508\n",
      "Test Epoch:16 [(0%)]\t Loss: 0.5264  Pearson:0.8212 Spearman:0.7999\n",
      "Test : Loss:0.5255 \n",
      "pearson： 0.7897364270521795 ALL Pearson: 0.7891487243510671\n",
      "spearman： 0.7718582404082796 ALL Spearman: 0.7744549196863625\n",
      "time: 36.75381517410278\n",
      "0.001\n",
      "Train Epoch:17 [(0%)]\t Loss: 0.3120  Pearson:0.7367 Spearman:0.7097\n",
      "Train ALL Pearson: 0.7084894823801248\n",
      "Train  ALL Spearman: 0.6828042140434556\n",
      "32.10382390022278\n",
      "Test Epoch:17 [(0%)]\t Loss: 0.5127  Pearson:0.7994 Spearman:0.7728\n",
      "Test : Loss:0.5272 \n",
      "pearson： 0.797813784737685 ALL Pearson: 0.79436554160836\n",
      "spearman： 0.7753134871290072 ALL Spearman: 0.78009289950864\n",
      "time: 36.94327187538147\n",
      "0.001\n",
      "Train Epoch:18 [(0%)]\t Loss: 0.3229  Pearson:0.7228 Spearman:0.6411\n",
      "Train ALL Pearson: 0.7149602074665579\n",
      "Train  ALL Spearman: 0.6856883772486798\n",
      "31.629359245300293\n",
      "Test Epoch:18 [(0%)]\t Loss: 0.5161  Pearson:0.8304 Spearman:0.8300\n",
      "Test : Loss:0.5267 \n",
      "pearson： 0.8017666147856545 ALL Pearson: 0.7989102346790509\n",
      "spearman： 0.7846949212202639 ALL Spearman: 0.7849343425724526\n",
      "time: 36.5367865562439\n",
      "0.001\n",
      "Train Epoch:19 [(0%)]\t Loss: 0.3155  Pearson:0.7210 Spearman:0.7237\n",
      "Train ALL Pearson: 0.716908161155154\n",
      "Train  ALL Spearman: 0.6932783060359108\n",
      "31.71794629096985\n",
      "Test Epoch:19 [(0%)]\t Loss: 0.5033  Pearson:0.7848 Spearman:0.7773\n",
      "Test : Loss:0.5339 \n",
      "pearson： 0.8046205506744869 ALL Pearson: 0.8036924490840376\n",
      "spearman： 0.788137105814486 ALL Spearman: 0.7898277438254442\n",
      "time: 36.3264684677124\n",
      "0.001\n",
      "Train Epoch:20 [(0%)]\t Loss: 0.2746  Pearson:0.7638 Spearman:0.7357\n",
      "Train ALL Pearson: 0.7269911086105809\n",
      "Train  ALL Spearman: 0.701146805017214\n",
      "31.900885105133057\n",
      "Test Epoch:20 [(0%)]\t Loss: 0.4973  Pearson:0.8183 Spearman:0.8219\n",
      "Test : Loss:0.5138 \n",
      "pearson： 0.8090811184071609 ALL Pearson: 0.8076752769569718\n",
      "spearman： 0.8022357515267179 ALL Spearman: 0.7929386658889868\n",
      "time: 36.795315980911255\n",
      "0.005\n",
      "Train Epoch:21 [(0%)]\t Loss: 0.2905  Pearson:0.7399 Spearman:0.6933\n",
      "Train ALL Pearson: 0.7344293808552974\n",
      "Train  ALL Spearman: 0.7020047509689652\n",
      "31.535035610198975\n",
      "Test Epoch:21 [(0%)]\t Loss: 0.4761  Pearson:0.8406 Spearman:0.8123\n",
      "Test : Loss:0.5041 \n",
      "pearson： 0.8275293793169226 ALL Pearson: 0.8260563059878591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.8102571830562559 ALL Spearman: 0.8108565465169068\n",
      "time: 36.153554916381836\n",
      "0.005\n",
      "Train Epoch:22 [(0%)]\t Loss: 0.3164  Pearson:0.7687 Spearman:0.7411\n",
      "Train ALL Pearson: 0.7431878919949456\n",
      "Train  ALL Spearman: 0.718889807327371\n",
      "31.686975717544556\n",
      "Test Epoch:22 [(0%)]\t Loss: 0.5004  Pearson:0.8006 Spearman:0.7367\n",
      "Test : Loss:0.4945 \n",
      "pearson： 0.8348262783601433 ALL Pearson: 0.8377236135340641\n",
      "spearman： 0.8118729961035732 ALL Spearman: 0.8222816349129473\n",
      "time: 36.30949258804321\n",
      "0.005\n",
      "Train Epoch:23 [(0%)]\t Loss: 0.2967  Pearson:0.7743 Spearman:0.7596\n",
      "Train ALL Pearson: 0.7715763046603579\n",
      "Train  ALL Spearman: 0.7469931617569575\n",
      "31.883683443069458\n",
      "Test Epoch:23 [(0%)]\t Loss: 0.4435  Pearson:0.8512 Spearman:0.8433\n",
      "Test : Loss:0.4423 \n",
      "pearson： 0.8517454051565797 ALL Pearson: 0.8482357954746097\n",
      "spearman： 0.8390443043006706 ALL Spearman: 0.8333590059992766\n",
      "time: 36.876357316970825\n",
      "0.005\n",
      "Train Epoch:24 [(0%)]\t Loss: 0.2833  Pearson:0.7859 Spearman:0.7667\n",
      "Train ALL Pearson: 0.7859971968946802\n",
      "Train  ALL Spearman: 0.7601072216595435\n",
      "31.794808864593506\n",
      "Test Epoch:24 [(0%)]\t Loss: 0.4263  Pearson:0.8784 Spearman:0.8493\n",
      "Test : Loss:0.4360 \n",
      "pearson： 0.8644667122878746 ALL Pearson: 0.858652000368501\n",
      "spearman： 0.8488601704294049 ALL Spearman: 0.8448277304670152\n",
      "time: 36.41585731506348\n",
      "0.005\n",
      "Train Epoch:25 [(0%)]\t Loss: 0.2660  Pearson:0.7788 Spearman:0.7104\n",
      "Train ALL Pearson: 0.7990051103265151\n",
      "Train  ALL Spearman: 0.7710216828843472\n",
      "31.910088062286377\n",
      "Test Epoch:25 [(0%)]\t Loss: 0.4701  Pearson:0.8536 Spearman:0.8378\n",
      "Test : Loss:0.4530 \n",
      "pearson： 0.8555036167024965 ALL Pearson: 0.8651993732435002\n",
      "spearman： 0.8467301882645181 ALL Spearman: 0.8510993045190008\n",
      "time: 36.82351326942444\n",
      "0.005\n",
      "Train Epoch:26 [(0%)]\t Loss: 0.2668  Pearson:0.8132 Spearman:0.7981\n",
      "Train ALL Pearson: 0.806779675797338\n",
      "Train  ALL Spearman: 0.7795964819474164\n",
      "31.716151237487793\n",
      "Test Epoch:26 [(0%)]\t Loss: 0.4208  Pearson:0.8864 Spearman:0.8791\n",
      "Test : Loss:0.4292 \n",
      "pearson： 0.8782488278531354 ALL Pearson: 0.8732323046331522\n",
      "spearman： 0.865428931520042 ALL Spearman: 0.8581229890161123\n",
      "time: 36.43763852119446\n",
      "0.005\n",
      "Train Epoch:27 [(0%)]\t Loss: 0.2514  Pearson:0.8442 Spearman:0.7691\n",
      "Train ALL Pearson: 0.815896007211805\n",
      "Train  ALL Spearman: 0.7866291087789836\n",
      "31.628535509109497\n",
      "Test Epoch:27 [(0%)]\t Loss: 0.4085  Pearson:0.8747 Spearman:0.8393\n",
      "Test : Loss:0.4096 \n",
      "pearson： 0.8774319519093811 ALL Pearson: 0.8798005215659215\n",
      "spearman： 0.8614329609232478 ALL Spearman: 0.8647960271362019\n",
      "time: 36.34792160987854\n",
      "0.005\n",
      "Train Epoch:28 [(0%)]\t Loss: 0.2207  Pearson:0.8515 Spearman:0.8220\n",
      "Train ALL Pearson: 0.8253257953812277\n",
      "Train  ALL Spearman: 0.7964019532866556\n",
      "31.607465028762817\n",
      "Test Epoch:28 [(0%)]\t Loss: 0.3326  Pearson:0.8765 Spearman:0.8250\n",
      "Test : Loss:0.3625 \n",
      "pearson： 0.8854968001879538 ALL Pearson: 0.8858298764876461\n",
      "spearman： 0.8635263627192767 ALL Spearman: 0.8691997843546915\n",
      "time: 36.59097075462341\n",
      "0.005\n",
      "Train Epoch:29 [(0%)]\t Loss: 0.2050  Pearson:0.8793 Spearman:0.8254\n",
      "Train ALL Pearson: 0.8377822866587644\n",
      "Train  ALL Spearman: 0.8065276313558527\n",
      "31.501728534698486\n",
      "Test Epoch:29 [(0%)]\t Loss: 0.3680  Pearson:0.9178 Spearman:0.9026\n",
      "Test : Loss:0.3706 \n",
      "pearson： 0.898650917222697 ALL Pearson: 0.8905022685332741\n",
      "spearman： 0.8844554430186157 ALL Spearman: 0.874781818627109\n",
      "time: 36.254127740859985\n",
      "0.005\n",
      "Train Epoch:30 [(0%)]\t Loss: 0.2267  Pearson:0.8695 Spearman:0.8443\n",
      "Train ALL Pearson: 0.843015892620474\n",
      "Train  ALL Spearman: 0.8128807323843662\n",
      "32.11848592758179\n",
      "Test Epoch:30 [(0%)]\t Loss: 0.3423  Pearson:0.9072 Spearman:0.8901\n",
      "Test : Loss:0.3300 \n",
      "pearson： 0.8941587452795812 ALL Pearson: 0.895386615937964\n",
      "spearman： 0.8769379258395166 ALL Spearman: 0.8791770225311545\n",
      "time: 36.961490869522095\n",
      "0.03\n",
      "Train Epoch:31 [(0%)]\t Loss: 0.2168  Pearson:0.8672 Spearman:0.8313\n",
      "Train ALL Pearson: 0.5875188362535605\n",
      "Train  ALL Spearman: 0.5760978665136736\n",
      "31.82929515838623\n",
      "Test Epoch:31 [(0%)]\t Loss: 0.2555  Pearson:0.8515 Spearman:0.8077\n",
      "Test : Loss:0.2570 \n",
      "pearson： 0.8634277301707776 ALL Pearson: 0.8572594704728526\n",
      "spearman： 0.8319661931702756 ALL Spearman: 0.832316495089804\n",
      "time: 36.712727069854736\n",
      "0.03\n",
      "Train Epoch:32 [(0%)]\t Loss: 0.3645  Pearson:0.8438 Spearman:0.7957\n",
      "Train ALL Pearson: 0.629493815730789\n",
      "Train  ALL Spearman: 0.6138890519756022\n",
      "31.84728503227234\n",
      "Test Epoch:32 [(0%)]\t Loss: 0.6206  Pearson:0.8560 Spearman:0.8323\n",
      "Test : Loss:0.6092 \n",
      "pearson： 0.86644946371661 ALL Pearson: 0.8636733564292736\n",
      "spearman： 0.8486202662994734 ALL Spearman: 0.8455028748184025\n",
      "time: 36.555774211883545\n",
      "0.03\n",
      "Train Epoch:33 [(0%)]\t Loss: 0.3316  Pearson:0.8170 Spearman:0.7701\n",
      "Train ALL Pearson: 0.76715500231427\n",
      "Train  ALL Spearman: 0.7335851081675447\n",
      "31.93284249305725\n",
      "Test Epoch:33 [(0%)]\t Loss: 0.4971  Pearson:0.8620 Spearman:0.8501\n",
      "Test : Loss:0.4834 \n",
      "pearson： 0.874055923977635 ALL Pearson: 0.8779813264930841\n",
      "spearman： 0.8599967940939198 ALL Spearman: 0.8622547966542675\n",
      "time: 36.74729871749878\n",
      "0.03\n",
      "Train Epoch:34 [(0%)]\t Loss: 0.2482  Pearson:0.8716 Spearman:0.8496\n",
      "Train ALL Pearson: 0.7787128648639547\n",
      "Train  ALL Spearman: 0.7430530199576132\n",
      "31.646610736846924\n",
      "Test Epoch:34 [(0%)]\t Loss: 0.4472  Pearson:0.8732 Spearman:0.8414\n",
      "Test : Loss:0.4387 \n",
      "pearson： 0.8840246582931179 ALL Pearson: 0.8850169724002401\n",
      "spearman： 0.8666371916752699 ALL Spearman: 0.8679852162613158\n",
      "time: 36.58193492889404\n",
      "0.03\n",
      "Train Epoch:35 [(0%)]\t Loss: 0.2214  Pearson:0.8776 Spearman:0.8641\n",
      "Train ALL Pearson: 0.8218690244064951\n",
      "Train  ALL Spearman: 0.7913094654097838\n",
      "31.54115128517151\n",
      "Test Epoch:35 [(0%)]\t Loss: 0.4456  Pearson:0.8378 Spearman:0.8550\n",
      "Test : Loss:0.4274 \n",
      "pearson： 0.8764171036880465 ALL Pearson: 0.8906536274418817\n",
      "spearman： 0.8731884986513467 ALL Spearman: 0.880582770671168\n",
      "time: 36.17197871208191\n",
      "0.03\n",
      "Train Epoch:36 [(0%)]\t Loss: 0.2317  Pearson:0.8514 Spearman:0.8595\n",
      "Train ALL Pearson: 0.8118109714489535\n",
      "Train  ALL Spearman: 0.7774777062168523\n",
      "31.955883502960205\n",
      "Test Epoch:36 [(0%)]\t Loss: 0.4610  Pearson:0.8930 Spearman:0.8653\n",
      "Test : Loss:0.4681 \n",
      "pearson： 0.8932168545696556 ALL Pearson: 0.892848164421009\n",
      "spearman： 0.8828847425909374 ALL Spearman: 0.8824384468957539\n",
      "time: 36.5364146232605\n",
      "0.03\n",
      "Train Epoch:37 [(0%)]\t Loss: 0.2803  Pearson:0.8761 Spearman:0.8825\n",
      "Train ALL Pearson: 0.819051138242056\n",
      "Train  ALL Spearman: 0.7870622707743\n",
      "31.757653951644897\n",
      "Test Epoch:37 [(0%)]\t Loss: 0.5144  Pearson:0.8943 Spearman:0.8849\n",
      "Test : Loss:0.5128 \n",
      "pearson： 0.8901618178822168 ALL Pearson: 0.8924686512509247\n",
      "spearman： 0.8858205674037914 ALL Spearman: 0.8860597473473029\n",
      "time: 36.52812457084656\n",
      "0.03\n",
      "Train Epoch:38 [(0%)]\t Loss: 0.3090  Pearson:0.8844 Spearman:0.8712\n",
      "Train ALL Pearson: 0.8368182317383867\n",
      "Train  ALL Spearman: 0.8052310666272408\n",
      "31.65524673461914\n",
      "Test Epoch:38 [(0%)]\t Loss: 0.3955  Pearson:0.9345 Spearman:0.8963\n",
      "Test : Loss:0.4103 \n",
      "pearson： 0.9078080703083251 ALL Pearson: 0.8976034389846603\n",
      "spearman： 0.8936182815559502 ALL Spearman: 0.8896058351318001\n",
      "time: 36.1668004989624\n",
      "0.03\n",
      "Train Epoch:39 [(0%)]\t Loss: 0.2562  Pearson:0.8779 Spearman:0.8562\n",
      "Train ALL Pearson: 0.8541170246783899\n",
      "Train  ALL Spearman: 0.8273775124376301\n",
      "32.10136842727661\n",
      "Test Epoch:39 [(0%)]\t Loss: 0.1992  Pearson:0.9025 Spearman:0.8961\n",
      "Test : Loss:0.2001 \n",
      "pearson： 0.9062811748942804 ALL Pearson: 0.9072981614470246\n",
      "spearman： 0.8921607811091369 ALL Spearman: 0.8925415197969205\n",
      "time: 36.74808311462402\n",
      "0.03\n",
      "Train Epoch:40 [(0%)]\t Loss: 0.2434  Pearson:0.8534 Spearman:0.7851\n",
      "Train ALL Pearson: 0.8373213674155857\n",
      "Train  ALL Spearman: 0.8113536415981973\n",
      "31.462114095687866\n",
      "Test Epoch:40 [(0%)]\t Loss: 0.2392  Pearson:0.8960 Spearman:0.8816\n",
      "Test : Loss:0.2110 \n",
      "pearson： 0.9041332254537688 ALL Pearson: 0.907334424317033\n",
      "spearman： 0.891377117801223 ALL Spearman: 0.8952950471419672\n",
      "time: 36.07420539855957\n",
      "0.03\n",
      "Train Epoch:41 [(0%)]\t Loss: 0.2196  Pearson:0.8849 Spearman:0.8577\n",
      "Train ALL Pearson: 0.8549413965941943\n",
      "Train  ALL Spearman: 0.8284580222236236\n",
      "31.756381511688232\n",
      "Test Epoch:41 [(0%)]\t Loss: 0.2068  Pearson:0.9057 Spearman:0.8641\n",
      "Test : Loss:0.1904 \n",
      "pearson： 0.9094990252932511 ALL Pearson: 0.9114394444484617\n",
      "spearman： 0.8875283125389912 ALL Spearman: 0.8981951417857659\n",
      "time: 36.36174702644348\n",
      "0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:42 [(0%)]\t Loss: 0.2459  Pearson:0.8928 Spearman:0.8737\n",
      "Train ALL Pearson: 0.8723508479108644\n",
      "Train  ALL Spearman: 0.8437667798219881\n",
      "31.88836169242859\n",
      "Test Epoch:42 [(0%)]\t Loss: 0.2229  Pearson:0.9005 Spearman:0.8824\n",
      "Test : Loss:0.1933 \n",
      "pearson： 0.9125198187086704 ALL Pearson: 0.9149412318065069\n",
      "spearman： 0.8950743242305397 ALL Spearman: 0.8976086767419477\n",
      "time: 36.69231867790222\n",
      "0.03\n",
      "Train Epoch:43 [(0%)]\t Loss: 0.1883  Pearson:0.9077 Spearman:0.8484\n",
      "Train ALL Pearson: 0.8713521177802939\n",
      "Train  ALL Spearman: 0.8377176875802215\n",
      "31.865476608276367\n",
      "Test Epoch:43 [(0%)]\t Loss: 0.1894  Pearson:0.9113 Spearman:0.8961\n",
      "Test : Loss:0.1814 \n",
      "pearson： 0.9096834608126891 ALL Pearson: 0.9152358075812405\n",
      "spearman： 0.8923662116786524 ALL Spearman: 0.8976484051984214\n",
      "time: 36.72291946411133\n",
      "0.03\n",
      "Train Epoch:44 [(0%)]\t Loss: 0.2198  Pearson:0.8896 Spearman:0.8689\n",
      "Train ALL Pearson: 0.8736749817274722\n",
      "Train  ALL Spearman: 0.8374045523339095\n",
      "31.768918752670288\n",
      "Test Epoch:44 [(0%)]\t Loss: 0.3621  Pearson:0.9200 Spearman:0.9144\n",
      "Test : Loss:0.3839 \n",
      "pearson： 0.9178999563041391 ALL Pearson: 0.9157134641089062\n",
      "spearman： 0.9022594048019884 ALL Spearman: 0.9019072286745565\n",
      "time: 36.48540496826172\n",
      "0.03\n",
      "Train Epoch:45 [(0%)]\t Loss: 0.2267  Pearson:0.9107 Spearman:0.8934\n",
      "Train ALL Pearson: 0.8760692110239074\n",
      "Train  ALL Spearman: 0.8412010160786277\n",
      "31.682167768478394\n",
      "Test Epoch:45 [(0%)]\t Loss: 0.3875  Pearson:0.8881 Spearman:0.8481\n",
      "Test : Loss:0.3672 \n",
      "pearson： 0.9132943843165345 ALL Pearson: 0.9183660554548196\n",
      "spearman： 0.8905447737145294 ALL Spearman: 0.9023955332004561\n",
      "time: 36.46822237968445\n",
      "0.03\n",
      "Train Epoch:46 [(0%)]\t Loss: 0.2152  Pearson:0.9384 Spearman:0.9189\n",
      "Train ALL Pearson: 0.8906848344843763\n",
      "Train  ALL Spearman: 0.8577196861777217\n",
      "31.30928325653076\n",
      "Test Epoch:46 [(0%)]\t Loss: 0.1911  Pearson:0.9373 Spearman:0.9165\n",
      "Test : Loss:0.1872 \n",
      "pearson： 0.9241493436197099 ALL Pearson: 0.9209868522639282\n",
      "spearman： 0.9053173737039344 ALL Spearman: 0.9038119615240272\n",
      "time: 36.30289602279663\n",
      "0.03\n",
      "Train Epoch:47 [(0%)]\t Loss: 0.1976  Pearson:0.9060 Spearman:0.8851\n",
      "Train ALL Pearson: 0.8917856880703056\n",
      "Train  ALL Spearman: 0.8603685680974946\n",
      "31.757262229919434\n",
      "Test Epoch:47 [(0%)]\t Loss: 0.3072  Pearson:0.9143 Spearman:0.8973\n",
      "Test : Loss:0.2888 \n",
      "pearson： 0.9178963161035987 ALL Pearson: 0.9222224214396388\n",
      "spearman： 0.89943625188629 ALL Spearman: 0.9072535718548094\n",
      "time: 36.311909914016724\n",
      "0.03\n",
      "Train Epoch:48 [(0%)]\t Loss: 0.1858  Pearson:0.9262 Spearman:0.9172\n",
      "Train ALL Pearson: 0.9053658146944993\n",
      "Train  ALL Spearman: 0.8798111247322936\n",
      "31.65972328186035\n",
      "Test Epoch:48 [(0%)]\t Loss: 0.2986  Pearson:0.9295 Spearman:0.8950\n",
      "Test : Loss:0.2889 \n",
      "pearson： 0.9240942733698958 ALL Pearson: 0.9250333135016827\n",
      "spearman： 0.9072826599464161 ALL Spearman: 0.9094897929723208\n",
      "time: 36.31370186805725\n",
      "0.03\n",
      "Train Epoch:49 [(0%)]\t Loss: 0.1530  Pearson:0.9174 Spearman:0.9076\n",
      "Train ALL Pearson: 0.9009478076077871\n",
      "Train  ALL Spearman: 0.8699656028251952\n",
      "31.74852991104126\n",
      "Test Epoch:49 [(0%)]\t Loss: 0.2262  Pearson:0.9023 Spearman:0.9045\n",
      "Test : Loss:0.2262 \n",
      "pearson： 0.9235259594085469 ALL Pearson: 0.9245866026801174\n",
      "spearman： 0.9113423213468017 ALL Spearman: 0.9086266590823214\n",
      "time: 36.43203020095825\n",
      "0.03\n",
      "Train Epoch:50 [(0%)]\t Loss: 0.1391  Pearson:0.9564 Spearman:0.9547\n",
      "Train ALL Pearson: 0.901982035436964\n",
      "Train  ALL Spearman: 0.8746591440857956\n",
      "32.00889563560486\n",
      "Test Epoch:50 [(0%)]\t Loss: 0.1728  Pearson:0.9161 Spearman:0.9041\n",
      "Test : Loss:0.1708 \n",
      "pearson： 0.9248914215584313 ALL Pearson: 0.9260588946160446\n",
      "spearman： 0.9071282964955184 ALL Spearman: 0.9092962221694355\n",
      "time: 36.69839310646057\n",
      "0.03\n",
      "Train Epoch:51 [(0%)]\t Loss: 0.1754  Pearson:0.9207 Spearman:0.8997\n",
      "Train ALL Pearson: 0.9061742266140447\n",
      "Train  ALL Spearman: 0.8772468724716634\n",
      "31.44440746307373\n",
      "Test Epoch:51 [(0%)]\t Loss: 0.2068  Pearson:0.9338 Spearman:0.9175\n",
      "Test : Loss:0.2096 \n",
      "pearson： 0.9266234377360693 ALL Pearson: 0.9256519989846169\n",
      "spearman： 0.9093456062561345 ALL Spearman: 0.9094373698230592\n",
      "time: 36.21987581253052\n",
      "0.03\n",
      "Train Epoch:52 [(0%)]\t Loss: 0.1473  Pearson:0.9281 Spearman:0.9201\n",
      "Train ALL Pearson: 0.9027599379348716\n",
      "Train  ALL Spearman: 0.8731793296818614\n",
      "31.511026620864868\n",
      "Test Epoch:52 [(0%)]\t Loss: 0.2549  Pearson:0.9280 Spearman:0.8932\n",
      "Test : Loss:0.2661 \n",
      "pearson： 0.9284992382888102 ALL Pearson: 0.9266791143508585\n",
      "spearman： 0.9098227713956465 ALL Spearman: 0.9114055040507402\n",
      "time: 36.323485136032104\n",
      "0.03\n",
      "Train Epoch:53 [(0%)]\t Loss: 0.1488  Pearson:0.9509 Spearman:0.9165\n",
      "Train ALL Pearson: 0.9122222315765659\n",
      "Train  ALL Spearman: 0.8862575336578015\n",
      "31.411486864089966\n",
      "Test Epoch:53 [(0%)]\t Loss: 0.3107  Pearson:0.9098 Spearman:0.8708\n",
      "Test : Loss:0.3192 \n",
      "pearson： 0.9185094236786638 ALL Pearson: 0.9274734245703342\n",
      "spearman： 0.9010716507528678 ALL Spearman: 0.9120739321412407\n",
      "time: 36.10997986793518\n",
      "0.03\n",
      "Train Epoch:54 [(0%)]\t Loss: 0.2021  Pearson:0.9417 Spearman:0.8821\n",
      "Train ALL Pearson: 0.9121830760935681\n",
      "Train  ALL Spearman: 0.887134653241493\n",
      "31.561676263809204\n",
      "Test Epoch:54 [(0%)]\t Loss: 0.3234  Pearson:0.9329 Spearman:0.9173\n",
      "Test : Loss:0.3280 \n",
      "pearson： 0.9332705863351071 ALL Pearson: 0.9264941273468692\n",
      "spearman： 0.919416865763512 ALL Spearman: 0.9114322817912759\n",
      "time: 36.329148292541504\n",
      "0.03\n",
      "Train Epoch:55 [(0%)]\t Loss: 0.2173  Pearson:0.9401 Spearman:0.9275\n",
      "Train ALL Pearson: 0.9094195496548128\n",
      "Train  ALL Spearman: 0.8832964290838511\n",
      "31.61835217475891\n",
      "Test Epoch:55 [(0%)]\t Loss: 0.1816  Pearson:0.9222 Spearman:0.9231\n",
      "Test : Loss:0.1823 \n",
      "pearson： 0.9241736820419778 ALL Pearson: 0.9271146176973912\n",
      "spearman： 0.9136213683031251 ALL Spearman: 0.9116193126855109\n",
      "time: 36.27885675430298\n",
      "0.03\n",
      "Train Epoch:56 [(0%)]\t Loss: 0.1520  Pearson:0.9298 Spearman:0.9051\n",
      "Train ALL Pearson: 0.9137191314448438\n",
      "Train  ALL Spearman: 0.8898456332602134\n",
      "31.614543676376343\n",
      "Test Epoch:56 [(0%)]\t Loss: 0.1767  Pearson:0.9124 Spearman:0.9044\n",
      "Test : Loss:0.1688 \n",
      "pearson： 0.9251832786507802 ALL Pearson: 0.9274370130448748\n",
      "spearman： 0.9110712626000886 ALL Spearman: 0.9111370534066491\n",
      "time: 36.25905466079712\n",
      "0.03\n",
      "Train Epoch:57 [(0%)]\t Loss: 0.1802  Pearson:0.9154 Spearman:0.9065\n",
      "Train ALL Pearson: 0.9268558982121649\n",
      "Train  ALL Spearman: 0.905751594247874\n",
      "31.388649225234985\n",
      "Test Epoch:57 [(0%)]\t Loss: 0.1650  Pearson:0.9390 Spearman:0.9160\n",
      "Test : Loss:0.1687 \n",
      "pearson： 0.9315006622219826 ALL Pearson: 0.9290791633236912\n",
      "spearman： 0.9142823203157768 ALL Spearman: 0.9145736598849008\n",
      "time: 36.36105465888977\n",
      "0.03\n",
      "Train Epoch:58 [(0%)]\t Loss: 0.1605  Pearson:0.9351 Spearman:0.9235\n",
      "Train ALL Pearson: 0.9239287354745744\n",
      "Train  ALL Spearman: 0.900808325941338\n",
      "31.73846673965454\n",
      "Test Epoch:58 [(0%)]\t Loss: 0.2302  Pearson:0.9379 Spearman:0.9071\n",
      "Test : Loss:0.2248 \n",
      "pearson： 0.9304646056557643 ALL Pearson: 0.9289058305963372\n",
      "spearman： 0.9114281592077952 ALL Spearman: 0.9140170146921662\n",
      "time: 36.29900503158569\n",
      "0.03\n",
      "Train Epoch:59 [(0%)]\t Loss: 0.1597  Pearson:0.9209 Spearman:0.9218\n",
      "Train ALL Pearson: 0.9196023301503143\n",
      "Train  ALL Spearman: 0.8975089253121048\n",
      "31.699374437332153\n",
      "Test Epoch:59 [(0%)]\t Loss: 0.2151  Pearson:0.9371 Spearman:0.9123\n",
      "Test : Loss:0.2258 \n",
      "pearson： 0.9366245264626862 ALL Pearson: 0.9309387109125524\n",
      "spearman： 0.9201656894784667 ALL Spearman: 0.9170593782903523\n",
      "time: 36.43585562705994\n",
      "0.03\n",
      "Train Epoch:60 [(0%)]\t Loss: 0.1468  Pearson:0.9213 Spearman:0.9120\n",
      "Train ALL Pearson: 0.9228688212806149\n",
      "Train  ALL Spearman: 0.8999358293328356\n",
      "31.797242403030396\n",
      "Test Epoch:60 [(0%)]\t Loss: 0.1696  Pearson:0.9116 Spearman:0.8912\n",
      "Test : Loss:0.1596 \n",
      "pearson： 0.9252945259690144 ALL Pearson: 0.9309821395691619\n",
      "spearman： 0.9096426479546664 ALL Spearman: 0.9163997686397797\n",
      "time: 36.38542556762695\n",
      "0.03\n",
      "Train Epoch:61 [(0%)]\t Loss: 0.1661  Pearson:0.9380 Spearman:0.9206\n",
      "Train ALL Pearson: 0.9211664371732147\n",
      "Train  ALL Spearman: 0.8989347478335293\n",
      "31.95497441291809\n",
      "Test Epoch:61 [(0%)]\t Loss: 0.2818  Pearson:0.9346 Spearman:0.9167\n",
      "Test : Loss:0.2825 \n",
      "pearson： 0.9301640271439235 ALL Pearson: 0.9298260136635664\n",
      "spearman： 0.9156842108852197 ALL Spearman: 0.9147469403872058\n",
      "time: 36.71944522857666\n",
      "0.03\n",
      "Train Epoch:62 [(0%)]\t Loss: 0.1794  Pearson:0.9327 Spearman:0.9135\n",
      "Train ALL Pearson: 0.9249488874945364\n",
      "Train  ALL Spearman: 0.9053129615920443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.353837490081787\n",
      "Test Epoch:62 [(0%)]\t Loss: 0.1788  Pearson:0.9069 Spearman:0.8971\n",
      "Test : Loss:0.1687 \n",
      "pearson： 0.9263494664737965 ALL Pearson: 0.9308670198798241\n",
      "spearman： 0.9104544019595783 ALL Spearman: 0.9160512736932547\n",
      "time: 36.21927833557129\n",
      "0.03\n",
      "Train Epoch:63 [(0%)]\t Loss: 0.1716  Pearson:0.9077 Spearman:0.8996\n",
      "Train ALL Pearson: 0.9269850041848471\n",
      "Train  ALL Spearman: 0.9076073342437715\n",
      "32.10370445251465\n",
      "Test Epoch:63 [(0%)]\t Loss: 0.2694  Pearson:0.9261 Spearman:0.9025\n",
      "Test : Loss:0.2562 \n",
      "pearson： 0.928702318047272 ALL Pearson: 0.931054082876194\n",
      "spearman： 0.9112984678768431 ALL Spearman: 0.9164871877357077\n",
      "time: 37.02265930175781\n",
      "0.03\n",
      "Train Epoch:64 [(0%)]\t Loss: 0.1610  Pearson:0.9382 Spearman:0.9237\n",
      "Train ALL Pearson: 0.9256130281383297\n",
      "Train  ALL Spearman: 0.904191399870955\n",
      "31.781752109527588\n",
      "Test Epoch:64 [(0%)]\t Loss: 0.2614  Pearson:0.9340 Spearman:0.8924\n",
      "Test : Loss:0.2731 \n",
      "pearson： 0.9268843690735192 ALL Pearson: 0.93135362166048\n",
      "spearman： 0.9113735637686644 ALL Spearman: 0.91841530089771\n",
      "time: 36.720858573913574\n",
      "0.03\n",
      "Train Epoch:65 [(0%)]\t Loss: 0.1928  Pearson:0.9409 Spearman:0.9054\n",
      "Train ALL Pearson: 0.9246348804585721\n",
      "Train  ALL Spearman: 0.9010110412923887\n",
      "31.238961219787598\n",
      "Test Epoch:65 [(0%)]\t Loss: 0.1638  Pearson:0.9335 Spearman:0.9169\n",
      "Test : Loss:0.1605 \n",
      "pearson： 0.9341009607679734 ALL Pearson: 0.9328744602690698\n",
      "spearman： 0.9190500109658436 ALL Spearman: 0.9191774426813937\n",
      "time: 35.98075747489929\n",
      "0.03\n",
      "Train Epoch:66 [(0%)]\t Loss: 0.1643  Pearson:0.9585 Spearman:0.9267\n",
      "Train ALL Pearson: 0.9260477250302422\n",
      "Train  ALL Spearman: 0.9030194997715083\n",
      "32.284194469451904\n",
      "Test Epoch:66 [(0%)]\t Loss: 0.1641  Pearson:0.9459 Spearman:0.9219\n",
      "Test : Loss:0.1637 \n",
      "pearson： 0.9320945414703714 ALL Pearson: 0.9324535350438558\n",
      "spearman： 0.9156893367351777 ALL Spearman: 0.9184583258695379\n",
      "time: 36.91970944404602\n",
      "0.03\n",
      "Train Epoch:67 [(0%)]\t Loss: 0.1526  Pearson:0.9486 Spearman:0.9161\n",
      "Train ALL Pearson: 0.9300921918319254\n",
      "Train  ALL Spearman: 0.9104271461686196\n",
      "31.313826084136963\n",
      "Test Epoch:67 [(0%)]\t Loss: 0.1751  Pearson:0.9304 Spearman:0.9186\n",
      "Test : Loss:0.1592 \n",
      "pearson： 0.9333039904321673 ALL Pearson: 0.9319805531796704\n",
      "spearman： 0.9213671205810962 ALL Spearman: 0.9177327695946135\n",
      "time: 36.025314807891846\n",
      "0.03\n",
      "Train Epoch:68 [(0%)]\t Loss: 0.1636  Pearson:0.9583 Spearman:0.9503\n",
      "Train ALL Pearson: 0.9335132700562926\n",
      "Train  ALL Spearman: 0.9152541127081202\n",
      "31.988312244415283\n",
      "Test Epoch:68 [(0%)]\t Loss: 0.1564  Pearson:0.9286 Spearman:0.9041\n",
      "Test : Loss:0.1624 \n",
      "pearson： 0.9314748277075232 ALL Pearson: 0.9328441268588228\n",
      "spearman： 0.9154274213947297 ALL Spearman: 0.9185991017017706\n",
      "time: 36.70480012893677\n",
      "0.03\n",
      "Train Epoch:69 [(0%)]\t Loss: 0.1480  Pearson:0.9466 Spearman:0.9388\n",
      "Train ALL Pearson: 0.932132414471742\n",
      "Train  ALL Spearman: 0.912227630987172\n",
      "31.994163274765015\n",
      "Test Epoch:69 [(0%)]\t Loss: 0.2412  Pearson:0.9326 Spearman:0.9306\n",
      "Test : Loss:0.2375 \n",
      "pearson： 0.9322885675640106 ALL Pearson: 0.9333384677654025\n",
      "spearman： 0.9235447599302595 ALL Spearman: 0.9192799558049585\n",
      "time: 36.59668684005737\n",
      "0.03\n",
      "Train Epoch:70 [(0%)]\t Loss: 0.1564  Pearson:0.9347 Spearman:0.9343\n",
      "Train ALL Pearson: 0.9296777575979714\n",
      "Train  ALL Spearman: 0.9082807070219765\n",
      "31.823874950408936\n",
      "Test Epoch:70 [(0%)]\t Loss: 0.1945  Pearson:0.9269 Spearman:0.9327\n",
      "Test : Loss:0.2053 \n",
      "pearson： 0.9325508945400771 ALL Pearson: 0.9330980954983713\n",
      "spearman： 0.924721273183857 ALL Spearman: 0.9190083549166022\n",
      "time: 36.4513897895813\n",
      "0.03\n",
      "Train Epoch:71 [(0%)]\t Loss: 0.1288  Pearson:0.9531 Spearman:0.9432\n",
      "Train ALL Pearson: 0.9324270563339289\n",
      "Train  ALL Spearman: 0.9126966988817136\n",
      "32.10295844078064\n",
      "Test Epoch:71 [(0%)]\t Loss: 0.1696  Pearson:0.9259 Spearman:0.9274\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.9298079021532781 ALL Pearson: 0.9332630703733968\n",
      "spearman： 0.9181366100396027 ALL Spearman: 0.9185899410782385\n",
      "time: 36.98139429092407\n",
      "0.03\n",
      "Train Epoch:72 [(0%)]\t Loss: 0.1651  Pearson:0.9540 Spearman:0.9485\n",
      "Train ALL Pearson: 0.9294662107254831\n",
      "Train  ALL Spearman: 0.9088680986455047\n",
      "31.624329090118408\n",
      "Test Epoch:72 [(0%)]\t Loss: 0.1585  Pearson:0.9313 Spearman:0.9087\n",
      "Test : Loss:0.1669 \n",
      "pearson： 0.9308154428219386 ALL Pearson: 0.9325182553917785\n",
      "spearman： 0.9160612956895118 ALL Spearman: 0.9176172878842301\n",
      "time: 36.29083251953125\n",
      "0.03\n",
      "Train Epoch:73 [(0%)]\t Loss: 0.1530  Pearson:0.9298 Spearman:0.9094\n",
      "Train ALL Pearson: 0.9332541187887741\n",
      "Train  ALL Spearman: 0.9143547791844534\n",
      "31.536576986312866\n",
      "Test Epoch:73 [(0%)]\t Loss: 0.1688  Pearson:0.9446 Spearman:0.9194\n",
      "Test : Loss:0.1780 \n",
      "pearson： 0.9383051008317994 ALL Pearson: 0.9347679962089553\n",
      "spearman： 0.9216101401848381 ALL Spearman: 0.9202693175233999\n",
      "time: 36.29504990577698\n",
      "0.03\n",
      "Train Epoch:74 [(0%)]\t Loss: 0.1254  Pearson:0.9636 Spearman:0.9525\n",
      "Train ALL Pearson: 0.9347650419330846\n",
      "Train  ALL Spearman: 0.9146532993436775\n",
      "31.414586067199707\n",
      "Test Epoch:74 [(0%)]\t Loss: 0.1575  Pearson:0.9352 Spearman:0.9287\n",
      "Test : Loss:0.1625 \n",
      "pearson： 0.9333984296026578 ALL Pearson: 0.9341746372828837\n",
      "spearman： 0.919628864559613 ALL Spearman: 0.9191617899401544\n",
      "time: 36.11607789993286\n",
      "0.03\n",
      "Train Epoch:75 [(0%)]\t Loss: 0.1525  Pearson:0.9509 Spearman:0.9440\n",
      "Train ALL Pearson: 0.931523772212819\n",
      "Train  ALL Spearman: 0.9104135722236352\n",
      "32.13166427612305\n",
      "Test Epoch:75 [(0%)]\t Loss: 0.2525  Pearson:0.9194 Spearman:0.9143\n",
      "Test : Loss:0.2683 \n",
      "pearson： 0.9296103004909597 ALL Pearson: 0.9323990464346809\n",
      "spearman： 0.9204799799480288 ALL Spearman: 0.9186337551620671\n",
      "time: 36.879140853881836\n",
      "0.03\n",
      "Train Epoch:76 [(0%)]\t Loss: 0.1612  Pearson:0.9635 Spearman:0.9494\n",
      "Train ALL Pearson: 0.9339614754380743\n",
      "Train  ALL Spearman: 0.9145071524982867\n",
      "31.888845920562744\n",
      "Test Epoch:76 [(0%)]\t Loss: 0.1594  Pearson:0.9346 Spearman:0.9086\n",
      "Test : Loss:0.1590 \n",
      "pearson： 0.9356439155139022 ALL Pearson: 0.932935100313483\n",
      "spearman： 0.9172264141900236 ALL Spearman: 0.9180402952200396\n",
      "time: 36.516361713409424\n",
      "0.03\n",
      "Train Epoch:77 [(0%)]\t Loss: 0.1342  Pearson:0.9516 Spearman:0.9349\n",
      "Train ALL Pearson: 0.9343661166423797\n",
      "Train  ALL Spearman: 0.9147225294366145\n",
      "31.9157977104187\n",
      "Test Epoch:77 [(0%)]\t Loss: 0.1682  Pearson:0.9310 Spearman:0.9072\n",
      "Test : Loss:0.1585 \n",
      "pearson： 0.9320202653798106 ALL Pearson: 0.9330688240959945\n",
      "spearman： 0.9134601053557383 ALL Spearman: 0.9187723319380035\n",
      "time: 36.672271966934204\n",
      "0.03\n",
      "Train Epoch:78 [(0%)]\t Loss: 0.1757  Pearson:0.9651 Spearman:0.9458\n",
      "Train ALL Pearson: 0.9302205512734301\n",
      "Train  ALL Spearman: 0.9090918915724742\n",
      "32.238664865493774\n",
      "Test Epoch:78 [(0%)]\t Loss: 0.1607  Pearson:0.9273 Spearman:0.8979\n",
      "Test : Loss:0.1591 \n",
      "pearson： 0.9320950501026498 ALL Pearson: 0.9334798765611575\n",
      "spearman： 0.915965730298293 ALL Spearman: 0.9187839856715653\n",
      "time: 37.01541185379028\n",
      "0.03\n",
      "Train Epoch:79 [(0%)]\t Loss: 0.1709  Pearson:0.9438 Spearman:0.9117\n",
      "Train ALL Pearson: 0.9332918450811272\n",
      "Train  ALL Spearman: 0.9119653032569134\n",
      "32.01283407211304\n",
      "Test Epoch:79 [(0%)]\t Loss: 0.2440  Pearson:0.9446 Spearman:0.9122\n",
      "Test : Loss:0.2240 \n",
      "pearson： 0.9369509317751024 ALL Pearson: 0.9338265798413588\n",
      "spearman： 0.920318599899219 ALL Spearman: 0.9198581967380959\n",
      "time: 36.69033479690552\n",
      "0.03\n",
      "Train Epoch:80 [(0%)]\t Loss: 0.1572  Pearson:0.9502 Spearman:0.9422\n",
      "Train ALL Pearson: 0.9402261816517999\n",
      "Train  ALL Spearman: 0.9227559807208044\n",
      "32.231300592422485\n",
      "Test Epoch:80 [(0%)]\t Loss: 0.1811  Pearson:0.9246 Spearman:0.9005\n",
      "Test : Loss:0.1712 \n",
      "pearson： 0.9292957047888059 ALL Pearson: 0.93430644887725\n",
      "spearman： 0.914679090541172 ALL Spearman: 0.9197819770198065\n",
      "time: 36.93779158592224\n",
      "0.03\n",
      "Train Epoch:81 [(0%)]\t Loss: 0.1313  Pearson:0.9475 Spearman:0.9497\n",
      "Train ALL Pearson: 0.9429472891158756\n",
      "Train  ALL Spearman: 0.926364014194521\n",
      "32.00613737106323\n",
      "Test Epoch:81 [(0%)]\t Loss: 0.1701  Pearson:0.9171 Spearman:0.8920\n",
      "Test : Loss:0.1661 \n",
      "pearson： 0.9304957954760215 ALL Pearson: 0.9342343025647178\n",
      "spearman： 0.91527310346391 ALL Spearman: 0.919881828268252\n",
      "time: 36.60166144371033\n",
      "0.03\n",
      "Train Epoch:82 [(0%)]\t Loss: 0.1874  Pearson:0.9529 Spearman:0.9405\n",
      "Train ALL Pearson: 0.9405460964611032\n",
      "Train  ALL Spearman: 0.9206346958938194\n",
      "32.076019525527954\n",
      "Test Epoch:82 [(0%)]\t Loss: 0.1418  Pearson:0.9476 Spearman:0.9256\n",
      "Test : Loss:0.1571 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9367922742536131 ALL Pearson: 0.9340329226945211\n",
      "spearman： 0.9218331483459568 ALL Spearman: 0.920496453061462\n",
      "time: 36.87342071533203\n",
      "0.03\n",
      "Train Epoch:83 [(0%)]\t Loss: 0.1405  Pearson:0.9601 Spearman:0.9502\n",
      "Train ALL Pearson: 0.9369281620155362\n",
      "Train  ALL Spearman: 0.9173035884414543\n",
      "31.966328620910645\n",
      "Test Epoch:83 [(0%)]\t Loss: 0.1524  Pearson:0.9459 Spearman:0.9446\n",
      "Test : Loss:0.1623 \n",
      "pearson： 0.9355702412394219 ALL Pearson: 0.9344900448080689\n",
      "spearman： 0.9238303622451867 ALL Spearman: 0.9208014577376747\n",
      "time: 36.91405129432678\n",
      "0.03\n",
      "Train Epoch:84 [(0%)]\t Loss: 0.1382  Pearson:0.9576 Spearman:0.9455\n",
      "Train ALL Pearson: 0.942205181472243\n",
      "Train  ALL Spearman: 0.9247532413016046\n",
      "31.550004959106445\n",
      "Test Epoch:84 [(0%)]\t Loss: 0.2227  Pearson:0.9227 Spearman:0.8954\n",
      "Test : Loss:0.2133 \n",
      "pearson： 0.9333382052604076 ALL Pearson: 0.9343519554715871\n",
      "spearman： 0.9178617978136745 ALL Spearman: 0.9203738668048355\n",
      "time: 36.16252541542053\n",
      "0.03\n",
      "Train Epoch:85 [(0%)]\t Loss: 0.1514  Pearson:0.9667 Spearman:0.9471\n",
      "Train ALL Pearson: 0.9409047723484222\n",
      "Train  ALL Spearman: 0.9236730343769339\n",
      "31.432501792907715\n",
      "Test Epoch:85 [(0%)]\t Loss: 0.1882  Pearson:0.9460 Spearman:0.9189\n",
      "Test : Loss:0.1995 \n",
      "pearson： 0.9363235297276241 ALL Pearson: 0.934777164396402\n",
      "spearman： 0.9228716273383384 ALL Spearman: 0.9213066371765082\n",
      "time: 36.18897604942322\n",
      "0.03\n",
      "Train Epoch:86 [(0%)]\t Loss: 0.1309  Pearson:0.9550 Spearman:0.9488\n",
      "Train ALL Pearson: 0.9330906649746699\n",
      "Train  ALL Spearman: 0.9131809891441774\n",
      "31.49699330329895\n",
      "Test Epoch:86 [(0%)]\t Loss: 0.2181  Pearson:0.9255 Spearman:0.9183\n",
      "Test : Loss:0.2290 \n",
      "pearson： 0.9289835742208381 ALL Pearson: 0.9347627516754493\n",
      "spearman： 0.9180738963802828 ALL Spearman: 0.9208879681523182\n",
      "time: 36.14650201797485\n",
      "0.03\n",
      "Train Epoch:87 [(0%)]\t Loss: 0.1469  Pearson:0.9662 Spearman:0.9563\n",
      "Train ALL Pearson: 0.9363943139225723\n",
      "Train  ALL Spearman: 0.9188426974069713\n",
      "31.53299641609192\n",
      "Test Epoch:87 [(0%)]\t Loss: 0.2071  Pearson:0.9336 Spearman:0.8963\n",
      "Test : Loss:0.2177 \n",
      "pearson： 0.9334297364548919 ALL Pearson: 0.9334920599598353\n",
      "spearman： 0.9147430485534087 ALL Spearman: 0.9192673329158222\n",
      "time: 36.300052642822266\n",
      "0.03\n",
      "Train Epoch:88 [(0%)]\t Loss: 0.1418  Pearson:0.9596 Spearman:0.9348\n",
      "Train ALL Pearson: 0.9440099878095607\n",
      "Train  ALL Spearman: 0.9272748232414216\n",
      "31.975769758224487\n",
      "Test Epoch:88 [(0%)]\t Loss: 0.2020  Pearson:0.9299 Spearman:0.9070\n",
      "Test : Loss:0.2019 \n",
      "pearson： 0.933968315837981 ALL Pearson: 0.9341001919442297\n",
      "spearman： 0.9190931027785072 ALL Spearman: 0.9206543631419206\n",
      "time: 36.66588759422302\n",
      "0.03\n",
      "Train Epoch:89 [(0%)]\t Loss: 0.1254  Pearson:0.9599 Spearman:0.9528\n",
      "Train ALL Pearson: 0.9402335276994431\n",
      "Train  ALL Spearman: 0.9227770813797976\n",
      "31.417421579360962\n",
      "Test Epoch:89 [(0%)]\t Loss: 0.1988  Pearson:0.9348 Spearman:0.9151\n",
      "Test : Loss:0.1964 \n",
      "pearson： 0.9358096966171247 ALL Pearson: 0.9352646996702938\n",
      "spearman： 0.919678932391319 ALL Spearman: 0.9216037221589881\n",
      "time: 36.133880376815796\n",
      "0.03\n",
      "Train Epoch:90 [(0%)]\t Loss: 0.1255  Pearson:0.9568 Spearman:0.9403\n",
      "Train ALL Pearson: 0.9417490274096163\n",
      "Train  ALL Spearman: 0.924067537955045\n",
      "31.250179767608643\n",
      "Test Epoch:90 [(0%)]\t Loss: 0.2667  Pearson:0.9304 Spearman:0.9182\n",
      "Test : Loss:0.2691 \n",
      "pearson： 0.9355118325878433 ALL Pearson: 0.9344113598459898\n",
      "spearman： 0.9218191474393929 ALL Spearman: 0.920972152695525\n",
      "time: 35.9536714553833\n",
      "0.03\n",
      "Train Epoch:91 [(0%)]\t Loss: 0.1987  Pearson:0.9493 Spearman:0.9332\n",
      "Train ALL Pearson: 0.9410975747794698\n",
      "Train  ALL Spearman: 0.9236648774101632\n",
      "31.88477921485901\n",
      "Test Epoch:91 [(0%)]\t Loss: 0.1678  Pearson:0.9447 Spearman:0.9246\n",
      "Test : Loss:0.1760 \n",
      "pearson： 0.9379032349542062 ALL Pearson: 0.9348513707990624\n",
      "spearman： 0.9252932167811978 ALL Spearman: 0.9220435831977323\n",
      "time: 36.53428792953491\n",
      "0.03\n",
      "Train Epoch:92 [(0%)]\t Loss: 0.1128  Pearson:0.9627 Spearman:0.9432\n",
      "Train ALL Pearson: 0.9468530583756548\n",
      "Train  ALL Spearman: 0.9308202229911646\n",
      "31.678263902664185\n",
      "Test Epoch:92 [(0%)]\t Loss: 0.1848  Pearson:0.9350 Spearman:0.9083\n",
      "Test : Loss:0.1809 \n",
      "pearson： 0.935418317107107 ALL Pearson: 0.9331258440099143\n",
      "spearman： 0.9209298793289324 ALL Spearman: 0.9202710893586677\n",
      "time: 36.381922006607056\n",
      "0.03\n",
      "Train Epoch:93 [(0%)]\t Loss: 0.1128  Pearson:0.9595 Spearman:0.9506\n",
      "Train ALL Pearson: 0.9441511229093408\n",
      "Train  ALL Spearman: 0.9267107372744152\n",
      "31.713515043258667\n",
      "Test Epoch:93 [(0%)]\t Loss: 0.2162  Pearson:0.9359 Spearman:0.9215\n",
      "Test : Loss:0.2078 \n",
      "pearson： 0.9335889998869482 ALL Pearson: 0.9350835832640692\n",
      "spearman： 0.9214713119374175 ALL Spearman: 0.9221852302800884\n",
      "time: 36.40500855445862\n",
      "0.03\n",
      "Train Epoch:94 [(0%)]\t Loss: 0.1264  Pearson:0.9668 Spearman:0.9524\n",
      "Train ALL Pearson: 0.9432463152911361\n",
      "Train  ALL Spearman: 0.9263573734261019\n",
      "31.86889910697937\n",
      "Test Epoch:94 [(0%)]\t Loss: 0.1712  Pearson:0.9455 Spearman:0.9274\n",
      "Test : Loss:0.1793 \n",
      "pearson： 0.9353941957170856 ALL Pearson: 0.934645591632881\n",
      "spearman： 0.9222810080226004 ALL Spearman: 0.9221689170749154\n",
      "time: 36.619375705718994\n",
      "0.03\n",
      "Train Epoch:95 [(0%)]\t Loss: 0.1132  Pearson:0.9616 Spearman:0.9377\n",
      "Train ALL Pearson: 0.9443906512776045\n",
      "Train  ALL Spearman: 0.9270811446809029\n",
      "31.816816329956055\n",
      "Test Epoch:95 [(0%)]\t Loss: 0.1712  Pearson:0.9407 Spearman:0.9347\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.9373026185809222 ALL Pearson: 0.9347695028382288\n",
      "spearman： 0.9252746387697045 ALL Spearman: 0.9216738511935441\n",
      "time: 36.521307945251465\n",
      "0.03\n",
      "Train Epoch:96 [(0%)]\t Loss: 0.1129  Pearson:0.9638 Spearman:0.9439\n",
      "Train ALL Pearson: 0.9443123366158426\n",
      "Train  ALL Spearman: 0.9268855898154474\n",
      "31.81911039352417\n",
      "Test Epoch:96 [(0%)]\t Loss: 0.2025  Pearson:0.9274 Spearman:0.9247\n",
      "Test : Loss:0.1854 \n",
      "pearson： 0.9343427868871971 ALL Pearson: 0.9343121079932326\n",
      "spearman： 0.922680648951312 ALL Spearman: 0.9208855876153639\n",
      "time: 36.57758450508118\n",
      "0.03\n",
      "Train Epoch:97 [(0%)]\t Loss: 0.1130  Pearson:0.9632 Spearman:0.9566\n",
      "Train ALL Pearson: 0.947235960971151\n",
      "Train  ALL Spearman: 0.9301200374939212\n",
      "31.912962675094604\n",
      "Test Epoch:97 [(0%)]\t Loss: 0.1658  Pearson:0.9435 Spearman:0.9259\n",
      "Test : Loss:0.1849 \n",
      "pearson： 0.9363064171848507 ALL Pearson: 0.9348703729943411\n",
      "spearman： 0.9207468493377357 ALL Spearman: 0.9213019674367087\n",
      "time: 36.63056778907776\n",
      "0.03\n",
      "Train Epoch:98 [(0%)]\t Loss: 0.1072  Pearson:0.9563 Spearman:0.9398\n",
      "Train ALL Pearson: 0.9504678051850038\n",
      "Train  ALL Spearman: 0.9362772716443973\n",
      "31.986581563949585\n",
      "Test Epoch:98 [(0%)]\t Loss: 0.1795  Pearson:0.9297 Spearman:0.9183\n",
      "Test : Loss:0.1587 \n",
      "pearson： 0.9353107171389087 ALL Pearson: 0.935075664327859\n",
      "spearman： 0.9187872768540185 ALL Spearman: 0.9219684539802351\n",
      "time: 36.655213594436646\n",
      "0.03\n",
      "Train Epoch:99 [(0%)]\t Loss: 0.1210  Pearson:0.9568 Spearman:0.9220\n",
      "Train ALL Pearson: 0.9460801959719846\n",
      "Train  ALL Spearman: 0.9287392752931545\n",
      "31.884305715560913\n",
      "Test Epoch:99 [(0%)]\t Loss: 0.1444  Pearson:0.9475 Spearman:0.9314\n",
      "Test : Loss:0.1626 \n",
      "pearson： 0.9374498696568591 ALL Pearson: 0.9345847888818044\n",
      "spearman： 0.9229557049607616 ALL Spearman: 0.9207083645153116\n",
      "time: 36.71975541114807\n",
      "0.03\n",
      "Train Epoch:100 [(0%)]\t Loss: 0.1202  Pearson:0.9638 Spearman:0.9556\n",
      "Train ALL Pearson: 0.9483655004334691\n",
      "Train  ALL Spearman: 0.9319326637093884\n",
      "31.735819578170776\n",
      "Test Epoch:100 [(0%)]\t Loss: 0.1529  Pearson:0.9330 Spearman:0.9343\n",
      "Test : Loss:0.1646 \n",
      "pearson： 0.9310599235657503 ALL Pearson: 0.9335931679946007\n",
      "spearman： 0.920199877472729 ALL Spearman: 0.9194565108289179\n",
      "time: 36.64924359321594\n",
      "0.03\n",
      "Train Epoch:101 [(0%)]\t Loss: 0.1710  Pearson:0.9718 Spearman:0.9615\n",
      "Train ALL Pearson: 0.9465727280431618\n",
      "Train  ALL Spearman: 0.9267488995013536\n",
      "31.917887449264526\n",
      "Test Epoch:101 [(0%)]\t Loss: 0.2747  Pearson:0.9369 Spearman:0.9262\n",
      "Test : Loss:0.2583 \n",
      "pearson： 0.9324324051396363 ALL Pearson: 0.9342963997802591\n",
      "spearman： 0.9197571603958097 ALL Spearman: 0.9216450574036906\n",
      "time: 36.50465440750122\n",
      "0.03\n",
      "Train Epoch:102 [(0%)]\t Loss: 0.1719  Pearson:0.9678 Spearman:0.9556\n",
      "Train ALL Pearson: 0.9408995846610438\n",
      "Train  ALL Spearman: 0.9224337003310629\n",
      "31.862011432647705\n",
      "Test Epoch:102 [(0%)]\t Loss: 0.2150  Pearson:0.9464 Spearman:0.9277\n",
      "Test : Loss:0.2243 \n",
      "pearson： 0.9353022574812623 ALL Pearson: 0.9334630473336849\n",
      "spearman： 0.9204027916468164 ALL Spearman: 0.9202339405296495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36.72445273399353\n",
      "0.03\n",
      "Train Epoch:103 [(0%)]\t Loss: 0.1528  Pearson:0.9551 Spearman:0.9446\n",
      "Train ALL Pearson: 0.942757672643382\n",
      "Train  ALL Spearman: 0.9251944053207963\n",
      "31.90698003768921\n",
      "Test Epoch:103 [(0%)]\t Loss: 0.2429  Pearson:0.9245 Spearman:0.9338\n",
      "Test : Loss:0.2228 \n",
      "pearson： 0.9306069385776407 ALL Pearson: 0.9333830449189985\n",
      "spearman： 0.9210094231461589 ALL Spearman: 0.9190324078300744\n",
      "time: 36.55448842048645\n",
      "0.03\n",
      "Train Epoch:104 [(0%)]\t Loss: 0.1479  Pearson:0.9699 Spearman:0.9470\n",
      "Train ALL Pearson: 0.9430499889388022\n",
      "Train  ALL Spearman: 0.926185616870364\n",
      "31.861453771591187\n",
      "Test Epoch:104 [(0%)]\t Loss: 0.2010  Pearson:0.9267 Spearman:0.9129\n",
      "Test : Loss:0.1995 \n",
      "pearson： 0.9322794532258656 ALL Pearson: 0.9346872035187831\n",
      "spearman： 0.9193253368837314 ALL Spearman: 0.9202649392715169\n",
      "time: 36.42398977279663\n",
      "0.03\n",
      "Train Epoch:105 [(0%)]\t Loss: 0.1247  Pearson:0.9649 Spearman:0.9495\n",
      "Train ALL Pearson: 0.9425978212498536\n",
      "Train  ALL Spearman: 0.9254323436856339\n",
      "31.61721110343933\n",
      "Test Epoch:105 [(0%)]\t Loss: 0.2188  Pearson:0.9439 Spearman:0.9369\n",
      "Test : Loss:0.2193 \n",
      "pearson： 0.936156482989032 ALL Pearson: 0.9342566830707365\n",
      "spearman： 0.9241648894076758 ALL Spearman: 0.9201984433793107\n",
      "time: 36.44024705886841\n",
      "0.03\n",
      "Train Epoch:106 [(0%)]\t Loss: 0.1423  Pearson:0.9646 Spearman:0.9493\n",
      "Train ALL Pearson: 0.9490954880490277\n",
      "Train  ALL Spearman: 0.9346320750099965\n",
      "31.47070813179016\n",
      "Test Epoch:106 [(0%)]\t Loss: 0.1906  Pearson:0.9345 Spearman:0.9152\n",
      "Test : Loss:0.2048 \n",
      "pearson： 0.9346375419010066 ALL Pearson: 0.9352352720138257\n",
      "spearman： 0.921046338301689 ALL Spearman: 0.9224799100713967\n",
      "time: 36.15410614013672\n",
      "0.03\n",
      "Train Epoch:107 [(0%)]\t Loss: 0.1392  Pearson:0.9579 Spearman:0.9534\n",
      "Train ALL Pearson: 0.9465902726145854\n",
      "Train  ALL Spearman: 0.9298279042642295\n",
      "31.983543157577515\n",
      "Test Epoch:107 [(0%)]\t Loss: 0.1992  Pearson:0.9369 Spearman:0.9232\n",
      "Test : Loss:0.1949 \n",
      "pearson： 0.9330528878462453 ALL Pearson: 0.9346320477001997\n",
      "spearman： 0.9197893682000128 ALL Spearman: 0.9208499470606213\n",
      "time: 36.90396428108215\n",
      "0.03\n",
      "Train Epoch:108 [(0%)]\t Loss: 0.1307  Pearson:0.9672 Spearman:0.9605\n",
      "Train ALL Pearson: 0.9457312058276546\n",
      "Train  ALL Spearman: 0.9277110409046403\n",
      "31.93140411376953\n",
      "Test Epoch:108 [(0%)]\t Loss: 0.1940  Pearson:0.9507 Spearman:0.9314\n",
      "Test : Loss:0.2111 \n",
      "pearson： 0.936586464602113 ALL Pearson: 0.93436848977084\n",
      "spearman： 0.9228038382752088 ALL Spearman: 0.9205418805124729\n",
      "time: 36.798842906951904\n",
      "0.03\n",
      "Train Epoch:109 [(0%)]\t Loss: 0.1423  Pearson:0.9621 Spearman:0.9501\n",
      "Train ALL Pearson: 0.950727848172601\n",
      "Train  ALL Spearman: 0.9338145380121723\n",
      "31.62735891342163\n",
      "Test Epoch:109 [(0%)]\t Loss: 0.1840  Pearson:0.9383 Spearman:0.9270\n",
      "Test : Loss:0.1889 \n",
      "pearson： 0.9351158316149196 ALL Pearson: 0.9339262721141858\n",
      "spearman： 0.9227739982860569 ALL Spearman: 0.921104176086536\n",
      "time: 36.512213706970215\n",
      "0.03\n",
      "Train Epoch:110 [(0%)]\t Loss: 0.1217  Pearson:0.9646 Spearman:0.9494\n",
      "Train ALL Pearson: 0.9467820482211579\n",
      "Train  ALL Spearman: 0.9283198863674398\n",
      "31.888588905334473\n",
      "Test Epoch:110 [(0%)]\t Loss: 0.1851  Pearson:0.9264 Spearman:0.9096\n",
      "Test : Loss:0.1960 \n",
      "pearson： 0.9333088408109372 ALL Pearson: 0.93478194725947\n",
      "spearman： 0.9234498833067626 ALL Spearman: 0.9216536488979634\n",
      "time: 36.5032262802124\n",
      "0.009\n",
      "Train Epoch:111 [(0%)]\t Loss: 0.1254  Pearson:0.9605 Spearman:0.9432\n",
      "Train ALL Pearson: 0.9592487515716653\n",
      "Train  ALL Spearman: 0.947158531726927\n",
      "31.67655897140503\n",
      "Test Epoch:111 [(0%)]\t Loss: 0.1539  Pearson:0.9474 Spearman:0.9319\n",
      "Test : Loss:0.1704 \n",
      "pearson： 0.9404247315265851 ALL Pearson: 0.935842793313108\n",
      "spearman： 0.9235772309774063 ALL Spearman: 0.922194559550139\n",
      "time: 36.68735980987549\n",
      "0.009\n",
      "Train Epoch:112 [(0%)]\t Loss: 0.1251  Pearson:0.9510 Spearman:0.9238\n",
      "Train ALL Pearson: 0.9604484402950781\n",
      "Train  ALL Spearman: 0.9483466115049523\n",
      "31.77728271484375\n",
      "Test Epoch:112 [(0%)]\t Loss: 0.1573  Pearson:0.9417 Spearman:0.9092\n",
      "Test : Loss:0.1696 \n",
      "pearson： 0.9346956583675273 ALL Pearson: 0.9361497493417017\n",
      "spearman： 0.9195613301658252 ALL Spearman: 0.9224891227347424\n",
      "time: 36.38380479812622\n",
      "0.009\n",
      "Train Epoch:113 [(0%)]\t Loss: 0.1150  Pearson:0.9685 Spearman:0.9625\n",
      "Train ALL Pearson: 0.9604260396919989\n",
      "Train  ALL Spearman: 0.9483792860541952\n",
      "31.718547344207764\n",
      "Test Epoch:113 [(0%)]\t Loss: 0.1507  Pearson:0.9417 Spearman:0.9348\n",
      "Test : Loss:0.1664 \n",
      "pearson： 0.9376482404356888 ALL Pearson: 0.9362362377187129\n",
      "spearman： 0.9256693302912565 ALL Spearman: 0.9229360960777481\n",
      "time: 36.3610577583313\n",
      "0.009\n",
      "Train Epoch:114 [(0%)]\t Loss: 0.1281  Pearson:0.9491 Spearman:0.9357\n",
      "Train ALL Pearson: 0.9611137610638953\n",
      "Train  ALL Spearman: 0.9492171359828608\n",
      "31.693284511566162\n",
      "Test Epoch:114 [(0%)]\t Loss: 0.1539  Pearson:0.9391 Spearman:0.9309\n",
      "Test : Loss:0.1716 \n",
      "pearson： 0.9322756985007467 ALL Pearson: 0.9363660620167932\n",
      "spearman： 0.9199525155529379 ALL Spearman: 0.9228076893288107\n",
      "time: 36.47275376319885\n",
      "0.009\n",
      "Train Epoch:115 [(0%)]\t Loss: 0.1227  Pearson:0.9482 Spearman:0.9332\n",
      "Train ALL Pearson: 0.9609736703356945\n",
      "Train  ALL Spearman: 0.9497329902807841\n",
      "31.9079909324646\n",
      "Test Epoch:115 [(0%)]\t Loss: 0.1723  Pearson:0.9306 Spearman:0.9291\n",
      "Test : Loss:0.1702 \n",
      "pearson： 0.9340459058683092 ALL Pearson: 0.9359939241205408\n",
      "spearman： 0.9238691100873343 ALL Spearman: 0.9225367804382666\n",
      "time: 36.54450535774231\n",
      "0.009\n",
      "Train Epoch:116 [(0%)]\t Loss: 0.1068  Pearson:0.9677 Spearman:0.9596\n",
      "Train ALL Pearson: 0.96191867419234\n",
      "Train  ALL Spearman: 0.9513116490020109\n",
      "32.00505328178406\n",
      "Test Epoch:116 [(0%)]\t Loss: 0.1486  Pearson:0.9328 Spearman:0.9289\n",
      "Test : Loss:0.1589 \n",
      "pearson： 0.9374869623665567 ALL Pearson: 0.936464375383374\n",
      "spearman： 0.9257456261969884 ALL Spearman: 0.923229886312971\n",
      "time: 36.80369758605957\n",
      "0.009\n",
      "Train Epoch:117 [(0%)]\t Loss: 0.1250  Pearson:0.9453 Spearman:0.9437\n",
      "Train ALL Pearson: 0.9618785015983564\n",
      "Train  ALL Spearman: 0.9505158734059941\n",
      "31.386414051055908\n",
      "Test Epoch:117 [(0%)]\t Loss: 0.1655  Pearson:0.9308 Spearman:0.9011\n",
      "Test : Loss:0.1690 \n",
      "pearson： 0.9340383087337499 ALL Pearson: 0.9370447397207203\n",
      "spearman： 0.9177853594409804 ALL Spearman: 0.9240591154855791\n",
      "time: 35.99593472480774\n",
      "0.009\n",
      "Train Epoch:118 [(0%)]\t Loss: 0.1102  Pearson:0.9615 Spearman:0.9410\n",
      "Train ALL Pearson: 0.96250872115525\n",
      "Train  ALL Spearman: 0.9514128002317257\n",
      "31.607101440429688\n",
      "Test Epoch:118 [(0%)]\t Loss: 0.1762  Pearson:0.9322 Spearman:0.8998\n",
      "Test : Loss:0.1670 \n",
      "pearson： 0.9343366296634605 ALL Pearson: 0.9364688318304587\n",
      "spearman： 0.9169193107610599 ALL Spearman: 0.923394989650021\n",
      "time: 36.30959415435791\n",
      "0.009\n",
      "Train Epoch:119 [(0%)]\t Loss: 0.1088  Pearson:0.9614 Spearman:0.9546\n",
      "Train ALL Pearson: 0.9617749384904452\n",
      "Train  ALL Spearman: 0.9499165602313627\n",
      "31.954978942871094\n",
      "Test Epoch:119 [(0%)]\t Loss: 0.1619  Pearson:0.9313 Spearman:0.9176\n",
      "Test : Loss:0.1659 \n",
      "pearson： 0.9354535247256589 ALL Pearson: 0.9365442071363168\n",
      "spearman： 0.9251129459005842 ALL Spearman: 0.9236970945303665\n",
      "time: 36.620482444763184\n",
      "0.009\n",
      "Train Epoch:120 [(0%)]\t Loss: 0.1147  Pearson:0.9663 Spearman:0.9623\n",
      "Train ALL Pearson: 0.9626637780480958\n",
      "Train  ALL Spearman: 0.9513506255916818\n",
      "31.75859260559082\n",
      "Test Epoch:120 [(0%)]\t Loss: 0.1585  Pearson:0.9503 Spearman:0.9222\n",
      "Test : Loss:0.1718 \n",
      "pearson： 0.9415344555114806 ALL Pearson: 0.9370482325762652\n",
      "spearman： 0.9265521337001055 ALL Spearman: 0.92400803475235\n",
      "time: 36.585978746414185\n",
      "0.009\n",
      "Train Epoch:121 [(0%)]\t Loss: 0.1188  Pearson:0.9562 Spearman:0.9405\n",
      "Train ALL Pearson: 0.9636476869395201\n",
      "Train  ALL Spearman: 0.9525809970469062\n",
      "31.78492498397827\n",
      "Test Epoch:121 [(0%)]\t Loss: 0.1689  Pearson:0.9283 Spearman:0.9126\n",
      "Test : Loss:0.1717 \n",
      "pearson： 0.9364233128920018 ALL Pearson: 0.9367201355232119\n",
      "spearman： 0.9249638602962698 ALL Spearman: 0.9233661738869919\n",
      "time: 36.68713879585266\n",
      "0.009\n",
      "Train Epoch:122 [(0%)]\t Loss: 0.1102  Pearson:0.9705 Spearman:0.9584\n",
      "Train ALL Pearson: 0.96360172247003\n",
      "Train  ALL Spearman: 0.9518593580929873\n",
      "31.851171493530273\n",
      "Test Epoch:122 [(0%)]\t Loss: 0.1626  Pearson:0.9304 Spearman:0.9010\n",
      "Test : Loss:0.1619 \n",
      "pearson： 0.9368720253294973 ALL Pearson: 0.9365476130691284\n",
      "spearman： 0.9238847635957324 ALL Spearman: 0.9236520737349436\n",
      "time: 36.499680519104004\n",
      "0.009\n",
      "Train Epoch:123 [(0%)]\t Loss: 0.1082  Pearson:0.9633 Spearman:0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson: 0.9636309553841657\n",
      "Train  ALL Spearman: 0.9525143198551764\n",
      "32.0489981174469\n",
      "Test Epoch:123 [(0%)]\t Loss: 0.1630  Pearson:0.9302 Spearman:0.9189\n",
      "Test : Loss:0.1597 \n",
      "pearson： 0.9359639290330596 ALL Pearson: 0.936326316672569\n",
      "spearman： 0.9212717759310066 ALL Spearman: 0.9234879538263822\n",
      "time: 36.722676277160645\n",
      "0.009\n",
      "Train Epoch:124 [(0%)]\t Loss: 0.1080  Pearson:0.9676 Spearman:0.9490\n",
      "Train ALL Pearson: 0.9641414304717859\n",
      "Train  ALL Spearman: 0.9528803958469156\n",
      "31.800343990325928\n",
      "Test Epoch:124 [(0%)]\t Loss: 0.1654  Pearson:0.9278 Spearman:0.9148\n",
      "Test : Loss:0.1625 \n",
      "pearson： 0.934478055820814 ALL Pearson: 0.9360384967372763\n",
      "spearman： 0.9225589791653328 ALL Spearman: 0.9229995022588687\n",
      "time: 36.749297857284546\n",
      "0.009\n",
      "Train Epoch:125 [(0%)]\t Loss: 0.1036  Pearson:0.9676 Spearman:0.9555\n",
      "Train ALL Pearson: 0.9643115046819396\n",
      "Train  ALL Spearman: 0.9540346332123537\n",
      "31.341083526611328\n",
      "Test Epoch:125 [(0%)]\t Loss: 0.1431  Pearson:0.9434 Spearman:0.9170\n",
      "Test : Loss:0.1642 \n",
      "pearson： 0.9398866822472048 ALL Pearson: 0.9365618899026533\n",
      "spearman： 0.922661538826747 ALL Spearman: 0.9235735930199115\n",
      "time: 35.957602977752686\n",
      "0.009\n",
      "Train Epoch:126 [(0%)]\t Loss: 0.1085  Pearson:0.9642 Spearman:0.9492\n",
      "Train ALL Pearson: 0.9640561170383968\n",
      "Train  ALL Spearman: 0.9526674395152186\n",
      "31.573177814483643\n",
      "Test Epoch:126 [(0%)]\t Loss: 0.1838  Pearson:0.9203 Spearman:0.9074\n",
      "Test : Loss:0.1653 \n",
      "pearson： 0.9338333398814869 ALL Pearson: 0.9370260429281562\n",
      "spearman： 0.9212447719213713 ALL Spearman: 0.9238301189779695\n",
      "time: 36.34550881385803\n",
      "0.009\n",
      "Train Epoch:127 [(0%)]\t Loss: 0.1253  Pearson:0.9518 Spearman:0.9465\n",
      "Train ALL Pearson: 0.9630760295006676\n",
      "Train  ALL Spearman: 0.9518632348663417\n",
      "31.773374557495117\n",
      "Test Epoch:127 [(0%)]\t Loss: 0.1645  Pearson:0.9325 Spearman:0.9049\n",
      "Test : Loss:0.1625 \n",
      "pearson： 0.9361917461324029 ALL Pearson: 0.9368393339080695\n",
      "spearman： 0.919399377085501 ALL Spearman: 0.9235836996680185\n",
      "time: 36.48041486740112\n",
      "0.009\n",
      "Train Epoch:128 [(0%)]\t Loss: 0.1089  Pearson:0.9669 Spearman:0.9609\n",
      "Train ALL Pearson: 0.9648300188641806\n",
      "Train  ALL Spearman: 0.9544691818923222\n",
      "31.98764729499817\n",
      "Test Epoch:128 [(0%)]\t Loss: 0.1689  Pearson:0.9363 Spearman:0.9068\n",
      "Test : Loss:0.1706 \n",
      "pearson： 0.9366348072280761 ALL Pearson: 0.9370735070288799\n",
      "spearman： 0.9224469399314476 ALL Spearman: 0.9240377199880194\n",
      "time: 36.943647146224976\n",
      "0.009\n",
      "Train Epoch:129 [(0%)]\t Loss: 0.1073  Pearson:0.9630 Spearman:0.9409\n",
      "Train ALL Pearson: 0.9648630492255906\n",
      "Train  ALL Spearman: 0.9536977131889004\n",
      "31.938586950302124\n",
      "Test Epoch:129 [(0%)]\t Loss: 0.1574  Pearson:0.9285 Spearman:0.9128\n",
      "Test : Loss:0.1643 \n",
      "pearson： 0.934369687259122 ALL Pearson: 0.9370780703624462\n",
      "spearman： 0.9189450526233641 ALL Spearman: 0.9241329341324002\n",
      "time: 36.641282081604004\n",
      "0.009\n",
      "Train Epoch:130 [(0%)]\t Loss: 0.0994  Pearson:0.9756 Spearman:0.9598\n",
      "Train ALL Pearson: 0.9646413699333066\n",
      "Train  ALL Spearman: 0.9530049833656233\n",
      "31.669529676437378\n",
      "Test Epoch:130 [(0%)]\t Loss: 0.1619  Pearson:0.9214 Spearman:0.8900\n",
      "Test : Loss:0.1674 \n",
      "pearson： 0.932387277234633 ALL Pearson: 0.9372114578836785\n",
      "spearman： 0.9137574247957114 ALL Spearman: 0.9240672727936972\n",
      "time: 36.424004793167114\n",
      "0.009\n",
      "Train Epoch:131 [(0%)]\t Loss: 0.1097  Pearson:0.9688 Spearman:0.9536\n",
      "Train ALL Pearson: 0.9657784592703942\n",
      "Train  ALL Spearman: 0.9550826059684984\n",
      "32.02848958969116\n",
      "Test Epoch:131 [(0%)]\t Loss: 0.1752  Pearson:0.9250 Spearman:0.9056\n",
      "Test : Loss:0.1718 \n",
      "pearson： 0.9351360730112895 ALL Pearson: 0.9370298858434001\n",
      "spearman： 0.9212521424413846 ALL Spearman: 0.9243346072680831\n",
      "time: 36.76097345352173\n",
      "0.009\n",
      "Train Epoch:132 [(0%)]\t Loss: 0.1046  Pearson:0.9677 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9655400279916688\n",
      "Train  ALL Spearman: 0.9543810119110843\n",
      "31.97054886817932\n",
      "Test Epoch:132 [(0%)]\t Loss: 0.1632  Pearson:0.9336 Spearman:0.9148\n",
      "Test : Loss:0.1608 \n",
      "pearson： 0.9336986461135685 ALL Pearson: 0.9367267447293889\n",
      "spearman： 0.9197614392396857 ALL Spearman: 0.9238093226482138\n",
      "time: 36.797001361846924\n",
      "0.009\n",
      "Train Epoch:133 [(0%)]\t Loss: 0.1132  Pearson:0.9666 Spearman:0.9605\n",
      "Train ALL Pearson: 0.9664845086338851\n",
      "Train  ALL Spearman: 0.9557363705683243\n",
      "31.676775455474854\n",
      "Test Epoch:133 [(0%)]\t Loss: 0.1714  Pearson:0.9405 Spearman:0.9423\n",
      "Test : Loss:0.1756 \n",
      "pearson： 0.9326652869981593 ALL Pearson: 0.9370162073807553\n",
      "spearman： 0.9245350677801606 ALL Spearman: 0.9244168582666882\n",
      "time: 36.629189014434814\n",
      "0.009\n",
      "Train Epoch:134 [(0%)]\t Loss: 0.1136  Pearson:0.9662 Spearman:0.9554\n",
      "Train ALL Pearson: 0.965078833182969\n",
      "Train  ALL Spearman: 0.9551292561011456\n",
      "31.798729419708252\n",
      "Test Epoch:134 [(0%)]\t Loss: 0.1630  Pearson:0.9512 Spearman:0.9325\n",
      "Test : Loss:0.1677 \n",
      "pearson： 0.9406397399390193 ALL Pearson: 0.9372664154781436\n",
      "spearman： 0.9266016564009057 ALL Spearman: 0.9247574305266361\n",
      "time: 36.37026333808899\n",
      "0.009\n",
      "Train Epoch:135 [(0%)]\t Loss: 0.1201  Pearson:0.9566 Spearman:0.9472\n",
      "Train ALL Pearson: 0.9652945255510618\n",
      "Train  ALL Spearman: 0.953942674343006\n",
      "31.687429904937744\n",
      "Test Epoch:135 [(0%)]\t Loss: 0.1660  Pearson:0.9522 Spearman:0.9269\n",
      "Test : Loss:0.1695 \n",
      "pearson： 0.9383109400560703 ALL Pearson: 0.9369704791796135\n",
      "spearman： 0.9221669326236395 ALL Spearman: 0.9242151440619756\n",
      "time: 36.340555906295776\n",
      "0.009\n",
      "Train Epoch:136 [(0%)]\t Loss: 0.1075  Pearson:0.9715 Spearman:0.9453\n",
      "Train ALL Pearson: 0.9660944532020795\n",
      "Train  ALL Spearman: 0.9555658675608298\n",
      "31.065195322036743\n",
      "Test Epoch:136 [(0%)]\t Loss: 0.1741  Pearson:0.9386 Spearman:0.9416\n",
      "Test : Loss:0.1682 \n",
      "pearson： 0.938000348236688 ALL Pearson: 0.9370956876887642\n",
      "spearman： 0.9279544958283136 ALL Spearman: 0.9245534105892736\n",
      "time: 35.84066343307495\n",
      "0.009\n",
      "Train Epoch:137 [(0%)]\t Loss: 0.1149  Pearson:0.9590 Spearman:0.9412\n",
      "Train ALL Pearson: 0.9664263377762493\n",
      "Train  ALL Spearman: 0.9562148932125738\n",
      "31.477717638015747\n",
      "Test Epoch:137 [(0%)]\t Loss: 0.1681  Pearson:0.9523 Spearman:0.9408\n",
      "Test : Loss:0.1705 \n",
      "pearson： 0.9419179104654699 ALL Pearson: 0.9370685636448586\n",
      "spearman： 0.9295717935244444 ALL Spearman: 0.9247361971050672\n",
      "time: 36.13022708892822\n",
      "0.009\n",
      "Train Epoch:138 [(0%)]\t Loss: 0.1041  Pearson:0.9719 Spearman:0.9571\n",
      "Train ALL Pearson: 0.9665246575422571\n",
      "Train  ALL Spearman: 0.9555677308494545\n",
      "31.73555612564087\n",
      "Test Epoch:138 [(0%)]\t Loss: 0.1779  Pearson:0.9412 Spearman:0.9352\n",
      "Test : Loss:0.1693 \n",
      "pearson： 0.9370620236223854 ALL Pearson: 0.9364192906306202\n",
      "spearman： 0.9265400518370406 ALL Spearman: 0.9239964008775782\n",
      "time: 36.41705369949341\n",
      "0.009\n",
      "Train Epoch:139 [(0%)]\t Loss: 0.0953  Pearson:0.9761 Spearman:0.9687\n",
      "Train ALL Pearson: 0.9668375672383803\n",
      "Train  ALL Spearman: 0.9567644124622573\n",
      "31.895944118499756\n",
      "Test Epoch:139 [(0%)]\t Loss: 0.1680  Pearson:0.9448 Spearman:0.9321\n",
      "Test : Loss:0.1654 \n",
      "pearson： 0.9373584626696837 ALL Pearson: 0.9366929327599762\n",
      "spearman： 0.9251429411153071 ALL Spearman: 0.9247171978844293\n",
      "time: 36.61742973327637\n",
      "0.009\n",
      "Train Epoch:140 [(0%)]\t Loss: 0.0954  Pearson:0.9686 Spearman:0.9461\n",
      "Train ALL Pearson: 0.9675477428004801\n",
      "Train  ALL Spearman: 0.9570550504370398\n",
      "31.843416213989258\n",
      "Test Epoch:140 [(0%)]\t Loss: 0.1710  Pearson:0.9412 Spearman:0.9211\n",
      "Test : Loss:0.1695 \n",
      "pearson： 0.9353930127105143 ALL Pearson: 0.9364013282674334\n",
      "spearman： 0.9204604552926859 ALL Spearman: 0.9241288594979845\n",
      "time: 36.736788511276245\n",
      "0.009\n",
      "Train Epoch:141 [(0%)]\t Loss: 0.0979  Pearson:0.9722 Spearman:0.9561\n",
      "Train ALL Pearson: 0.9668125807593798\n",
      "Train  ALL Spearman: 0.9557391696922612\n",
      "31.55190086364746\n",
      "Test Epoch:141 [(0%)]\t Loss: 0.1583  Pearson:0.9459 Spearman:0.9324\n",
      "Test : Loss:0.1668 \n",
      "pearson： 0.9398227496297857 ALL Pearson: 0.9367749103049519\n",
      "spearman： 0.9287659261951536 ALL Spearman: 0.9246632720486977\n",
      "time: 36.28338360786438\n",
      "0.009\n",
      "Train Epoch:142 [(0%)]\t Loss: 0.1137  Pearson:0.9663 Spearman:0.9433\n",
      "Train ALL Pearson: 0.9673396045321162\n",
      "Train  ALL Spearman: 0.9566416910466912\n",
      "31.95290446281433\n",
      "Test Epoch:142 [(0%)]\t Loss: 0.1670  Pearson:0.9314 Spearman:0.9254\n",
      "Test : Loss:0.1579 \n",
      "pearson： 0.9359218511882926 ALL Pearson: 0.9372083737365956\n",
      "spearman： 0.9217737392379427 ALL Spearman: 0.9248806244140376\n",
      "time: 36.554707765579224\n",
      "0.009\n",
      "Train Epoch:143 [(0%)]\t Loss: 0.1023  Pearson:0.9722 Spearman:0.9609\n",
      "Train ALL Pearson: 0.9675699385054746\n",
      "Train  ALL Spearman: 0.9573504482512641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.89439630508423\n",
      "Test Epoch:143 [(0%)]\t Loss: 0.1775  Pearson:0.9460 Spearman:0.9335\n",
      "Test : Loss:0.1677 \n",
      "pearson： 0.9397187991622467 ALL Pearson: 0.9362513721588779\n",
      "spearman： 0.9275828291973263 ALL Spearman: 0.9238605987981545\n",
      "time: 36.596888303756714\n",
      "0.009\n",
      "Train Epoch:144 [(0%)]\t Loss: 0.1046  Pearson:0.9718 Spearman:0.9517\n",
      "Train ALL Pearson: 0.9674181279967298\n",
      "Train  ALL Spearman: 0.9566889916805181\n",
      "31.01465630531311\n",
      "Test Epoch:144 [(0%)]\t Loss: 0.1550  Pearson:0.9475 Spearman:0.9158\n",
      "Test : Loss:0.1614 \n",
      "pearson： 0.9394604907631834 ALL Pearson: 0.9368730488297067\n",
      "spearman： 0.9249612024382156 ALL Spearman: 0.9246395169430124\n",
      "time: 35.81200456619263\n",
      "0.009\n",
      "Train Epoch:145 [(0%)]\t Loss: 0.1040  Pearson:0.9688 Spearman:0.9444\n",
      "Train ALL Pearson: 0.9678029265430458\n",
      "Train  ALL Spearman: 0.9571816265161073\n",
      "31.74401068687439\n",
      "Test Epoch:145 [(0%)]\t Loss: 0.1663  Pearson:0.9422 Spearman:0.9407\n",
      "Test : Loss:0.1594 \n",
      "pearson： 0.938751111379908 ALL Pearson: 0.9370924285892811\n",
      "spearman： 0.9283237761091315 ALL Spearman: 0.9248570867265704\n",
      "time: 36.3505334854126\n",
      "0.009\n",
      "Train Epoch:146 [(0%)]\t Loss: 0.0955  Pearson:0.9755 Spearman:0.9587\n",
      "Train ALL Pearson: 0.9683620011135348\n",
      "Train  ALL Spearman: 0.9586104838079416\n",
      "31.742284774780273\n",
      "Test Epoch:146 [(0%)]\t Loss: 0.1816  Pearson:0.9327 Spearman:0.9239\n",
      "Test : Loss:0.1639 \n",
      "pearson： 0.9346430973404807 ALL Pearson: 0.9365191093207491\n",
      "spearman： 0.9215565236401827 ALL Spearman: 0.9242962150593614\n",
      "time: 36.509305477142334\n",
      "0.009\n",
      "Train Epoch:147 [(0%)]\t Loss: 0.0969  Pearson:0.9740 Spearman:0.9472\n",
      "Train ALL Pearson: 0.9672726475822325\n",
      "Train  ALL Spearman: 0.9572355428831676\n",
      "31.500664949417114\n",
      "Test Epoch:147 [(0%)]\t Loss: 0.1615  Pearson:0.9483 Spearman:0.9308\n",
      "Test : Loss:0.1653 \n",
      "pearson： 0.9352231460314839 ALL Pearson: 0.9369148746687085\n",
      "spearman： 0.9214987874370738 ALL Spearman: 0.9250153997677267\n",
      "time: 36.056560039520264\n",
      "0.009\n",
      "Train Epoch:148 [(0%)]\t Loss: 0.0967  Pearson:0.9710 Spearman:0.9548\n",
      "Train ALL Pearson: 0.9682505623390322\n",
      "Train  ALL Spearman: 0.958037745991055\n",
      "31.759737014770508\n",
      "Test Epoch:148 [(0%)]\t Loss: 0.1781  Pearson:0.9458 Spearman:0.9334\n",
      "Test : Loss:0.1656 \n",
      "pearson： 0.9385241214605343 ALL Pearson: 0.9365119676710847\n",
      "spearman： 0.9295256952724403 ALL Spearman: 0.9243336157443736\n",
      "time: 36.602184772491455\n",
      "0.009\n",
      "Train Epoch:149 [(0%)]\t Loss: 0.0992  Pearson:0.9678 Spearman:0.9679\n",
      "Train ALL Pearson: 0.9682037824223084\n",
      "Train  ALL Spearman: 0.9584072771402811\n",
      "31.86147975921631\n",
      "Test Epoch:149 [(0%)]\t Loss: 0.1753  Pearson:0.9378 Spearman:0.9136\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.9357958500251967 ALL Pearson: 0.9365516834312456\n",
      "spearman： 0.9209562288260809 ALL Spearman: 0.9250792892433181\n",
      "time: 36.51398706436157\n",
      "0.009\n",
      "Train Epoch:150 [(0%)]\t Loss: 0.1021  Pearson:0.9653 Spearman:0.9544\n",
      "Train ALL Pearson: 0.9693658942138363\n",
      "Train  ALL Spearman: 0.959991036927568\n",
      "31.732319355010986\n",
      "Test Epoch:150 [(0%)]\t Loss: 0.1495  Pearson:0.9385 Spearman:0.9380\n",
      "Test : Loss:0.1618 \n",
      "pearson： 0.9374462102582272 ALL Pearson: 0.9364247826298322\n",
      "spearman： 0.9315510420969783 ALL Spearman: 0.9250098415146245\n",
      "time: 36.34248375892639\n",
      "0.009\n",
      "Train Epoch:151 [(0%)]\t Loss: 0.0975  Pearson:0.9648 Spearman:0.9528\n",
      "Train ALL Pearson: 0.9690753761560842\n",
      "Train  ALL Spearman: 0.9586979411878988\n",
      "31.812413692474365\n",
      "Test Epoch:151 [(0%)]\t Loss: 0.1618  Pearson:0.9214 Spearman:0.9147\n",
      "Test : Loss:0.1596 \n",
      "pearson： 0.935244593467235 ALL Pearson: 0.9363487890298273\n",
      "spearman： 0.9233057848119345 ALL Spearman: 0.9246935289227342\n",
      "time: 36.718921422958374\n",
      "0.009\n",
      "Train Epoch:152 [(0%)]\t Loss: 0.1038  Pearson:0.9728 Spearman:0.9577\n",
      "Train ALL Pearson: 0.9694200280910991\n",
      "Train  ALL Spearman: 0.9590142261263157\n",
      "31.60043978691101\n",
      "Test Epoch:152 [(0%)]\t Loss: 0.1558  Pearson:0.9353 Spearman:0.9181\n",
      "Test : Loss:0.1595 \n",
      "pearson： 0.9373819147915945 ALL Pearson: 0.9365135014733725\n",
      "spearman： 0.9251875488716834 ALL Spearman: 0.924554340481774\n",
      "time: 36.4660279750824\n",
      "0.009\n",
      "Train Epoch:153 [(0%)]\t Loss: 0.0980  Pearson:0.9661 Spearman:0.9567\n",
      "Train ALL Pearson: 0.9691277529639777\n",
      "Train  ALL Spearman: 0.9590445321791324\n",
      "31.36149835586548\n",
      "Test Epoch:153 [(0%)]\t Loss: 0.1638  Pearson:0.9350 Spearman:0.9345\n",
      "Test : Loss:0.1631 \n",
      "pearson： 0.9365225461685859 ALL Pearson: 0.9360885150037145\n",
      "spearman： 0.9258116614927056 ALL Spearman: 0.9245088990951733\n",
      "time: 36.106842279434204\n",
      "0.009\n",
      "Train Epoch:154 [(0%)]\t Loss: 0.0988  Pearson:0.9679 Spearman:0.9522\n",
      "Train ALL Pearson: 0.9688089547109285\n",
      "Train  ALL Spearman: 0.9591695246731875\n",
      "31.240162134170532\n",
      "Test Epoch:154 [(0%)]\t Loss: 0.1633  Pearson:0.9441 Spearman:0.9203\n",
      "Test : Loss:0.1709 \n",
      "pearson： 0.9368148959217971 ALL Pearson: 0.936286048943871\n",
      "spearman： 0.9257521789878232 ALL Spearman: 0.9244649963027585\n",
      "time: 36.239556074142456\n",
      "0.009\n",
      "Train Epoch:155 [(0%)]\t Loss: 0.0901  Pearson:0.9767 Spearman:0.9605\n",
      "Train ALL Pearson: 0.9693210581704486\n",
      "Train  ALL Spearman: 0.9600850365550742\n",
      "31.737913846969604\n",
      "Test Epoch:155 [(0%)]\t Loss: 0.1612  Pearson:0.9288 Spearman:0.9304\n",
      "Test : Loss:0.1594 \n",
      "pearson： 0.9359856376799258 ALL Pearson: 0.9363799911715343\n",
      "spearman： 0.927324466751455 ALL Spearman: 0.9246596099380135\n",
      "time: 36.43297028541565\n",
      "0.0026999999999999997\n",
      "Train Epoch:156 [(0%)]\t Loss: 0.1064  Pearson:0.9629 Spearman:0.9595\n",
      "Train ALL Pearson: 0.9666673219956102\n",
      "Train  ALL Spearman: 0.9563573459956234\n",
      "31.623573303222656\n",
      "Test Epoch:156 [(0%)]\t Loss: 0.1706  Pearson:0.9357 Spearman:0.9192\n",
      "Test : Loss:0.1676 \n",
      "pearson： 0.9376292197403356 ALL Pearson: 0.9372840326688144\n",
      "spearman： 0.9253950315413227 ALL Spearman: 0.9248576250242054\n",
      "time: 36.41795468330383\n",
      "0.0026999999999999997\n",
      "Train Epoch:157 [(0%)]\t Loss: 0.1008  Pearson:0.9722 Spearman:0.9620\n",
      "Train ALL Pearson: 0.9665377831960488\n",
      "Train  ALL Spearman: 0.9555111038009797\n",
      "31.8587167263031\n",
      "Test Epoch:157 [(0%)]\t Loss: 0.1702  Pearson:0.9348 Spearman:0.9084\n",
      "Test : Loss:0.1635 \n",
      "pearson： 0.9364534354552329 ALL Pearson: 0.9374081129090506\n",
      "spearman： 0.9227121359027948 ALL Spearman: 0.9249776096603108\n",
      "time: 36.57820224761963\n",
      "0.0026999999999999997\n",
      "Train Epoch:158 [(0%)]\t Loss: 0.1125  Pearson:0.9618 Spearman:0.9433\n",
      "Train ALL Pearson: 0.9668020105266334\n",
      "Train  ALL Spearman: 0.9560899183084736\n",
      "31.510368585586548\n",
      "Test Epoch:158 [(0%)]\t Loss: 0.1640  Pearson:0.9391 Spearman:0.9228\n",
      "Test : Loss:0.1651 \n",
      "pearson： 0.93943793270689 ALL Pearson: 0.9373150778137239\n",
      "spearman： 0.9276769756162636 ALL Spearman: 0.9247660377608948\n",
      "time: 36.32482409477234\n",
      "0.0026999999999999997\n",
      "Train Epoch:159 [(0%)]\t Loss: 0.1125  Pearson:0.9674 Spearman:0.9491\n",
      "Train ALL Pearson: 0.967246658830522\n",
      "Train  ALL Spearman: 0.9563774600246009\n",
      "31.848752737045288\n",
      "Test Epoch:159 [(0%)]\t Loss: 0.1609  Pearson:0.9374 Spearman:0.9251\n",
      "Test : Loss:0.1671 \n",
      "pearson： 0.9356138144356592 ALL Pearson: 0.9371547502694356\n",
      "spearman： 0.9223059811204372 ALL Spearman: 0.9246806681104581\n",
      "time: 36.60722732543945\n",
      "0.0026999999999999997\n",
      "Train Epoch:160 [(0%)]\t Loss: 0.0912  Pearson:0.9779 Spearman:0.9688\n",
      "Train ALL Pearson: 0.9673086267705406\n",
      "Train  ALL Spearman: 0.956717518890571\n",
      "31.643590688705444\n",
      "Test Epoch:160 [(0%)]\t Loss: 0.1652  Pearson:0.9345 Spearman:0.9126\n",
      "Test : Loss:0.1640 \n",
      "pearson： 0.9363413703835534 ALL Pearson: 0.9372470360381648\n",
      "spearman： 0.9238082559121703 ALL Spearman: 0.924715034087549\n",
      "time: 36.288100719451904\n",
      "0.0026999999999999997\n",
      "Train Epoch:161 [(0%)]\t Loss: 0.1035  Pearson:0.9682 Spearman:0.9651\n",
      "Train ALL Pearson: 0.966925935456288\n",
      "Train  ALL Spearman: 0.9566314499590264\n",
      "31.573309659957886\n",
      "Test Epoch:161 [(0%)]\t Loss: 0.1619  Pearson:0.9437 Spearman:0.9316\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.9404698205117502 ALL Pearson: 0.9372618272202158\n",
      "spearman： 0.9294373242886006 ALL Spearman: 0.9248402190895006\n",
      "time: 36.36677312850952\n",
      "0.0026999999999999997\n",
      "Train Epoch:162 [(0%)]\t Loss: 0.1017  Pearson:0.9669 Spearman:0.9564\n",
      "Train ALL Pearson: 0.96669711861009\n",
      "Train  ALL Spearman: 0.9560384206991233\n",
      "31.20269274711609\n",
      "Test Epoch:162 [(0%)]\t Loss: 0.1643  Pearson:0.9358 Spearman:0.9090\n",
      "Test : Loss:0.1637 \n",
      "pearson： 0.9368409004203613 ALL Pearson: 0.9370824306566717\n",
      "spearman： 0.9217365763058503 ALL Spearman: 0.9246537623682647\n",
      "time: 35.88419055938721\n",
      "0.0026999999999999997\n",
      "Train Epoch:163 [(0%)]\t Loss: 0.1096  Pearson:0.9600 Spearman:0.9427\n",
      "Train ALL Pearson: 0.9667948217683208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  ALL Spearman: 0.9564104213041775\n",
      "31.881383657455444\n",
      "Test Epoch:163 [(0%)]\t Loss: 0.1657  Pearson:0.9365 Spearman:0.9143\n",
      "Test : Loss:0.1627 \n",
      "pearson： 0.9373650634759264 ALL Pearson: 0.9372246795661562\n",
      "spearman： 0.9224794061651143 ALL Spearman: 0.9249523239447608\n",
      "time: 36.71983218193054\n",
      "0.0026999999999999997\n",
      "Train Epoch:164 [(0%)]\t Loss: 0.1080  Pearson:0.9692 Spearman:0.9510\n",
      "Train ALL Pearson: 0.9670121212620897\n",
      "Train  ALL Spearman: 0.9559402422426291\n",
      "31.588881969451904\n",
      "Test Epoch:164 [(0%)]\t Loss: 0.1611  Pearson:0.9404 Spearman:0.9106\n",
      "Test : Loss:0.1634 \n",
      "pearson： 0.9384256429885695 ALL Pearson: 0.9372953190792536\n",
      "spearman： 0.9215057614636994 ALL Spearman: 0.9249880441272772\n",
      "time: 36.57928228378296\n",
      "0.0026999999999999997\n",
      "Train Epoch:165 [(0%)]\t Loss: 0.0926  Pearson:0.9799 Spearman:0.9753\n",
      "Train ALL Pearson: 0.966990584895572\n",
      "Train  ALL Spearman: 0.9566415708159515\n",
      "31.87987184524536\n",
      "Test Epoch:165 [(0%)]\t Loss: 0.1560  Pearson:0.9399 Spearman:0.9258\n",
      "Test : Loss:0.1640 \n",
      "pearson： 0.9368972162221005 ALL Pearson: 0.9371378295362053\n",
      "spearman： 0.9225267048670005 ALL Spearman: 0.9249181728466864\n",
      "time: 36.475396156311035\n",
      "0.0026999999999999997\n",
      "Train Epoch:166 [(0%)]\t Loss: 0.0926  Pearson:0.9808 Spearman:0.9696\n",
      "Train ALL Pearson: 0.9675331903756024\n",
      "Train  ALL Spearman: 0.9570992079142729\n",
      "31.681190729141235\n",
      "Test Epoch:166 [(0%)]\t Loss: 0.1618  Pearson:0.9512 Spearman:0.9376\n",
      "Test : Loss:0.1645 \n",
      "pearson： 0.9404455107882519 ALL Pearson: 0.9372160362664925\n",
      "spearman： 0.9283333746366746 ALL Spearman: 0.9249499672425111\n",
      "time: 36.51464009284973\n",
      "0.0026999999999999997\n",
      "Train Epoch:167 [(0%)]\t Loss: 0.1098  Pearson:0.9685 Spearman:0.9550\n",
      "Train ALL Pearson: 0.9681342193301189\n",
      "Train  ALL Spearman: 0.9580620453017393\n",
      "32.036731243133545\n",
      "Test Epoch:167 [(0%)]\t Loss: 0.1541  Pearson:0.9499 Spearman:0.9251\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.940307829360009 ALL Pearson: 0.9371967337324627\n",
      "spearman： 0.9260046570490623 ALL Spearman: 0.9250375587092077\n",
      "time: 36.61026430130005\n",
      "0.0026999999999999997\n",
      "Train Epoch:168 [(0%)]\t Loss: 0.1005  Pearson:0.9741 Spearman:0.9663\n",
      "Train ALL Pearson: 0.9673561971252523\n",
      "Train  ALL Spearman: 0.957159906994143\n",
      "31.64239740371704\n",
      "Test Epoch:168 [(0%)]\t Loss: 0.1536  Pearson:0.9279 Spearman:0.9248\n",
      "Test : Loss:0.1642 \n",
      "pearson： 0.936304641696228 ALL Pearson: 0.9370721752821916\n",
      "spearman： 0.9281652365716404 ALL Spearman: 0.9250629562095273\n",
      "time: 36.43186163902283\n",
      "0.0026999999999999997\n",
      "Train Epoch:169 [(0%)]\t Loss: 0.1107  Pearson:0.9657 Spearman:0.9524\n",
      "Train ALL Pearson: 0.9673225616500879\n",
      "Train  ALL Spearman: 0.956523755740373\n",
      "31.576846599578857\n",
      "Test Epoch:169 [(0%)]\t Loss: 0.1548  Pearson:0.9320 Spearman:0.9278\n",
      "Test : Loss:0.1663 \n",
      "pearson： 0.9342408807546011 ALL Pearson: 0.9370130566613554\n",
      "spearman： 0.9229710027796703 ALL Spearman: 0.9249615443360653\n",
      "time: 36.50426650047302\n",
      "0.0026999999999999997\n",
      "Train Epoch:170 [(0%)]\t Loss: 0.1048  Pearson:0.9661 Spearman:0.9554\n",
      "Train ALL Pearson: 0.9678414297590509\n",
      "Train  ALL Spearman: 0.9572651936695827\n",
      "31.776907682418823\n",
      "Test Epoch:170 [(0%)]\t Loss: 0.1628  Pearson:0.9464 Spearman:0.9304\n",
      "Test : Loss:0.1638 \n",
      "pearson： 0.9395613063758937 ALL Pearson: 0.9371770306523369\n",
      "spearman： 0.9262878752678211 ALL Spearman: 0.9249238701019825\n",
      "time: 36.37370681762695\n",
      "0.0026999999999999997\n",
      "Train Epoch:171 [(0%)]\t Loss: 0.0920  Pearson:0.9767 Spearman:0.9597\n",
      "Train ALL Pearson: 0.968073809515689\n",
      "Train  ALL Spearman: 0.9571370985337158\n",
      "31.85560655593872\n",
      "Test Epoch:171 [(0%)]\t Loss: 0.1835  Pearson:0.9233 Spearman:0.8902\n",
      "Test : Loss:0.1634 \n",
      "pearson： 0.9341020820575445 ALL Pearson: 0.9371046633225328\n",
      "spearman： 0.9176096344018767 ALL Spearman: 0.9248760349224565\n",
      "time: 36.656068086624146\n",
      "0.0026999999999999997\n",
      "Train Epoch:172 [(0%)]\t Loss: 0.1005  Pearson:0.9732 Spearman:0.9621\n",
      "Train ALL Pearson: 0.9664735287891216\n",
      "Train  ALL Spearman: 0.9560441332463219\n",
      "31.406298398971558\n",
      "Test Epoch:172 [(0%)]\t Loss: 0.1735  Pearson:0.9245 Spearman:0.9143\n",
      "Test : Loss:0.1614 \n",
      "pearson： 0.9345833867928682 ALL Pearson: 0.9371229942715276\n",
      "spearman： 0.9216705398643433 ALL Spearman: 0.9250654819823133\n",
      "time: 36.228291273117065\n",
      "0.0026999999999999997\n",
      "Train Epoch:173 [(0%)]\t Loss: 0.1074  Pearson:0.9667 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9683955543284357\n",
      "Train  ALL Spearman: 0.9576131100765516\n",
      "31.75121521949768\n",
      "Test Epoch:173 [(0%)]\t Loss: 0.1786  Pearson:0.9215 Spearman:0.9073\n",
      "Test : Loss:0.1654 \n",
      "pearson： 0.9339942236404423 ALL Pearson: 0.9371184801187296\n",
      "spearman： 0.9233148079490668 ALL Spearman: 0.9249479636605767\n",
      "time: 36.717623472213745\n",
      "0.0026999999999999997\n",
      "Train Epoch:174 [(0%)]\t Loss: 0.1161  Pearson:0.9578 Spearman:0.9299\n",
      "Train ALL Pearson: 0.9673172144603974\n",
      "Train  ALL Spearman: 0.956975028306776\n",
      "31.827818393707275\n",
      "Test Epoch:174 [(0%)]\t Loss: 0.1565  Pearson:0.9396 Spearman:0.9160\n",
      "Test : Loss:0.1642 \n",
      "pearson： 0.9399799716728482 ALL Pearson: 0.9370633027029996\n",
      "spearman： 0.9233273574909545 ALL Spearman: 0.924715554050796\n",
      "time: 36.50395059585571\n",
      "0.0026999999999999997\n",
      "Train Epoch:175 [(0%)]\t Loss: 0.0975  Pearson:0.9747 Spearman:0.9625\n",
      "Train ALL Pearson: 0.9685327508680441\n",
      "Train  ALL Spearman: 0.9584061052118302\n",
      "31.964844226837158\n",
      "Test Epoch:175 [(0%)]\t Loss: 0.1705  Pearson:0.9382 Spearman:0.9115\n",
      "Test : Loss:0.1637 \n",
      "pearson： 0.9365826108661458 ALL Pearson: 0.9370925506238871\n",
      "spearman： 0.9205587673826243 ALL Spearman: 0.924896742541321\n",
      "time: 36.7792592048645\n",
      "0.0026999999999999997\n",
      "Train Epoch:176 [(0%)]\t Loss: 0.0902  Pearson:0.9772 Spearman:0.9644\n",
      "Train ALL Pearson: 0.9684667232193639\n",
      "Train  ALL Spearman: 0.9587027970578654\n",
      "31.283018350601196\n",
      "Test Epoch:176 [(0%)]\t Loss: 0.1747  Pearson:0.9293 Spearman:0.9082\n",
      "Test : Loss:0.1649 \n",
      "pearson： 0.9351965156106005 ALL Pearson: 0.9372236272238358\n",
      "spearman： 0.9215165713577709 ALL Spearman: 0.9250476426226732\n",
      "time: 36.1514573097229\n",
      "0.0026999999999999997\n",
      "Train Epoch:177 [(0%)]\t Loss: 0.0968  Pearson:0.9711 Spearman:0.9563\n",
      "Train ALL Pearson: 0.9684020839190105\n",
      "Train  ALL Spearman: 0.9575974389909706\n",
      "31.93426012992859\n",
      "Test Epoch:177 [(0%)]\t Loss: 0.1626  Pearson:0.9364 Spearman:0.9102\n",
      "Test : Loss:0.1656 \n",
      "pearson： 0.9359425723876835 ALL Pearson: 0.9372071845913569\n",
      "spearman： 0.9209303098769724 ALL Spearman: 0.9249352656300485\n",
      "time: 36.581770181655884\n",
      "0.0026999999999999997\n",
      "Train Epoch:178 [(0%)]\t Loss: 0.1042  Pearson:0.9604 Spearman:0.9602\n",
      "Train ALL Pearson: 0.9683947572012606\n",
      "Train  ALL Spearman: 0.958296138875891\n",
      "31.4953932762146\n",
      "Test Epoch:178 [(0%)]\t Loss: 0.1651  Pearson:0.9486 Spearman:0.9175\n",
      "Test : Loss:0.1622 \n",
      "pearson： 0.939782270240679 ALL Pearson: 0.9373352533777661\n",
      "spearman： 0.92605791756701 ALL Spearman: 0.9252069523211679\n",
      "time: 36.23548436164856\n",
      "0.0008099999999999998\n",
      "Train Epoch:179 [(0%)]\t Loss: 0.1099  Pearson:0.9658 Spearman:0.9460\n",
      "Train ALL Pearson: 0.9668926567974256\n",
      "Train  ALL Spearman: 0.9562315362337257\n",
      "31.859416484832764\n",
      "Test Epoch:179 [(0%)]\t Loss: 0.1678  Pearson:0.9220 Spearman:0.9133\n",
      "Test : Loss:0.1649 \n",
      "pearson： 0.9337656743127457 ALL Pearson: 0.9373531774395548\n",
      "spearman： 0.9234288090252197 ALL Spearman: 0.9249288834570697\n",
      "time: 36.69986319541931\n",
      "0.0008099999999999998\n",
      "Train Epoch:180 [(0%)]\t Loss: 0.1055  Pearson:0.9654 Spearman:0.9573\n",
      "Train ALL Pearson: 0.9667592310326408\n",
      "Train  ALL Spearman: 0.9563072640229727\n",
      "31.895440578460693\n",
      "Test Epoch:180 [(0%)]\t Loss: 0.1654  Pearson:0.9396 Spearman:0.9328\n",
      "Test : Loss:0.1637 \n",
      "pearson： 0.9365604357065341 ALL Pearson: 0.9373690531916317\n",
      "spearman： 0.9263720208529894 ALL Spearman: 0.9249512543440773\n",
      "time: 36.583070039749146\n",
      "0.0008099999999999998\n",
      "Train Epoch:181 [(0%)]\t Loss: 0.1134  Pearson:0.9685 Spearman:0.9434\n",
      "Train ALL Pearson: 0.9669766076915465\n",
      "Train  ALL Spearman: 0.9560869038930172\n",
      "31.762667894363403\n",
      "Test Epoch:181 [(0%)]\t Loss: 0.1586  Pearson:0.9464 Spearman:0.9119\n",
      "Test : Loss:0.1645 \n",
      "pearson： 0.940065644603706 ALL Pearson: 0.9374250181967774\n",
      "spearman： 0.9206282808507728 ALL Spearman: 0.9250218347437751\n",
      "time: 36.5561306476593\n",
      "0.0008099999999999998\n",
      "Train Epoch:182 [(0%)]\t Loss: 0.1005  Pearson:0.9711 Spearman:0.9620\n",
      "Train ALL Pearson: 0.9671107687085457\n",
      "Train  ALL Spearman: 0.9564614606616202\n",
      "31.011359691619873\n",
      "Test Epoch:182 [(0%)]\t Loss: 0.1620  Pearson:0.9364 Spearman:0.9307\n",
      "Test : Loss:0.1649 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9364522069828557 ALL Pearson: 0.9373555558652377\n",
      "spearman： 0.9224812568931745 ALL Spearman: 0.9249528120336918\n",
      "time: 35.68386101722717\n",
      "0.0008099999999999998\n",
      "Train Epoch:183 [(0%)]\t Loss: 0.1086  Pearson:0.9667 Spearman:0.9473\n",
      "Train ALL Pearson: 0.9675807183502155\n",
      "Train  ALL Spearman: 0.9574449177805042\n",
      "31.02859377861023\n",
      "Test Epoch:183 [(0%)]\t Loss: 0.1476  Pearson:0.9483 Spearman:0.9331\n",
      "Test : Loss:0.1656 \n",
      "pearson： 0.9405124546761825 ALL Pearson: 0.9372887951466693\n",
      "spearman： 0.9274480131311763 ALL Spearman: 0.9249008296431208\n",
      "time: 35.71708869934082\n",
      "0.0008099999999999998\n",
      "Train Epoch:184 [(0%)]\t Loss: 0.1065  Pearson:0.9563 Spearman:0.9422\n",
      "Train ALL Pearson: 0.9659198993818071\n",
      "Train  ALL Spearman: 0.9552723717695328\n",
      "32.066728353500366\n",
      "Test Epoch:184 [(0%)]\t Loss: 0.1751  Pearson:0.9315 Spearman:0.9227\n",
      "Test : Loss:0.1659 \n",
      "pearson： 0.9369729380812685 ALL Pearson: 0.9373381035805434\n",
      "spearman： 0.926225254871607 ALL Spearman: 0.924941404377404\n",
      "time: 36.77621793746948\n",
      "0.0008099999999999998\n",
      "Train Epoch:185 [(0%)]\t Loss: 0.0982  Pearson:0.9749 Spearman:0.9592\n",
      "Train ALL Pearson: 0.9673761441018289\n",
      "Train  ALL Spearman: 0.9567133578542661\n",
      "31.728206634521484\n",
      "Test Epoch:185 [(0%)]\t Loss: 0.1600  Pearson:0.9436 Spearman:0.9192\n",
      "Test : Loss:0.1629 \n",
      "pearson： 0.940044194079161 ALL Pearson: 0.9372950212623458\n",
      "spearman： 0.9211924985854764 ALL Spearman: 0.924946447800888\n",
      "time: 36.62997651100159\n",
      "0.0008099999999999998\n",
      "Train Epoch:186 [(0%)]\t Loss: 0.1130  Pearson:0.9655 Spearman:0.9499\n",
      "Train ALL Pearson: 0.9670184581176583\n",
      "Train  ALL Spearman: 0.956897325296928\n",
      "31.575933933258057\n",
      "Test Epoch:186 [(0%)]\t Loss: 0.1757  Pearson:0.9155 Spearman:0.8955\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.9332307405815364 ALL Pearson: 0.9373441494421343\n",
      "spearman： 0.9196853113071509 ALL Spearman: 0.9249155909981507\n",
      "time: 36.29510712623596\n",
      "0.0008099999999999998\n",
      "Train Epoch:187 [(0%)]\t Loss: 0.1195  Pearson:0.9660 Spearman:0.9570\n",
      "Train ALL Pearson: 0.9668414386705075\n",
      "Train  ALL Spearman: 0.9563788162243813\n",
      "31.722289085388184\n",
      "Test Epoch:187 [(0%)]\t Loss: 0.1628  Pearson:0.9561 Spearman:0.9450\n",
      "Test : Loss:0.1653 \n",
      "pearson： 0.9401220396777675 ALL Pearson: 0.9373247808686106\n",
      "spearman： 0.9259696618003793 ALL Spearman: 0.9249310821168929\n",
      "time: 36.43877649307251\n",
      "0.0008099999999999998\n",
      "Train Epoch:188 [(0%)]\t Loss: 0.0958  Pearson:0.9708 Spearman:0.9583\n",
      "Train ALL Pearson: 0.9669933169288795\n",
      "Train  ALL Spearman: 0.9568004878187999\n",
      "31.493025064468384\n",
      "Test Epoch:188 [(0%)]\t Loss: 0.1699  Pearson:0.9359 Spearman:0.9284\n",
      "Test : Loss:0.1652 \n",
      "pearson： 0.9370952168834902 ALL Pearson: 0.9373379971723386\n",
      "spearman： 0.926651814536685 ALL Spearman: 0.9249249254293623\n",
      "time: 36.23150587081909\n",
      "0.0008099999999999998\n",
      "Train Epoch:189 [(0%)]\t Loss: 0.0987  Pearson:0.9775 Spearman:0.9645\n",
      "Train ALL Pearson: 0.9673973787278078\n",
      "Train  ALL Spearman: 0.957544316400546\n",
      "31.982481241226196\n",
      "Test Epoch:189 [(0%)]\t Loss: 0.1770  Pearson:0.9137 Spearman:0.8942\n",
      "Test : Loss:0.1645 \n",
      "pearson： 0.9325504475774449 ALL Pearson: 0.9372507576538255\n",
      "spearman： 0.9186907358243729 ALL Spearman: 0.9248213236691049\n",
      "time: 36.49403381347656\n",
      "0.0008099999999999998\n",
      "Train Epoch:190 [(0%)]\t Loss: 0.0987  Pearson:0.9715 Spearman:0.9619\n",
      "Train ALL Pearson: 0.9673748725304779\n",
      "Train  ALL Spearman: 0.9570376660667782\n",
      "31.894320964813232\n",
      "Test Epoch:190 [(0%)]\t Loss: 0.1750  Pearson:0.9299 Spearman:0.9296\n",
      "Test : Loss:0.1646 \n",
      "pearson： 0.9348790725357944 ALL Pearson: 0.9373082217151815\n",
      "spearman： 0.9244449944775962 ALL Spearman: 0.9248830683604293\n",
      "time: 36.53117609024048\n",
      "0.0008099999999999998\n",
      "Train Epoch:191 [(0%)]\t Loss: 0.1022  Pearson:0.9664 Spearman:0.9536\n",
      "Train ALL Pearson: 0.9673769875954927\n",
      "Train  ALL Spearman: 0.957240626779545\n",
      "31.588117599487305\n",
      "Test Epoch:191 [(0%)]\t Loss: 0.1784  Pearson:0.9233 Spearman:0.9007\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.9350223668006993 ALL Pearson: 0.9372620540505658\n",
      "spearman： 0.9196023295176409 ALL Spearman: 0.9247817803999153\n",
      "time: 36.31360054016113\n",
      "0.0008099999999999998\n",
      "Train Epoch:192 [(0%)]\t Loss: 0.1222  Pearson:0.9388 Spearman:0.9338\n",
      "Train ALL Pearson: 0.9669821143625499\n",
      "Train  ALL Spearman: 0.9571187415587498\n",
      "31.171858072280884\n",
      "Test Epoch:192 [(0%)]\t Loss: 0.1639  Pearson:0.9385 Spearman:0.9295\n",
      "Test : Loss:0.1638 \n",
      "pearson： 0.9374808902336421 ALL Pearson: 0.9372981189166538\n",
      "spearman： 0.927446893672387 ALL Spearman: 0.9248632686604525\n",
      "time: 36.03729844093323\n",
      "0.0008099999999999998\n",
      "Train Epoch:193 [(0%)]\t Loss: 0.0977  Pearson:0.9715 Spearman:0.9581\n",
      "Train ALL Pearson: 0.9680794769939107\n",
      "Train  ALL Spearman: 0.9587632629516515\n",
      "31.731526374816895\n",
      "Test Epoch:193 [(0%)]\t Loss: 0.1631  Pearson:0.9322 Spearman:0.9170\n",
      "Test : Loss:0.1632 \n",
      "pearson： 0.9358285217979696 ALL Pearson: 0.9372081129121794\n",
      "spearman： 0.9252673988001026 ALL Spearman: 0.924757687941445\n",
      "time: 36.68693733215332\n",
      "0.0008099999999999998\n",
      "Train Epoch:194 [(0%)]\t Loss: 0.1082  Pearson:0.9638 Spearman:0.9425\n",
      "Train ALL Pearson: 0.9668145290156628\n",
      "Train  ALL Spearman: 0.9560880556323514\n",
      "31.696630477905273\n",
      "Test Epoch:194 [(0%)]\t Loss: 0.1572  Pearson:0.9415 Spearman:0.9195\n",
      "Test : Loss:0.1635 \n",
      "pearson： 0.9368447215654677 ALL Pearson: 0.9371531509352972\n",
      "spearman： 0.9226903325386271 ALL Spearman: 0.9246823211112728\n",
      "time: 36.353137254714966\n",
      "0.0008099999999999998\n",
      "Train Epoch:195 [(0%)]\t Loss: 0.1012  Pearson:0.9644 Spearman:0.9618\n",
      "Train ALL Pearson: 0.9669900295133613\n",
      "Train  ALL Spearman: 0.9561034859353671\n",
      "32.01024508476257\n",
      "Test Epoch:195 [(0%)]\t Loss: 0.1579  Pearson:0.9364 Spearman:0.9072\n",
      "Test : Loss:0.1644 \n",
      "pearson： 0.9369191276706362 ALL Pearson: 0.9372234531659693\n",
      "spearman： 0.9205763800984534 ALL Spearman: 0.9247943523174305\n",
      "time: 36.911012172698975\n",
      "0.0008099999999999998\n",
      "Train Epoch:196 [(0%)]\t Loss: 0.1000  Pearson:0.9688 Spearman:0.9662\n",
      "Train ALL Pearson: 0.9669202450773021\n",
      "Train  ALL Spearman: 0.9566709430440359\n",
      "31.730756521224976\n",
      "Test Epoch:196 [(0%)]\t Loss: 0.1683  Pearson:0.9333 Spearman:0.9166\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9373801360089046 ALL Pearson: 0.9372313262859929\n",
      "spearman： 0.9221741343172302 ALL Spearman: 0.9247962854953139\n",
      "time: 36.514222145080566\n",
      "0.0008099999999999998\n",
      "Train Epoch:197 [(0%)]\t Loss: 0.1089  Pearson:0.9662 Spearman:0.9472\n",
      "Train ALL Pearson: 0.9673192266151406\n",
      "Train  ALL Spearman: 0.9569497961673397\n",
      "31.209609746932983\n",
      "Test Epoch:197 [(0%)]\t Loss: 0.1689  Pearson:0.9299 Spearman:0.9279\n",
      "Test : Loss:0.1663 \n",
      "pearson： 0.9341228082866754 ALL Pearson: 0.9371967387394108\n",
      "spearman： 0.921940588317616 ALL Spearman: 0.9247187677023456\n",
      "time: 36.14002728462219\n",
      "0.0008099999999999998\n",
      "Train Epoch:198 [(0%)]\t Loss: 0.1035  Pearson:0.9688 Spearman:0.9531\n",
      "Train ALL Pearson: 0.9674389178881211\n",
      "Train  ALL Spearman: 0.9573771116399361\n",
      "32.31961488723755\n",
      "Test Epoch:198 [(0%)]\t Loss: 0.1638  Pearson:0.9432 Spearman:0.9229\n",
      "Test : Loss:0.1635 \n",
      "pearson： 0.9406468538402325 ALL Pearson: 0.9372674841109155\n",
      "spearman： 0.9277149127539089 ALL Spearman: 0.9248276405716225\n",
      "time: 37.09211015701294\n",
      "0.0008099999999999998\n",
      "Train Epoch:199 [(0%)]\t Loss: 0.1061  Pearson:0.9719 Spearman:0.9560\n",
      "Train ALL Pearson: 0.9668184407778172\n",
      "Train  ALL Spearman: 0.9555467998283098\n",
      "31.691958904266357\n",
      "Test Epoch:199 [(0%)]\t Loss: 0.1871  Pearson:0.9334 Spearman:0.9356\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9368286011524473 ALL Pearson: 0.9372111196782873\n",
      "spearman： 0.929142545935084 ALL Spearman: 0.9248232722203077\n",
      "time: 36.298381090164185\n",
      "0.0008099999999999998\n",
      "Train Epoch:200 [(0%)]\t Loss: 0.0882  Pearson:0.9785 Spearman:0.9698\n",
      "Train ALL Pearson: 0.9673883624753641\n",
      "Train  ALL Spearman: 0.9570610337591134\n",
      "31.84941339492798\n",
      "Test Epoch:200 [(0%)]\t Loss: 0.1652  Pearson:0.9087 Spearman:0.8875\n",
      "Test : Loss:0.1632 \n",
      "pearson： 0.9318216642364565 ALL Pearson: 0.9372176318215448\n",
      "spearman： 0.9186953321796323 ALL Spearman: 0.9248489799935194\n",
      "time: 36.47775983810425\n",
      "0.0008099999999999998\n",
      "Train Epoch:201 [(0%)]\t Loss: 0.1014  Pearson:0.9703 Spearman:0.9587\n",
      "Train ALL Pearson: 0.9681482003909744\n",
      "Train  ALL Spearman: 0.9584744271894899\n",
      "31.434635162353516\n",
      "Test Epoch:201 [(0%)]\t Loss: 0.1611  Pearson:0.9402 Spearman:0.9309\n",
      "Test : Loss:0.1642 \n",
      "pearson： 0.9401706984820236 ALL Pearson: 0.9371911923840676\n",
      "spearman： 0.9281761939079813 ALL Spearman: 0.9248365595456319\n",
      "time: 36.22309851646423\n",
      "0.0008099999999999998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:202 [(0%)]\t Loss: 0.1026  Pearson:0.9651 Spearman:0.9469\n",
      "Train ALL Pearson: 0.9674405610264587\n",
      "Train  ALL Spearman: 0.9561858706509322\n",
      "31.871196031570435\n",
      "Test Epoch:202 [(0%)]\t Loss: 0.1773  Pearson:0.9367 Spearman:0.9164\n",
      "Test : Loss:0.1649 \n",
      "pearson： 0.93915303065233 ALL Pearson: 0.9371823550008689\n",
      "spearman： 0.9244408903025513 ALL Spearman: 0.9248003945983789\n",
      "time: 36.529866218566895\n",
      "0.00024299999999999994\n",
      "Train Epoch:203 [(0%)]\t Loss: 0.1079  Pearson:0.9654 Spearman:0.9371\n",
      "Train ALL Pearson: 0.9669409773516344\n",
      "Train  ALL Spearman: 0.9568719393343708\n",
      "31.3474383354187\n",
      "Test Epoch:203 [(0%)]\t Loss: 0.1570  Pearson:0.9462 Spearman:0.9271\n",
      "Test : Loss:0.1651 \n",
      "pearson： 0.9401530633668407 ALL Pearson: 0.9374189222912866\n",
      "spearman： 0.9244442446560414 ALL Spearman: 0.9249973018657116\n",
      "time: 36.41863965988159\n",
      "0.00024299999999999994\n",
      "Train Epoch:204 [(0%)]\t Loss: 0.1136  Pearson:0.9683 Spearman:0.9510\n",
      "Train ALL Pearson: 0.9673811796734749\n",
      "Train  ALL Spearman: 0.9575357376835718\n",
      "31.936867713928223\n",
      "Test Epoch:204 [(0%)]\t Loss: 0.1635  Pearson:0.9431 Spearman:0.9247\n",
      "Test : Loss:0.1641 \n",
      "pearson： 0.9395533196793822 ALL Pearson: 0.937418705534857\n",
      "spearman： 0.9261947085255676 ALL Spearman: 0.9250055336667921\n",
      "time: 36.78131556510925\n",
      "0.00024299999999999994\n",
      "Train Epoch:205 [(0%)]\t Loss: 0.1062  Pearson:0.9681 Spearman:0.9475\n",
      "Train ALL Pearson: 0.966887415652218\n",
      "Train  ALL Spearman: 0.9566274577707671\n",
      "31.4880268573761\n",
      "Test Epoch:205 [(0%)]\t Loss: 0.1631  Pearson:0.9269 Spearman:0.9257\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9364809677811947 ALL Pearson: 0.937413920369323\n",
      "spearman： 0.9245433174889574 ALL Spearman: 0.9250005555137298\n",
      "time: 36.17346906661987\n",
      "0.00024299999999999994\n",
      "Train Epoch:206 [(0%)]\t Loss: 0.1016  Pearson:0.9694 Spearman:0.9456\n",
      "Train ALL Pearson: 0.9677245408113471\n",
      "Train  ALL Spearman: 0.9578802394121921\n",
      "32.308571338653564\n",
      "Test Epoch:206 [(0%)]\t Loss: 0.1680  Pearson:0.9444 Spearman:0.9176\n",
      "Test : Loss:0.1640 \n",
      "pearson： 0.9354318771932589 ALL Pearson: 0.9374212237924904\n",
      "spearman： 0.9200931489236261 ALL Spearman: 0.9250359874246573\n",
      "time: 37.09603667259216\n",
      "0.00024299999999999994\n",
      "Train Epoch:207 [(0%)]\t Loss: 0.1161  Pearson:0.9580 Spearman:0.9513\n",
      "Train ALL Pearson: 0.9674636831964665\n",
      "Train  ALL Spearman: 0.9572046531972571\n",
      "31.606842041015625\n",
      "Test Epoch:207 [(0%)]\t Loss: 0.1654  Pearson:0.9388 Spearman:0.9198\n",
      "Test : Loss:0.1627 \n",
      "pearson： 0.9376570432159377 ALL Pearson: 0.9374066700945676\n",
      "spearman： 0.9245633576922327 ALL Spearman: 0.9250012873675012\n",
      "time: 36.59024477005005\n",
      "0.00024299999999999994\n",
      "Train Epoch:208 [(0%)]\t Loss: 0.1149  Pearson:0.9572 Spearman:0.9496\n",
      "Train ALL Pearson: 0.9673669978602307\n",
      "Train  ALL Spearman: 0.9571483226343429\n",
      "31.481422185897827\n",
      "Test Epoch:208 [(0%)]\t Loss: 0.1643  Pearson:0.9269 Spearman:0.9167\n",
      "Test : Loss:0.1652 \n",
      "pearson： 0.9375620602034477 ALL Pearson: 0.9373864517017055\n",
      "spearman： 0.9259134278698489 ALL Spearman: 0.9250146131949801\n",
      "time: 36.104939222335815\n",
      "0.00024299999999999994\n",
      "Train Epoch:209 [(0%)]\t Loss: 0.0978  Pearson:0.9669 Spearman:0.9684\n",
      "Train ALL Pearson: 0.9677756080261591\n",
      "Train  ALL Spearman: 0.9572097487842822\n",
      "31.927483558654785\n",
      "Test Epoch:209 [(0%)]\t Loss: 0.1577  Pearson:0.9464 Spearman:0.9348\n",
      "Test : Loss:0.1640 \n",
      "pearson： 0.9384999364679278 ALL Pearson: 0.9373741743088746\n",
      "spearman： 0.927718386940739 ALL Spearman: 0.9250084180327309\n",
      "time: 36.80562925338745\n",
      "0.00024299999999999994\n",
      "Train Epoch:210 [(0%)]\t Loss: 0.1098  Pearson:0.9558 Spearman:0.9552\n",
      "Train ALL Pearson: 0.9672572382759441\n",
      "Train  ALL Spearman: 0.9574106369819945\n",
      "31.641839504241943\n",
      "Test Epoch:210 [(0%)]\t Loss: 0.1651  Pearson:0.9332 Spearman:0.9210\n",
      "Test : Loss:0.1643 \n",
      "pearson： 0.9348422389927484 ALL Pearson: 0.9373710557596056\n",
      "spearman： 0.9223525519458043 ALL Spearman: 0.9249812842108691\n",
      "time: 36.24436354637146\n",
      "0.00024299999999999994\n",
      "Train Epoch:211 [(0%)]\t Loss: 0.1046  Pearson:0.9666 Spearman:0.9577\n",
      "Train ALL Pearson: 0.9670916809959127\n",
      "Train  ALL Spearman: 0.9564137501785479\n",
      "31.922985553741455\n",
      "Test Epoch:211 [(0%)]\t Loss: 0.1491  Pearson:0.9460 Spearman:0.9127\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9395982399469262 ALL Pearson: 0.9373754175029353\n",
      "spearman： 0.9243484433606253 ALL Spearman: 0.9249975761206554\n",
      "time: 36.681458711624146\n",
      "0.00024299999999999994\n",
      "Train Epoch:212 [(0%)]\t Loss: 0.1095  Pearson:0.9548 Spearman:0.9404\n",
      "Train ALL Pearson: 0.9670041691103954\n",
      "Train  ALL Spearman: 0.9556253112102532\n",
      "31.6813485622406\n",
      "Test Epoch:212 [(0%)]\t Loss: 0.1849  Pearson:0.9473 Spearman:0.9366\n",
      "Test : Loss:0.1650 \n",
      "pearson： 0.9395324261618749 ALL Pearson: 0.9373743623090021\n",
      "spearman： 0.9269767268927399 ALL Spearman: 0.9250026503734157\n",
      "time: 36.44082307815552\n",
      "0.00024299999999999994\n",
      "Train Epoch:213 [(0%)]\t Loss: 0.0962  Pearson:0.9820 Spearman:0.9717\n",
      "Train ALL Pearson: 0.9677275470903121\n",
      "Train  ALL Spearman: 0.9575821211100782\n",
      "31.55993938446045\n",
      "Test Epoch:213 [(0%)]\t Loss: 0.1801  Pearson:0.9316 Spearman:0.9247\n",
      "Test : Loss:0.1647 \n",
      "pearson： 0.93641496074367 ALL Pearson: 0.937357052512162\n",
      "spearman： 0.9245029599072592 ALL Spearman: 0.9249910392329205\n",
      "time: 36.26742935180664\n",
      "0.00024299999999999994\n",
      "Train Epoch:214 [(0%)]\t Loss: 0.1156  Pearson:0.9593 Spearman:0.9536\n",
      "Train ALL Pearson: 0.967404322988204\n",
      "Train  ALL Spearman: 0.9571753224172925\n",
      "31.85827136039734\n",
      "Test Epoch:214 [(0%)]\t Loss: 0.1717  Pearson:0.9293 Spearman:0.9449\n",
      "Test : Loss:0.1651 \n",
      "pearson： 0.935283599213657 ALL Pearson: 0.9373346181464761\n",
      "spearman： 0.9293187810663597 ALL Spearman: 0.9249828598956927\n",
      "time: 36.64028882980347\n",
      "0.00024299999999999994\n",
      "Train Epoch:215 [(0%)]\t Loss: 0.1044  Pearson:0.9659 Spearman:0.9433\n",
      "Train ALL Pearson: 0.9666815479049233\n",
      "Train  ALL Spearman: 0.9566713774975939\n",
      "32.14403986930847\n",
      "Test Epoch:215 [(0%)]\t Loss: 0.1676  Pearson:0.9396 Spearman:0.9200\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9390952380619048 ALL Pearson: 0.9373458591158494\n",
      "spearman： 0.92420461404065 ALL Spearman: 0.9249903270977807\n",
      "time: 36.87652230262756\n",
      "0.00024299999999999994\n",
      "Train Epoch:216 [(0%)]\t Loss: 0.1021  Pearson:0.9732 Spearman:0.9617\n",
      "Train ALL Pearson: 0.9672051602538988\n",
      "Train  ALL Spearman: 0.9569881156681048\n",
      "31.967856645584106\n",
      "Test Epoch:216 [(0%)]\t Loss: 0.1493  Pearson:0.9366 Spearman:0.9373\n",
      "Test : Loss:0.1642 \n",
      "pearson： 0.9346859830558126 ALL Pearson: 0.9373473101532849\n",
      "spearman： 0.9247554099800919 ALL Spearman: 0.9249635797923191\n",
      "time: 36.61936402320862\n",
      "0.00024299999999999994\n",
      "Train Epoch:217 [(0%)]\t Loss: 0.1054  Pearson:0.9679 Spearman:0.9499\n",
      "Train ALL Pearson: 0.966908143669918\n",
      "Train  ALL Spearman: 0.956015817915947\n",
      "32.21704602241516\n",
      "Test Epoch:217 [(0%)]\t Loss: 0.1617  Pearson:0.9334 Spearman:0.8917\n",
      "Test : Loss:0.1645 \n",
      "pearson： 0.9358101406456213 ALL Pearson: 0.9373671991982938\n",
      "spearman： 0.9171002892691881 ALL Spearman: 0.9249695674643063\n",
      "time: 37.15746212005615\n",
      "0.00024299999999999994\n",
      "Train Epoch:218 [(0%)]\t Loss: 0.1162  Pearson:0.9673 Spearman:0.9531\n",
      "Train ALL Pearson: 0.9673150262837068\n",
      "Train  ALL Spearman: 0.9571643394836542\n",
      "31.635979175567627\n",
      "Test Epoch:218 [(0%)]\t Loss: 0.1660  Pearson:0.9385 Spearman:0.9304\n",
      "Test : Loss:0.1647 \n",
      "pearson： 0.9374663671237221 ALL Pearson: 0.9373775360632746\n",
      "spearman： 0.9254034931316607 ALL Spearman: 0.9249742992031922\n",
      "time: 36.45943236351013\n",
      "0.00024299999999999994\n",
      "Train Epoch:219 [(0%)]\t Loss: 0.0933  Pearson:0.9750 Spearman:0.9730\n",
      "Train ALL Pearson: 0.9674743068172339\n",
      "Train  ALL Spearman: 0.957105161781771\n",
      "31.888718843460083\n",
      "Test Epoch:219 [(0%)]\t Loss: 0.1766  Pearson:0.9337 Spearman:0.9189\n",
      "Test : Loss:0.1634 \n",
      "pearson： 0.9386300889538297 ALL Pearson: 0.9373686188742775\n",
      "spearman： 0.9272213249882925 ALL Spearman: 0.924975424201247\n",
      "time: 36.46579718589783\n",
      "0.00024299999999999994\n",
      "Train Epoch:220 [(0%)]\t Loss: 0.1092  Pearson:0.9617 Spearman:0.9421\n",
      "Train ALL Pearson: 0.9672483999871434\n",
      "Train  ALL Spearman: 0.9573131155738674\n",
      "31.82060670852661\n",
      "Test Epoch:220 [(0%)]\t Loss: 0.1699  Pearson:0.9346 Spearman:0.9262\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.9350946091513257 ALL Pearson: 0.9373645004880815\n",
      "spearman： 0.9239388603213515 ALL Spearman: 0.9250108216435156\n",
      "time: 36.78271675109863\n",
      "0.00024299999999999994\n",
      "Train Epoch:221 [(0%)]\t Loss: 0.1030  Pearson:0.9669 Spearman:0.9617\n",
      "Train ALL Pearson: 0.9670943107381381\n",
      "Train  ALL Spearman: 0.9566313053256483\n",
      "31.213512420654297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:221 [(0%)]\t Loss: 0.1709  Pearson:0.9310 Spearman:0.8958\n",
      "Test : Loss:0.1637 \n",
      "pearson： 0.9396528552844973 ALL Pearson: 0.9373624612947362\n",
      "spearman： 0.9232022203768754 ALL Spearman: 0.925010265405678\n",
      "time: 35.84315776824951\n",
      "0.00024299999999999994\n",
      "Train Epoch:222 [(0%)]\t Loss: 0.1038  Pearson:0.9745 Spearman:0.9627\n",
      "Train ALL Pearson: 0.967456327729888\n",
      "Train  ALL Spearman: 0.9574175639273198\n",
      "31.36072278022766\n",
      "Test Epoch:222 [(0%)]\t Loss: 0.1472  Pearson:0.9552 Spearman:0.9445\n",
      "Test : Loss:0.1647 \n",
      "pearson： 0.9410162278627435 ALL Pearson: 0.9373588723656298\n",
      "spearman： 0.9296910905331388 ALL Spearman: 0.924996310002817\n",
      "time: 36.000235080718994\n",
      "0.00024299999999999994\n",
      "Train Epoch:223 [(0%)]\t Loss: 0.1135  Pearson:0.9669 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9672365644929699\n",
      "Train  ALL Spearman: 0.9566239643324743\n",
      "32.10023641586304\n",
      "Test Epoch:223 [(0%)]\t Loss: 0.1641  Pearson:0.9350 Spearman:0.9280\n",
      "Test : Loss:0.1645 \n",
      "pearson： 0.9389592377908168 ALL Pearson: 0.9373662051363789\n",
      "spearman： 0.9277535590304972 ALL Spearman: 0.9250325357927488\n",
      "time: 37.104631662368774\n",
      "7.289999999999998e-05\n",
      "Train Epoch:224 [(0%)]\t Loss: 0.1036  Pearson:0.9673 Spearman:0.9575\n",
      "Train ALL Pearson: 0.9675443994798394\n",
      "Train  ALL Spearman: 0.9574994532282204\n",
      "33.094155073165894\n",
      "Test Epoch:224 [(0%)]\t Loss: 0.1599  Pearson:0.9493 Spearman:0.9371\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9396036736313157 ALL Pearson: 0.9374176371468851\n",
      "spearman： 0.9262343511917713 ALL Spearman: 0.9250242541496322\n",
      "time: 37.77373790740967\n",
      "7.289999999999998e-05\n",
      "Train Epoch:225 [(0%)]\t Loss: 0.1098  Pearson:0.9672 Spearman:0.9575\n",
      "Train ALL Pearson: 0.9672774416659232\n",
      "Train  ALL Spearman: 0.955976205246871\n",
      "32.210944414138794\n",
      "Test Epoch:225 [(0%)]\t Loss: 0.1729  Pearson:0.9327 Spearman:0.9057\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9365665776409632 ALL Pearson: 0.9374031682461739\n",
      "spearman： 0.9239951980780433 ALL Spearman: 0.9250125699832701\n",
      "time: 37.014405250549316\n",
      "7.289999999999998e-05\n",
      "Train Epoch:226 [(0%)]\t Loss: 0.1021  Pearson:0.9675 Spearman:0.9569\n",
      "Train ALL Pearson: 0.9672214177891574\n",
      "Train  ALL Spearman: 0.95672504410276\n",
      "31.752944469451904\n",
      "Test Epoch:226 [(0%)]\t Loss: 0.1660  Pearson:0.9396 Spearman:0.9262\n",
      "Test : Loss:0.1655 \n",
      "pearson： 0.939233930923976 ALL Pearson: 0.9374003064395008\n",
      "spearman： 0.9263071082602088 ALL Spearman: 0.9250024549563391\n",
      "time: 36.62938046455383\n",
      "7.289999999999998e-05\n",
      "Train Epoch:227 [(0%)]\t Loss: 0.1011  Pearson:0.9638 Spearman:0.9548\n",
      "Train ALL Pearson: 0.9670888289874412\n",
      "Train  ALL Spearman: 0.9566518975894396\n",
      "31.76935601234436\n",
      "Test Epoch:227 [(0%)]\t Loss: 0.1864  Pearson:0.9296 Spearman:0.9283\n",
      "Test : Loss:0.1650 \n",
      "pearson： 0.9338106372113589 ALL Pearson: 0.9373965959436046\n",
      "spearman： 0.9249044988781584 ALL Spearman: 0.9249819079742615\n",
      "time: 36.331891775131226\n",
      "7.289999999999998e-05\n",
      "Train Epoch:228 [(0%)]\t Loss: 0.1174  Pearson:0.9609 Spearman:0.9368\n",
      "Train ALL Pearson: 0.9672991630731486\n",
      "Train  ALL Spearman: 0.9576402145125866\n",
      "33.199427366256714\n",
      "Test Epoch:228 [(0%)]\t Loss: 0.1616  Pearson:0.9385 Spearman:0.9247\n",
      "Test : Loss:0.1652 \n",
      "pearson： 0.9394134569686216 ALL Pearson: 0.937389512774454\n",
      "spearman： 0.9272199250799229 ALL Spearman: 0.9249784112397595\n",
      "time: 37.972896099090576\n",
      "7.289999999999998e-05\n",
      "Train Epoch:229 [(0%)]\t Loss: 0.1103  Pearson:0.9588 Spearman:0.9445\n",
      "Train ALL Pearson: 0.9674125878457221\n",
      "Train  ALL Spearman: 0.9571523092558603\n",
      "32.53211760520935\n",
      "Test Epoch:229 [(0%)]\t Loss: 0.1663  Pearson:0.9401 Spearman:0.9165\n",
      "Test : Loss:0.1646 \n",
      "pearson： 0.9371886035534721 ALL Pearson: 0.9373857391887602\n",
      "spearman： 0.9209934647769632 ALL Spearman: 0.9249761406541137\n",
      "time: 37.07565999031067\n",
      "7.289999999999998e-05\n",
      "Train Epoch:230 [(0%)]\t Loss: 0.1000  Pearson:0.9751 Spearman:0.9534\n",
      "Train ALL Pearson: 0.9667731114341401\n",
      "Train  ALL Spearman: 0.9559857725248522\n",
      "32.80634832382202\n",
      "Test Epoch:230 [(0%)]\t Loss: 0.1614  Pearson:0.9246 Spearman:0.9126\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.936564687246619 ALL Pearson: 0.9373740186176642\n",
      "spearman： 0.9238409674097956 ALL Spearman: 0.9249451885951119\n",
      "time: 37.39687705039978\n",
      "7.289999999999998e-05\n",
      "Train Epoch:231 [(0%)]\t Loss: 0.1131  Pearson:0.9642 Spearman:0.9584\n",
      "Train ALL Pearson: 0.9666884893190391\n",
      "Train  ALL Spearman: 0.9562279633559203\n",
      "32.086655616760254\n",
      "Test Epoch:231 [(0%)]\t Loss: 0.1468  Pearson:0.9424 Spearman:0.9331\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.9375988134996686 ALL Pearson: 0.9373709599698725\n",
      "spearman： 0.927020820437622 ALL Spearman: 0.9249290330106211\n",
      "time: 37.072057485580444\n",
      "7.289999999999998e-05\n",
      "Train Epoch:232 [(0%)]\t Loss: 0.0938  Pearson:0.9779 Spearman:0.9670\n",
      "Train ALL Pearson: 0.967067246132526\n",
      "Train  ALL Spearman: 0.9568262357890868\n",
      "32.72358465194702\n",
      "Test Epoch:232 [(0%)]\t Loss: 0.1653  Pearson:0.9451 Spearman:0.9280\n",
      "Test : Loss:0.1647 \n",
      "pearson： 0.941685599726709 ALL Pearson: 0.9373709330066627\n",
      "spearman： 0.9280389630083836 ALL Spearman: 0.9249395599104353\n",
      "time: 37.24821996688843\n",
      "7.289999999999998e-05\n",
      "Train Epoch:233 [(0%)]\t Loss: 0.0974  Pearson:0.9724 Spearman:0.9615\n",
      "Train ALL Pearson: 0.9675675962757405\n",
      "Train  ALL Spearman: 0.9581701294825923\n",
      "33.29704475402832\n",
      "Test Epoch:233 [(0%)]\t Loss: 0.1596  Pearson:0.9367 Spearman:0.9097\n",
      "Test : Loss:0.1637 \n",
      "pearson： 0.9358255162189423 ALL Pearson: 0.9373629566801682\n",
      "spearman： 0.9195649440398735 ALL Spearman: 0.9249295750301895\n",
      "time: 37.991095542907715\n",
      "7.289999999999998e-05\n",
      "Train Epoch:234 [(0%)]\t Loss: 0.0951  Pearson:0.9729 Spearman:0.9637\n",
      "Train ALL Pearson: 0.9671553444795346\n",
      "Train  ALL Spearman: 0.9573286096412846\n",
      "33.4920392036438\n",
      "Test Epoch:234 [(0%)]\t Loss: 0.1690  Pearson:0.9291 Spearman:0.9017\n",
      "Test : Loss:0.1649 \n",
      "pearson： 0.937065509174267 ALL Pearson: 0.937360915867086\n",
      "spearman： 0.9194698848425038 ALL Spearman: 0.9249316908185785\n",
      "time: 38.35448670387268\n",
      "7.289999999999998e-05\n",
      "Train Epoch:235 [(0%)]\t Loss: 0.1149  Pearson:0.9498 Spearman:0.9369\n",
      "Train ALL Pearson: 0.9677380789016753\n",
      "Train  ALL Spearman: 0.9567833653522566\n",
      "33.12745928764343\n",
      "Test Epoch:235 [(0%)]\t Loss: 0.1613  Pearson:0.9289 Spearman:0.9177\n",
      "Test : Loss:0.1638 \n",
      "pearson： 0.9373202675427916 ALL Pearson: 0.9373624480888159\n",
      "spearman： 0.9258191533918175 ALL Spearman: 0.9249341178973517\n",
      "time: 37.683998107910156\n",
      "7.289999999999998e-05\n",
      "Train Epoch:236 [(0%)]\t Loss: 0.1013  Pearson:0.9742 Spearman:0.9651\n",
      "Train ALL Pearson: 0.966961967252711\n",
      "Train  ALL Spearman: 0.9565219783596884\n",
      "34.07857346534729\n",
      "Test Epoch:236 [(0%)]\t Loss: 0.1680  Pearson:0.9366 Spearman:0.9155\n",
      "Test : Loss:0.1636 \n",
      "pearson： 0.937694173084111 ALL Pearson: 0.9373701346855715\n",
      "spearman： 0.9260354064252845 ALL Spearman: 0.9249355652139505\n",
      "time: 39.096964836120605\n",
      "7.289999999999998e-05\n",
      "Train Epoch:237 [(0%)]\t Loss: 0.1059  Pearson:0.9698 Spearman:0.9617\n",
      "Train ALL Pearson: 0.9669520534587529\n",
      "Train  ALL Spearman: 0.9568904189662042\n",
      "32.44821500778198\n",
      "Test Epoch:237 [(0%)]\t Loss: 0.1575  Pearson:0.9432 Spearman:0.9255\n",
      "Test : Loss:0.1640 \n",
      "pearson： 0.9352335533782864 ALL Pearson: 0.9373628746282672\n",
      "spearman： 0.921658188982127 ALL Spearman: 0.9249353129602952\n",
      "time: 37.156014919281006\n",
      "7.289999999999998e-05\n",
      "Train Epoch:238 [(0%)]\t Loss: 0.1041  Pearson:0.9526 Spearman:0.9495\n",
      "Train ALL Pearson: 0.9667231640393984\n",
      "Train  ALL Spearman: 0.9561265372320509\n",
      "32.50641965866089\n",
      "Test Epoch:238 [(0%)]\t Loss: 0.1634  Pearson:0.9262 Spearman:0.9190\n",
      "Test : Loss:0.1641 \n",
      "pearson： 0.9326241811581508 ALL Pearson: 0.937361401845837\n",
      "spearman： 0.9212427446499533 ALL Spearman: 0.9249317172200975\n",
      "time: 37.01897192001343\n",
      "7.289999999999998e-05\n",
      "Train Epoch:239 [(0%)]\t Loss: 0.1062  Pearson:0.9705 Spearman:0.9585\n",
      "Train ALL Pearson: 0.9670158036712482\n",
      "Train  ALL Spearman: 0.9570539206737066\n",
      "34.13021159172058\n",
      "Test Epoch:239 [(0%)]\t Loss: 0.1673  Pearson:0.9349 Spearman:0.9196\n",
      "Test : Loss:0.1638 \n",
      "pearson： 0.9366844967695024 ALL Pearson: 0.9373645156354108\n",
      "spearman： 0.925623756413533 ALL Spearman: 0.9249311327198042\n",
      "time: 38.93967008590698\n",
      "7.289999999999998e-05\n",
      "Train Epoch:240 [(0%)]\t Loss: 0.1120  Pearson:0.9537 Spearman:0.9552\n",
      "Train ALL Pearson: 0.9673245267262502\n",
      "Train  ALL Spearman: 0.9567763951275113\n",
      "32.58524560928345\n",
      "Test Epoch:240 [(0%)]\t Loss: 0.1675  Pearson:0.9545 Spearman:0.9313\n",
      "Test : Loss:0.1658 \n",
      "pearson： 0.9397188652034018 ALL Pearson: 0.9373601688362746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9230062844605491 ALL Spearman: 0.9249364951616412\n",
      "time: 37.38770580291748\n",
      "7.289999999999998e-05\n",
      "Train Epoch:241 [(0%)]\t Loss: 0.0991  Pearson:0.9725 Spearman:0.9595\n",
      "Train ALL Pearson: 0.9667189798187624\n",
      "Train  ALL Spearman: 0.9556675109418907\n",
      "33.76531481742859\n",
      "Test Epoch:241 [(0%)]\t Loss: 0.1736  Pearson:0.9347 Spearman:0.9267\n",
      "Test : Loss:0.1638 \n",
      "pearson： 0.9371660560925467 ALL Pearson: 0.9373648949821242\n",
      "spearman： 0.9240392187695334 ALL Spearman: 0.9249275355128551\n",
      "time: 38.5547776222229\n",
      "7.289999999999998e-05\n",
      "Train Epoch:242 [(0%)]\t Loss: 0.1011  Pearson:0.9566 Spearman:0.9525\n",
      "Train ALL Pearson: 0.9677085936325338\n",
      "Train  ALL Spearman: 0.9580663926351196\n",
      "33.45802855491638\n",
      "Test Epoch:242 [(0%)]\t Loss: 0.1767  Pearson:0.9420 Spearman:0.8933\n",
      "Test : Loss:0.1645 \n",
      "pearson： 0.9387648117406429 ALL Pearson: 0.9373636644225591\n",
      "spearman： 0.9190692235248012 ALL Spearman: 0.9249333328188521\n",
      "time: 38.235405921936035\n",
      "7.289999999999998e-05\n",
      "Train Epoch:243 [(0%)]\t Loss: 0.1074  Pearson:0.9698 Spearman:0.9491\n",
      "Train ALL Pearson: 0.9668777412127232\n",
      "Train  ALL Spearman: 0.956768200264125\n",
      "32.44355320930481\n",
      "Test Epoch:243 [(0%)]\t Loss: 0.1702  Pearson:0.9199 Spearman:0.9102\n",
      "Test : Loss:0.1644 \n",
      "pearson： 0.9370993702885255 ALL Pearson: 0.9373618719764621\n",
      "spearman： 0.9239725264291913 ALL Spearman: 0.9249509617272429\n",
      "time: 36.98398780822754\n",
      "7.289999999999998e-05\n",
      "Train Epoch:244 [(0%)]\t Loss: 0.1072  Pearson:0.9728 Spearman:0.9500\n",
      "Train ALL Pearson: 0.9673742354137942\n",
      "Train  ALL Spearman: 0.9566855258223624\n",
      "32.51516842842102\n",
      "Test Epoch:244 [(0%)]\t Loss: 0.1527  Pearson:0.9411 Spearman:0.9243\n",
      "Test : Loss:0.1639 \n",
      "pearson： 0.9381081957132942 ALL Pearson: 0.9373639960143118\n",
      "spearman： 0.9233813670878367 ALL Spearman: 0.9249554463185768\n",
      "time: 37.3276252746582\n",
      "Test Epoch:-1 [(0%)]\t Loss: 0.1604  Pearson:0.9457 Spearman:0.9370\n",
      "Test : Loss:0.1648 \n",
      "pearson： 0.940184759452483 ALL Pearson: 0.9374250181967769\n",
      "spearman： 0.931244361039957 ALL Spearman: 0.9250218347437751\n",
      "PLCC: [0.9390283689521908, 0.9380595102191237, 0.9333994268588421, 0.9374250181967769] SRCC: [0.9231706745873007, 0.9249109784651682, 0.9185469034467663, 0.9250218347437751]\n",
      "Split: 3 Median PLCC: 0.9377422642079503 SRCC: 0.9240408265262345\n",
      "Test Epoch:-1 [(0%)]\t Loss: 3.1462  Pearson:-0.2226 Spearman:-0.2083\n",
      "Test : Loss:3.1285 \n",
      "pearson： -0.1287100891787172 ALL Pearson: -0.1007690102207116\n",
      "spearman： -0.1232492554318013 ALL Spearman: -0.0956824206613411\n",
      "0.01\n",
      "Train Epoch:0 [(0%)]\t Loss: 3.1121  Pearson:0.0119 Spearman:-0.0182\n",
      "Train ALL Pearson: -0.023911035681063994\n",
      "Train  ALL Spearman: -0.021257547216456143\n",
      "20.080562114715576\n",
      "Test Epoch:0 [(0%)]\t Loss: 2.6122  Pearson:-0.2484 Spearman:-0.2376\n",
      "Test : Loss:2.6007 \n",
      "pearson： -0.15818579036438066 ALL Pearson: -0.13549128898841856\n",
      "spearman： -0.1517139233880487 ALL Spearman: -0.13293905114181695\n",
      "time: 24.504144430160522\n",
      "0.01\n",
      "Train Epoch:1 [(0%)]\t Loss: 2.5922  Pearson:-0.0941 Spearman:-0.0554\n",
      "Train ALL Pearson: -0.06744833723400338\n",
      "Train  ALL Spearman: -0.05065899844519721\n",
      "20.163238525390625\n",
      "Test Epoch:1 [(0%)]\t Loss: 1.1721  Pearson:-0.2242 Spearman:-0.1946\n",
      "Test : Loss:1.1997 \n",
      "pearson： -0.2471803133558512 ALL Pearson: -0.2393696818549747\n",
      "spearman： -0.22690432340563224 ALL Spearman: -0.22871436712449034\n",
      "time: 25.074249267578125\n",
      "0.01\n",
      "Train Epoch:2 [(0%)]\t Loss: 1.0135  Pearson:-0.0317 Spearman:-0.0481\n",
      "Train ALL Pearson: 0.03491181289430643\n",
      "Train  ALL Spearman: 0.0213307320986704\n",
      "19.81065011024475\n",
      "Test Epoch:2 [(0%)]\t Loss: 0.5714  Pearson:0.4071 Spearman:0.3646\n",
      "Test : Loss:0.5667 \n",
      "pearson： 0.35803432926066 ALL Pearson: 0.35355530807873053\n",
      "spearman： 0.3290705835359218 ALL Spearman: 0.32592665666674564\n",
      "time: 24.5207200050354\n",
      "0.01\n",
      "Train Epoch:3 [(0%)]\t Loss: 0.5794  Pearson:0.1839 Spearman:0.1708\n",
      "Train ALL Pearson: 0.37058028949737204\n",
      "Train  ALL Spearman: 0.3473689458609089\n",
      "20.29549479484558\n",
      "Test Epoch:3 [(0%)]\t Loss: 0.4667  Pearson:0.6562 Spearman:0.6186\n",
      "Test : Loss:0.5044 \n",
      "pearson： 0.5847773285285585 ALL Pearson: 0.5735842019566044\n",
      "spearman： 0.5418945351696489 ALL Spearman: 0.5336676599144486\n",
      "time: 25.168932914733887\n",
      "0.01\n",
      "Train Epoch:4 [(0%)]\t Loss: 0.4235  Pearson:0.4370 Spearman:0.3647\n",
      "Train ALL Pearson: 0.4945192926438917\n",
      "Train  ALL Spearman: 0.4688196042035152\n",
      "20.134079694747925\n",
      "Test Epoch:4 [(0%)]\t Loss: 0.4544  Pearson:0.6269 Spearman:0.6025\n",
      "Test : Loss:0.4774 \n",
      "pearson： 0.6424450181428821 ALL Pearson: 0.648644381081009\n",
      "spearman： 0.6062575048433243 ALL Spearman: 0.6090652784670774\n",
      "time: 24.742602586746216\n",
      "0.01\n",
      "Train Epoch:5 [(0%)]\t Loss: 0.4254  Pearson:0.5323 Spearman:0.5063\n",
      "Train ALL Pearson: 0.5592797077589722\n",
      "Train  ALL Spearman: 0.5259714929583519\n",
      "20.106099367141724\n",
      "Test Epoch:5 [(0%)]\t Loss: 0.4811  Pearson:0.6125 Spearman:0.6072\n",
      "Test : Loss:0.4711 \n",
      "pearson： 0.6692291803752505 ALL Pearson: 0.6872805697394052\n",
      "spearman： 0.6415759494210583 ALL Spearman: 0.6522530285250061\n",
      "time: 25.127489805221558\n",
      "0.01\n",
      "Train Epoch:6 [(0%)]\t Loss: 0.3692  Pearson:0.5897 Spearman:0.5841\n",
      "Train ALL Pearson: 0.5875693086648824\n",
      "Train  ALL Spearman: 0.5576188429792814\n",
      "19.7636239528656\n",
      "Test Epoch:6 [(0%)]\t Loss: 0.4602  Pearson:0.7027 Spearman:0.6652\n",
      "Test : Loss:0.4610 \n",
      "pearson： 0.7159180778494308 ALL Pearson: 0.7068938570918636\n",
      "spearman： 0.6851608575735444 ALL Spearman: 0.6761058227780916\n",
      "time: 24.65125846862793\n",
      "0.01\n",
      "Train Epoch:7 [(0%)]\t Loss: 0.3378  Pearson:0.6649 Spearman:0.6377\n",
      "Train ALL Pearson: 0.6123109449861195\n",
      "Train  ALL Spearman: 0.5847865196147437\n",
      "20.02420139312744\n",
      "Test Epoch:7 [(0%)]\t Loss: 0.4904  Pearson:0.7190 Spearman:0.6708\n",
      "Test : Loss:0.4876 \n",
      "pearson： 0.7106187890090311 ALL Pearson: 0.7197359797309892\n",
      "spearman： 0.6733171328958135 ALL Spearman: 0.6904178139780186\n",
      "time: 24.701701402664185\n",
      "0.01\n",
      "Train Epoch:8 [(0%)]\t Loss: 0.4251  Pearson:0.4895 Spearman:0.4493\n",
      "Train ALL Pearson: 0.623358257608573\n",
      "Train  ALL Spearman: 0.5981878283240697\n",
      "20.23178458213806\n",
      "Test Epoch:8 [(0%)]\t Loss: 0.4788  Pearson:0.7309 Spearman:0.6848\n",
      "Test : Loss:0.4623 \n",
      "pearson： 0.720142658349501 ALL Pearson: 0.7292726810379145\n",
      "spearman： 0.6815083114049523 ALL Spearman: 0.6991672049933884\n",
      "time: 25.060237646102905\n",
      "0.01\n",
      "Train Epoch:9 [(0%)]\t Loss: 0.3528  Pearson:0.6283 Spearman:0.6451\n",
      "Train ALL Pearson: 0.6320028323654033\n",
      "Train  ALL Spearman: 0.6061860598660497\n",
      "20.01820707321167\n",
      "Test Epoch:9 [(0%)]\t Loss: 0.4293  Pearson:0.7307 Spearman:0.6571\n",
      "Test : Loss:0.4471 \n",
      "pearson： 0.740274092820804 ALL Pearson: 0.7367913870488003\n",
      "spearman： 0.7046950463418192 ALL Spearman: 0.7051107345770482\n",
      "time: 24.61473250389099\n",
      "0.01\n",
      "Train Epoch:10 [(0%)]\t Loss: 0.3617  Pearson:0.6124 Spearman:0.5548\n",
      "Train ALL Pearson: 0.6479219219860924\n",
      "Train  ALL Spearman: 0.6129307420976465\n",
      "20.452029705047607\n",
      "Test Epoch:10 [(0%)]\t Loss: 0.4517  Pearson:0.7076 Spearman:0.6646\n",
      "Test : Loss:0.4311 \n",
      "pearson： 0.7380096909064254 ALL Pearson: 0.7461647923006757\n",
      "spearman： 0.7069491655545598 ALL Spearman: 0.7179648889539263\n",
      "time: 25.255817651748657\n",
      "0.001\n",
      "Train Epoch:11 [(0%)]\t Loss: 0.3265  Pearson:0.6807 Spearman:0.6533\n",
      "Train ALL Pearson: 0.6578618886059121\n",
      "Train  ALL Spearman: 0.6308464703152397\n",
      "31.682963371276855\n",
      "Test Epoch:11 [(0%)]\t Loss: 0.4874  Pearson:0.8055 Spearman:0.7730\n",
      "Test : Loss:0.4490 \n",
      "pearson： 0.76536598029608 ALL Pearson: 0.7544920479465126\n",
      "spearman： 0.7412318298446382 ALL Spearman: 0.7282112562100745\n",
      "time: 36.518954038619995\n",
      "0.001\n",
      "Train Epoch:12 [(0%)]\t Loss: 0.3378  Pearson:0.6620 Spearman:0.6688\n",
      "Train ALL Pearson: 0.669041086472789\n",
      "Train  ALL Spearman: 0.6419972008850473\n",
      "32.39819860458374\n",
      "Test Epoch:12 [(0%)]\t Loss: 0.4659  Pearson:0.7707 Spearman:0.7797\n",
      "Test : Loss:0.4437 \n",
      "pearson： 0.7575559479308115 ALL Pearson: 0.7635234531724354\n",
      "spearman： 0.7462475601270188 ALL Spearman: 0.7389434419325269\n",
      "time: 37.14501118659973\n",
      "0.001\n",
      "Train Epoch:13 [(0%)]\t Loss: 0.3159  Pearson:0.7030 Spearman:0.6682\n",
      "Train ALL Pearson: 0.6843841350507842\n",
      "Train  ALL Spearman: 0.6581381766084259\n",
      "31.649441719055176\n",
      "Test Epoch:13 [(0%)]\t Loss: 0.4549  Pearson:0.7625 Spearman:0.7271\n",
      "Test : Loss:0.4392 \n",
      "pearson： 0.7725911799764262 ALL Pearson: 0.7703425123391032\n",
      "spearman： 0.7464509648640423 ALL Spearman: 0.7469351470301946\n",
      "time: 36.39891839027405\n",
      "0.001\n",
      "Train Epoch:14 [(0%)]\t Loss: 0.3389  Pearson:0.6907 Spearman:0.6697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson: 0.6914690476421688\n",
      "Train  ALL Spearman: 0.6725236946312558\n",
      "31.667569875717163\n",
      "Test Epoch:14 [(0%)]\t Loss: 0.4565  Pearson:0.7638 Spearman:0.7366\n",
      "Test : Loss:0.4407 \n",
      "pearson： 0.7693548922743916 ALL Pearson: 0.7757940843721135\n",
      "spearman： 0.7431928175453346 ALL Spearman: 0.7538027356186884\n",
      "time: 36.58499312400818\n",
      "0.001\n",
      "Train Epoch:15 [(0%)]\t Loss: 0.3090  Pearson:0.7210 Spearman:0.7304\n",
      "Train ALL Pearson: 0.7043213313539713\n",
      "Train  ALL Spearman: 0.6848291264476141\n",
      "31.154767751693726\n",
      "Test Epoch:15 [(0%)]\t Loss: 0.4325  Pearson:0.7255 Spearman:0.6899\n",
      "Test : Loss:0.4327 \n",
      "pearson： 0.7761235551553276 ALL Pearson: 0.7815156361471738\n",
      "spearman： 0.7517060297102482 ALL Spearman: 0.7600915988061816\n",
      "time: 35.88686943054199\n",
      "0.001\n",
      "Train Epoch:16 [(0%)]\t Loss: 0.3087  Pearson:0.7397 Spearman:0.7566\n",
      "Train ALL Pearson: 0.7126692021379123\n",
      "Train  ALL Spearman: 0.6876908834833744\n",
      "31.659395456314087\n",
      "Test Epoch:16 [(0%)]\t Loss: 0.4113  Pearson:0.7985 Spearman:0.7479\n",
      "Test : Loss:0.4359 \n",
      "pearson： 0.7934026544527785 ALL Pearson: 0.7858195160046779\n",
      "spearman： 0.7601504735338498 ALL Spearman: 0.765125881824766\n",
      "time: 36.39387607574463\n",
      "0.001\n",
      "Train Epoch:17 [(0%)]\t Loss: 0.3288  Pearson:0.6432 Spearman:0.5639\n",
      "Train ALL Pearson: 0.7162340901566799\n",
      "Train  ALL Spearman: 0.6970406056982006\n",
      "31.59405779838562\n",
      "Test Epoch:17 [(0%)]\t Loss: 0.4416  Pearson:0.7630 Spearman:0.7535\n",
      "Test : Loss:0.4228 \n",
      "pearson： 0.786142588163698 ALL Pearson: 0.7913078647737082\n",
      "spearman： 0.7662343369854433 ALL Spearman: 0.7719053986153429\n",
      "time: 36.29055213928223\n",
      "0.001\n",
      "Train Epoch:18 [(0%)]\t Loss: 0.3537  Pearson:0.6681 Spearman:0.6433\n",
      "Train ALL Pearson: 0.7244026934107342\n",
      "Train  ALL Spearman: 0.7065859774770074\n",
      "31.62842631340027\n",
      "Test Epoch:18 [(0%)]\t Loss: 0.4344  Pearson:0.8205 Spearman:0.7766\n",
      "Test : Loss:0.4227 \n",
      "pearson： 0.8047689394045231 ALL Pearson: 0.795232257917532\n",
      "spearman： 0.781858414106853 ALL Spearman: 0.7760667582483559\n",
      "time: 36.27693557739258\n",
      "0.001\n",
      "Train Epoch:19 [(0%)]\t Loss: 0.3021  Pearson:0.7705 Spearman:0.7686\n",
      "Train ALL Pearson: 0.7321268492999465\n",
      "Train  ALL Spearman: 0.7152907045672438\n",
      "32.15047860145569\n",
      "Test Epoch:19 [(0%)]\t Loss: 0.4172  Pearson:0.7927 Spearman:0.7718\n",
      "Test : Loss:0.4200 \n",
      "pearson： 0.7937813356868495 ALL Pearson: 0.7991241102731718\n",
      "spearman： 0.7747495582820505 ALL Spearman: 0.7797078180281057\n",
      "time: 36.95593619346619\n",
      "0.001\n",
      "Train Epoch:20 [(0%)]\t Loss: 0.3093  Pearson:0.7178 Spearman:0.6828\n",
      "Train ALL Pearson: 0.738518907507945\n",
      "Train  ALL Spearman: 0.7160078915239363\n",
      "31.38618016242981\n",
      "Test Epoch:20 [(0%)]\t Loss: 0.4173  Pearson:0.8030 Spearman:0.7263\n",
      "Test : Loss:0.4219 \n",
      "pearson： 0.7987400359868201 ALL Pearson: 0.802663743678151\n",
      "spearman： 0.7759752442912226 ALL Spearman: 0.7835436366741634\n",
      "time: 36.080673694610596\n",
      "0.005\n",
      "Train Epoch:21 [(0%)]\t Loss: 0.3017  Pearson:0.6990 Spearman:0.6385\n",
      "Train ALL Pearson: 0.7478761979069751\n",
      "Train  ALL Spearman: 0.7307907554127707\n",
      "31.553931951522827\n",
      "Test Epoch:21 [(0%)]\t Loss: 0.3825  Pearson:0.8416 Spearman:0.8058\n",
      "Test : Loss:0.4113 \n",
      "pearson： 0.8233037141118504 ALL Pearson: 0.8198552261599211\n",
      "spearman： 0.7984524979862385 ALL Spearman: 0.80252110591003\n",
      "time: 36.2584228515625\n",
      "0.005\n",
      "Train Epoch:22 [(0%)]\t Loss: 0.2786  Pearson:0.7631 Spearman:0.7490\n",
      "Train ALL Pearson: 0.7665356382611225\n",
      "Train  ALL Spearman: 0.7458050959238727\n",
      "31.730088472366333\n",
      "Test Epoch:22 [(0%)]\t Loss: 0.4069  Pearson:0.8389 Spearman:0.7766\n",
      "Test : Loss:0.3913 \n",
      "pearson： 0.8346664014046787 ALL Pearson: 0.8355981891591697\n",
      "spearman： 0.8050746194360052 ALL Spearman: 0.8185308215513979\n",
      "time: 36.49855875968933\n",
      "0.005\n",
      "Train Epoch:23 [(0%)]\t Loss: 0.2876  Pearson:0.7679 Spearman:0.7583\n",
      "Train ALL Pearson: 0.7782256764483414\n",
      "Train  ALL Spearman: 0.7599895918749343\n",
      "31.70338225364685\n",
      "Test Epoch:23 [(0%)]\t Loss: 0.3711  Pearson:0.8455 Spearman:0.8167\n",
      "Test : Loss:0.3787 \n",
      "pearson： 0.8445727517765639 ALL Pearson: 0.8447988416237612\n",
      "spearman： 0.8296613631522763 ALL Spearman: 0.8278209379144219\n",
      "time: 36.50184369087219\n",
      "0.005\n",
      "Train Epoch:24 [(0%)]\t Loss: 0.2482  Pearson:0.7950 Spearman:0.7687\n",
      "Train ALL Pearson: 0.7985708697787308\n",
      "Train  ALL Spearman: 0.7784991371014766\n",
      "31.75291872024536\n",
      "Test Epoch:24 [(0%)]\t Loss: 0.3480  Pearson:0.8466 Spearman:0.8045\n",
      "Test : Loss:0.3553 \n",
      "pearson： 0.8534845712533685 ALL Pearson: 0.854644695733212\n",
      "spearman： 0.8325523435825325 ALL Spearman: 0.8367263407680305\n",
      "time: 36.37343764305115\n",
      "0.005\n",
      "Train Epoch:25 [(0%)]\t Loss: 0.2872  Pearson:0.7848 Spearman:0.7746\n",
      "Train ALL Pearson: 0.8079789704165208\n",
      "Train  ALL Spearman: 0.7852963387882889\n",
      "31.71800136566162\n",
      "Test Epoch:25 [(0%)]\t Loss: 0.3581  Pearson:0.8732 Spearman:0.8553\n",
      "Test : Loss:0.3528 \n",
      "pearson： 0.8680684440293344 ALL Pearson: 0.8652264875769653\n",
      "spearman： 0.8475122902460046 ALL Spearman: 0.8486231854844709\n",
      "time: 36.500566720962524\n",
      "0.005\n",
      "Train Epoch:26 [(0%)]\t Loss: 0.2461  Pearson:0.8240 Spearman:0.8193\n",
      "Train ALL Pearson: 0.8166662764086116\n",
      "Train  ALL Spearman: 0.7984730445441818\n",
      "32.13752341270447\n",
      "Test Epoch:26 [(0%)]\t Loss: 0.3372  Pearson:0.8660 Spearman:0.8197\n",
      "Test : Loss:0.3405 \n",
      "pearson： 0.8736783337627955 ALL Pearson: 0.8691582084221801\n",
      "spearman： 0.8521170041534702 ALL Spearman: 0.8514131937910305\n",
      "time: 36.738046407699585\n",
      "0.005\n",
      "Train Epoch:27 [(0%)]\t Loss: 0.2296  Pearson:0.8502 Spearman:0.8063\n",
      "Train ALL Pearson: 0.8301495569130338\n",
      "Train  ALL Spearman: 0.80631073548655\n",
      "31.67473602294922\n",
      "Test Epoch:27 [(0%)]\t Loss: 0.3036  Pearson:0.8379 Spearman:0.8368\n",
      "Test : Loss:0.3027 \n",
      "pearson： 0.8661837785041526 ALL Pearson: 0.8754455672058266\n",
      "spearman： 0.8528754944680167 ALL Spearman: 0.8573347639119079\n",
      "time: 36.52074408531189\n",
      "0.005\n",
      "Train Epoch:28 [(0%)]\t Loss: 0.2256  Pearson:0.8103 Spearman:0.7919\n",
      "Train ALL Pearson: 0.8361417345511213\n",
      "Train  ALL Spearman: 0.812247238428431\n",
      "31.232516765594482\n",
      "Test Epoch:28 [(0%)]\t Loss: 0.3471  Pearson:0.8594 Spearman:0.8129\n",
      "Test : Loss:0.3118 \n",
      "pearson： 0.876117397424099 ALL Pearson: 0.8815526099538377\n",
      "spearman： 0.8513301188078124 ALL Spearman: 0.8651024257259292\n",
      "time: 36.21891927719116\n",
      "0.005\n",
      "Train Epoch:29 [(0%)]\t Loss: 0.2331  Pearson:0.8476 Spearman:0.8188\n",
      "Train ALL Pearson: 0.8406688167818528\n",
      "Train  ALL Spearman: 0.8149649615022739\n",
      "31.48781418800354\n",
      "Test Epoch:29 [(0%)]\t Loss: 0.2939  Pearson:0.8847 Spearman:0.8539\n",
      "Test : Loss:0.2895 \n",
      "pearson： 0.8877665073295617 ALL Pearson: 0.886491944563055\n",
      "spearman： 0.8696054482591513 ALL Spearman: 0.8715917582960808\n",
      "time: 35.94798922538757\n",
      "0.005\n",
      "Train Epoch:30 [(0%)]\t Loss: 0.2428  Pearson:0.8622 Spearman:0.8425\n",
      "Train ALL Pearson: 0.8489768293806349\n",
      "Train  ALL Spearman: 0.8223634487258921\n",
      "31.434861660003662\n",
      "Test Epoch:30 [(0%)]\t Loss: 0.2812  Pearson:0.8971 Spearman:0.8815\n",
      "Test : Loss:0.2771 \n",
      "pearson： 0.8922105824850387 ALL Pearson: 0.8905159215067723\n",
      "spearman： 0.8772330424252849 ALL Spearman: 0.8756810366507147\n",
      "time: 36.44619679450989\n",
      "0.03\n",
      "Train Epoch:31 [(0%)]\t Loss: 0.2203  Pearson:0.8616 Spearman:0.8404\n",
      "Train ALL Pearson: 0.6780550800778768\n",
      "Train  ALL Spearman: 0.6511235848038961\n",
      "31.59899640083313\n",
      "Test Epoch:31 [(0%)]\t Loss: 0.6529  Pearson:0.8625 Spearman:0.8643\n",
      "Test : Loss:0.6613 \n",
      "pearson： 0.8560511742457113 ALL Pearson: 0.861149550380154\n",
      "spearman： 0.8592850800022838 ALL Spearman: 0.858594914191878\n",
      "time: 36.37146544456482\n",
      "0.03\n",
      "Train Epoch:32 [(0%)]\t Loss: 0.4404  Pearson:0.8075 Spearman:0.7817\n",
      "Train ALL Pearson: 0.7624807492513945\n",
      "Train  ALL Spearman: 0.7280333414625101\n",
      "31.741302013397217\n",
      "Test Epoch:32 [(0%)]\t Loss: 0.5894  Pearson:0.8482 Spearman:0.8301\n",
      "Test : Loss:0.5732 \n",
      "pearson： 0.8692178116582561 ALL Pearson: 0.8753257499789155\n",
      "spearman： 0.8576257738138441 ALL Spearman: 0.8648417840710207\n",
      "time: 36.485780477523804\n",
      "0.03\n",
      "Train Epoch:33 [(0%)]\t Loss: 0.3933  Pearson:0.8626 Spearman:0.8348\n",
      "Train ALL Pearson: 0.7944600448264433\n",
      "Train  ALL Spearman: 0.7563055425978443\n",
      "31.645387887954712\n",
      "Test Epoch:33 [(0%)]\t Loss: 0.5112  Pearson:0.8659 Spearman:0.8534\n",
      "Test : Loss:0.4958 \n",
      "pearson： 0.8834842788724888 ALL Pearson: 0.8856742610760041\n",
      "spearman： 0.8692329320710983 ALL Spearman: 0.8678541987186403\n",
      "time: 36.521117210388184\n",
      "0.03\n",
      "Train Epoch:34 [(0%)]\t Loss: 0.3142  Pearson:0.8927 Spearman:0.8896\n",
      "Train ALL Pearson: 0.8176042665294129\n",
      "Train  ALL Spearman: 0.7842648803721362\n",
      "31.59537696838379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:34 [(0%)]\t Loss: 0.2045  Pearson:0.8837 Spearman:0.8004\n",
      "Test : Loss:0.1978 \n",
      "pearson： 0.8953751327426129 ALL Pearson: 0.8969907063540468\n",
      "spearman： 0.8542952281952612 ALL Spearman: 0.8688426704612353\n",
      "time: 36.27004528045654\n",
      "0.03\n",
      "Train Epoch:35 [(0%)]\t Loss: 0.2572  Pearson:0.8820 Spearman:0.8475\n",
      "Train ALL Pearson: 0.8163989978428184\n",
      "Train  ALL Spearman: 0.7782899067941802\n",
      "32.06257653236389\n",
      "Test Epoch:35 [(0%)]\t Loss: 0.1933  Pearson:0.9049 Spearman:0.8660\n",
      "Test : Loss:0.2079 \n",
      "pearson： 0.8993901474865971 ALL Pearson: 0.8995295097320142\n",
      "spearman： 0.8672362484715056 ALL Spearman: 0.8734720179076629\n",
      "time: 36.71808314323425\n",
      "0.03\n",
      "Train Epoch:36 [(0%)]\t Loss: 0.2036  Pearson:0.8931 Spearman:0.8646\n",
      "Train ALL Pearson: 0.8561532693816953\n",
      "Train  ALL Spearman: 0.820656651335252\n",
      "31.374513626098633\n",
      "Test Epoch:36 [(0%)]\t Loss: 0.2033  Pearson:0.8879 Spearman:0.8640\n",
      "Test : Loss:0.1955 \n",
      "pearson： 0.8958497548699307 ALL Pearson: 0.9066920158961382\n",
      "spearman： 0.8762028598625027 ALL Spearman: 0.8827459201180166\n",
      "time: 35.96903944015503\n",
      "0.03\n",
      "Train Epoch:37 [(0%)]\t Loss: 0.2071  Pearson:0.8875 Spearman:0.8534\n",
      "Train ALL Pearson: 0.8602505629925673\n",
      "Train  ALL Spearman: 0.8235889854218377\n",
      "31.941750288009644\n",
      "Test Epoch:37 [(0%)]\t Loss: 0.1960  Pearson:0.8979 Spearman:0.8915\n",
      "Test : Loss:0.1882 \n",
      "pearson： 0.9107783306890723 ALL Pearson: 0.9105629902958453\n",
      "spearman： 0.8893134404792149 ALL Spearman: 0.8874681712051862\n",
      "time: 36.771202087402344\n",
      "0.03\n",
      "Train Epoch:38 [(0%)]\t Loss: 0.2352  Pearson:0.8966 Spearman:0.8648\n",
      "Train ALL Pearson: 0.874646065209587\n",
      "Train  ALL Spearman: 0.8388292530333296\n",
      "31.936161756515503\n",
      "Test Epoch:38 [(0%)]\t Loss: 0.3463  Pearson:0.9294 Spearman:0.9168\n",
      "Test : Loss:0.3637 \n",
      "pearson： 0.9170436855568549 ALL Pearson: 0.9131899086148284\n",
      "spearman： 0.8984621600566229 ALL Spearman: 0.8937863801348442\n",
      "time: 36.767611265182495\n",
      "0.03\n",
      "Train Epoch:39 [(0%)]\t Loss: 0.2147  Pearson:0.9332 Spearman:0.9238\n",
      "Train ALL Pearson: 0.8741698548601526\n",
      "Train  ALL Spearman: 0.840107867280345\n",
      "31.963783740997314\n",
      "Test Epoch:39 [(0%)]\t Loss: 0.3176  Pearson:0.9270 Spearman:0.8974\n",
      "Test : Loss:0.3202 \n",
      "pearson： 0.9184956073079159 ALL Pearson: 0.917254687689381\n",
      "spearman： 0.8977868542104993 ALL Spearman: 0.8975325740227771\n",
      "time: 36.60929584503174\n",
      "0.03\n",
      "Train Epoch:40 [(0%)]\t Loss: 0.1942  Pearson:0.9149 Spearman:0.9112\n",
      "Train ALL Pearson: 0.8892020315208282\n",
      "Train  ALL Spearman: 0.8581589411235292\n",
      "31.774759769439697\n",
      "Test Epoch:40 [(0%)]\t Loss: 0.3730  Pearson:0.9209 Spearman:0.8875\n",
      "Test : Loss:0.3630 \n",
      "pearson： 0.9210101857112017 ALL Pearson: 0.9213972083347346\n",
      "spearman： 0.902990798221493 ALL Spearman: 0.9038533245028259\n",
      "time: 36.3203010559082\n",
      "0.03\n",
      "Train Epoch:41 [(0%)]\t Loss: 0.2441  Pearson:0.9113 Spearman:0.8963\n",
      "Train ALL Pearson: 0.8839498688702199\n",
      "Train  ALL Spearman: 0.8496116914475768\n",
      "31.780385732650757\n",
      "Test Epoch:41 [(0%)]\t Loss: 0.1637  Pearson:0.9216 Spearman:0.8879\n",
      "Test : Loss:0.1751 \n",
      "pearson： 0.9221997430310566 ALL Pearson: 0.9211721710811653\n",
      "spearman： 0.9025441811436689 ALL Spearman: 0.9005184496938169\n",
      "time: 36.55185556411743\n",
      "0.03\n",
      "Train Epoch:42 [(0%)]\t Loss: 0.1799  Pearson:0.9139 Spearman:0.9030\n",
      "Train ALL Pearson: 0.8895813969677335\n",
      "Train  ALL Spearman: 0.8575128655875047\n",
      "31.33565902709961\n",
      "Test Epoch:42 [(0%)]\t Loss: 0.1969  Pearson:0.9224 Spearman:0.9001\n",
      "Test : Loss:0.1897 \n",
      "pearson： 0.9238830716291894 ALL Pearson: 0.9237595063155369\n",
      "spearman： 0.9049893749293493 ALL Spearman: 0.9032726656098058\n",
      "time: 35.980170011520386\n",
      "0.03\n",
      "Train Epoch:43 [(0%)]\t Loss: 0.1875  Pearson:0.9240 Spearman:0.8888\n",
      "Train ALL Pearson: 0.9011136885397074\n",
      "Train  ALL Spearman: 0.8713813295458469\n",
      "31.44550085067749\n",
      "Test Epoch:43 [(0%)]\t Loss: 0.2909  Pearson:0.9385 Spearman:0.9295\n",
      "Test : Loss:0.2920 \n",
      "pearson： 0.9235720125356196 ALL Pearson: 0.9255872255747103\n",
      "spearman： 0.9092340701872577 ALL Spearman: 0.9073271658666527\n",
      "time: 36.015034675598145\n",
      "0.03\n",
      "Train Epoch:44 [(0%)]\t Loss: 0.1990  Pearson:0.9192 Spearman:0.8368\n",
      "Train ALL Pearson: 0.8996791181698968\n",
      "Train  ALL Spearman: 0.868555305771582\n",
      "31.74252223968506\n",
      "Test Epoch:44 [(0%)]\t Loss: 0.1808  Pearson:0.9123 Spearman:0.9071\n",
      "Test : Loss:0.1796 \n",
      "pearson： 0.9267498314030571 ALL Pearson: 0.9276872736252536\n",
      "spearman： 0.9060211569659391 ALL Spearman: 0.9078623349083045\n",
      "time: 36.57697319984436\n",
      "0.03\n",
      "Train Epoch:45 [(0%)]\t Loss: 0.1717  Pearson:0.9277 Spearman:0.9112\n",
      "Train ALL Pearson: 0.9029243290249368\n",
      "Train  ALL Spearman: 0.8736754211316\n",
      "31.433444499969482\n",
      "Test Epoch:45 [(0%)]\t Loss: 0.2393  Pearson:0.9361 Spearman:0.9082\n",
      "Test : Loss:0.2496 \n",
      "pearson： 0.9272977189645729 ALL Pearson: 0.9280364264505725\n",
      "spearman： 0.9106648654047936 ALL Spearman: 0.9106542243230374\n",
      "time: 36.07851696014404\n",
      "0.03\n",
      "Train Epoch:46 [(0%)]\t Loss: 0.1962  Pearson:0.9089 Spearman:0.8884\n",
      "Train ALL Pearson: 0.9140343080529041\n",
      "Train  ALL Spearman: 0.8898560964835354\n",
      "31.768787622451782\n",
      "Test Epoch:46 [(0%)]\t Loss: 0.2943  Pearson:0.9329 Spearman:0.9164\n",
      "Test : Loss:0.2986 \n",
      "pearson： 0.9301849963294537 ALL Pearson: 0.9284934225752901\n",
      "spearman： 0.9101741269517742 ALL Spearman: 0.9106381963685812\n",
      "time: 36.40929889678955\n",
      "0.03\n",
      "Train Epoch:47 [(0%)]\t Loss: 0.2208  Pearson:0.9140 Spearman:0.8848\n",
      "Train ALL Pearson: 0.9063093319044718\n",
      "Train  ALL Spearman: 0.8778962287523129\n",
      "31.915380477905273\n",
      "Test Epoch:47 [(0%)]\t Loss: 0.1733  Pearson:0.9183 Spearman:0.8993\n",
      "Test : Loss:0.1639 \n",
      "pearson： 0.9273234897673718 ALL Pearson: 0.9306263093913186\n",
      "spearman： 0.9087021164235172 ALL Spearman: 0.910846608519456\n",
      "time: 36.82943344116211\n",
      "0.03\n",
      "Train Epoch:48 [(0%)]\t Loss: 0.1804  Pearson:0.9220 Spearman:0.8882\n",
      "Train ALL Pearson: 0.9166855617068799\n",
      "Train  ALL Spearman: 0.8923399512121427\n",
      "31.51059889793396\n",
      "Test Epoch:48 [(0%)]\t Loss: 0.1816  Pearson:0.9412 Spearman:0.9407\n",
      "Test : Loss:0.1893 \n",
      "pearson： 0.9346134122012884 ALL Pearson: 0.931826304719333\n",
      "spearman： 0.9174482847317496 ALL Spearman: 0.9137991525761134\n",
      "time: 36.31643104553223\n",
      "0.03\n",
      "Train Epoch:49 [(0%)]\t Loss: 0.1527  Pearson:0.9423 Spearman:0.9215\n",
      "Train ALL Pearson: 0.9134004014420607\n",
      "Train  ALL Spearman: 0.8868127299314835\n",
      "31.611643314361572\n",
      "Test Epoch:49 [(0%)]\t Loss: 0.1967  Pearson:0.9310 Spearman:0.9222\n",
      "Test : Loss:0.2016 \n",
      "pearson： 0.9278535668982658 ALL Pearson: 0.9308820844389614\n",
      "spearman： 0.91100929658677 ALL Spearman: 0.9121539537652291\n",
      "time: 36.19117259979248\n",
      "0.03\n",
      "Train Epoch:50 [(0%)]\t Loss: 0.1469  Pearson:0.9444 Spearman:0.9335\n",
      "Train ALL Pearson: 0.9197969805820695\n",
      "Train  ALL Spearman: 0.8966489675790169\n",
      "31.88541579246521\n",
      "Test Epoch:50 [(0%)]\t Loss: 0.1621  Pearson:0.9198 Spearman:0.9062\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.925289868587397 ALL Pearson: 0.9322062766911506\n",
      "spearman： 0.9072320441899028 ALL Spearman: 0.9114221396154677\n",
      "time: 36.68087863922119\n",
      "0.03\n",
      "Train Epoch:51 [(0%)]\t Loss: 0.1894  Pearson:0.9385 Spearman:0.9174\n",
      "Train ALL Pearson: 0.9136524385623901\n",
      "Train  ALL Spearman: 0.8858133117331721\n",
      "31.42948031425476\n",
      "Test Epoch:51 [(0%)]\t Loss: 0.2762  Pearson:0.9349 Spearman:0.9225\n",
      "Test : Loss:0.2682 \n",
      "pearson： 0.9340913843332674 ALL Pearson: 0.9328314165846394\n",
      "spearman： 0.9189961541843981 ALL Spearman: 0.9150542012132953\n",
      "time: 35.99724817276001\n",
      "0.03\n",
      "Train Epoch:52 [(0%)]\t Loss: 0.2010  Pearson:0.9231 Spearman:0.9139\n",
      "Train ALL Pearson: 0.9224075661516037\n",
      "Train  ALL Spearman: 0.8995226143277791\n",
      "31.83560562133789\n",
      "Test Epoch:52 [(0%)]\t Loss: 0.2236  Pearson:0.9383 Spearman:0.9122\n",
      "Test : Loss:0.2332 \n",
      "pearson： 0.9340747120104034 ALL Pearson: 0.9321122768356699\n",
      "spearman： 0.9165024620563167 ALL Spearman: 0.9157512163288546\n",
      "time: 36.64969992637634\n",
      "0.03\n",
      "Train Epoch:53 [(0%)]\t Loss: 0.1494  Pearson:0.9349 Spearman:0.9008\n",
      "Train ALL Pearson: 0.9183391175960309\n",
      "Train  ALL Spearman: 0.8947474282601222\n",
      "31.329390287399292\n",
      "Test Epoch:53 [(0%)]\t Loss: 0.2197  Pearson:0.9516 Spearman:0.9440\n",
      "Test : Loss:0.2296 \n",
      "pearson： 0.9358460567128486 ALL Pearson: 0.9333113902779896\n",
      "spearman： 0.9232639403439259 ALL Spearman: 0.9162780043554145\n",
      "time: 35.90926122665405\n",
      "0.03\n",
      "Train Epoch:54 [(0%)]\t Loss: 0.1734  Pearson:0.9172 Spearman:0.8853\n",
      "Train ALL Pearson: 0.9235480319563585\n",
      "Train  ALL Spearman: 0.9009761504597433\n",
      "31.53316855430603\n",
      "Test Epoch:54 [(0%)]\t Loss: 0.2115  Pearson:0.9294 Spearman:0.9243\n",
      "Test : Loss:0.2173 \n",
      "pearson： 0.9364834832375948 ALL Pearson: 0.9343757343006838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9209659408675541 ALL Spearman: 0.9182777836871252\n",
      "time: 36.21966505050659\n",
      "0.03\n",
      "Train Epoch:55 [(0%)]\t Loss: 0.1637  Pearson:0.9517 Spearman:0.9365\n",
      "Train ALL Pearson: 0.9238547204862085\n",
      "Train  ALL Spearman: 0.9032584276823995\n",
      "31.834150075912476\n",
      "Test Epoch:55 [(0%)]\t Loss: 0.1866  Pearson:0.9402 Spearman:0.9187\n",
      "Test : Loss:0.2166 \n",
      "pearson： 0.9350238322459067 ALL Pearson: 0.9338130444060039\n",
      "spearman： 0.9160696653929644 ALL Spearman: 0.9165397131023846\n",
      "time: 36.6566047668457\n",
      "0.03\n",
      "Train Epoch:56 [(0%)]\t Loss: 0.1484  Pearson:0.9594 Spearman:0.9360\n",
      "Train ALL Pearson: 0.9253838340793646\n",
      "Train  ALL Spearman: 0.9048896667318592\n",
      "31.023872137069702\n",
      "Test Epoch:56 [(0%)]\t Loss: 0.2120  Pearson:0.9404 Spearman:0.9254\n",
      "Test : Loss:0.2272 \n",
      "pearson： 0.938588101789482 ALL Pearson: 0.9352312735739625\n",
      "spearman： 0.9215850600119136 ALL Spearman: 0.9177966728539234\n",
      "time: 35.69796633720398\n",
      "0.03\n",
      "Train Epoch:57 [(0%)]\t Loss: 0.1603  Pearson:0.9432 Spearman:0.9300\n",
      "Train ALL Pearson: 0.9311145082835988\n",
      "Train  ALL Spearman: 0.9112665966792654\n",
      "31.623637437820435\n",
      "Test Epoch:57 [(0%)]\t Loss: 0.2163  Pearson:0.9330 Spearman:0.9075\n",
      "Test : Loss:0.2167 \n",
      "pearson： 0.9366264058308548 ALL Pearson: 0.9342213251693208\n",
      "spearman： 0.919409518664683 ALL Spearman: 0.9175254086213954\n",
      "time: 36.38611030578613\n",
      "0.03\n",
      "Train Epoch:58 [(0%)]\t Loss: 0.1631  Pearson:0.9321 Spearman:0.9176\n",
      "Train ALL Pearson: 0.9279470484183434\n",
      "Train  ALL Spearman: 0.9067432827885563\n",
      "31.66500496864319\n",
      "Test Epoch:58 [(0%)]\t Loss: 0.2042  Pearson:0.9481 Spearman:0.9282\n",
      "Test : Loss:0.2073 \n",
      "pearson： 0.9394992414199101 ALL Pearson: 0.9356604248263168\n",
      "spearman： 0.9229196880212158 ALL Spearman: 0.9181201831242207\n",
      "time: 36.332048177719116\n",
      "0.03\n",
      "Train Epoch:59 [(0%)]\t Loss: 0.1479  Pearson:0.9409 Spearman:0.9192\n",
      "Train ALL Pearson: 0.9307051644604111\n",
      "Train  ALL Spearman: 0.9105672877830986\n",
      "31.711772203445435\n",
      "Test Epoch:59 [(0%)]\t Loss: 0.1780  Pearson:0.9319 Spearman:0.9157\n",
      "Test : Loss:0.1900 \n",
      "pearson： 0.9365893577213747 ALL Pearson: 0.9343248543311229\n",
      "spearman： 0.9217129941747951 ALL Spearman: 0.9175471447713444\n",
      "time: 36.59720706939697\n",
      "0.03\n",
      "Train Epoch:60 [(0%)]\t Loss: 0.1380  Pearson:0.9567 Spearman:0.9430\n",
      "Train ALL Pearson: 0.9275400657152031\n",
      "Train  ALL Spearman: 0.9054265013425735\n",
      "31.443225860595703\n",
      "Test Epoch:60 [(0%)]\t Loss: 0.2522  Pearson:0.9333 Spearman:0.9087\n",
      "Test : Loss:0.2510 \n",
      "pearson： 0.9376336810617633 ALL Pearson: 0.9350355187776053\n",
      "spearman： 0.9172339074833838 ALL Spearman: 0.9181971915859666\n",
      "time: 35.99076747894287\n",
      "0.03\n",
      "Train Epoch:61 [(0%)]\t Loss: 0.1889  Pearson:0.9418 Spearman:0.9366\n",
      "Train ALL Pearson: 0.9299782919891502\n",
      "Train  ALL Spearman: 0.9091728512694509\n",
      "31.783419132232666\n",
      "Test Epoch:61 [(0%)]\t Loss: 0.1568  Pearson:0.9378 Spearman:0.9114\n",
      "Test : Loss:0.1551 \n",
      "pearson： 0.9356241255883176 ALL Pearson: 0.9363673792001918\n",
      "spearman： 0.914677563874531 ALL Spearman: 0.918560586513047\n",
      "time: 36.46491718292236\n",
      "0.03\n",
      "Train Epoch:62 [(0%)]\t Loss: 0.1327  Pearson:0.9520 Spearman:0.9318\n",
      "Train ALL Pearson: 0.9264849993847671\n",
      "Train  ALL Spearman: 0.9028162905700847\n",
      "31.770423650741577\n",
      "Test Epoch:62 [(0%)]\t Loss: 0.1996  Pearson:0.9433 Spearman:0.9262\n",
      "Test : Loss:0.2115 \n",
      "pearson： 0.9386337769798135 ALL Pearson: 0.9361117143820251\n",
      "spearman： 0.920573024627744 ALL Spearman: 0.919118452059865\n",
      "time: 36.43692636489868\n",
      "0.03\n",
      "Train Epoch:63 [(0%)]\t Loss: 0.1539  Pearson:0.9465 Spearman:0.9384\n",
      "Train ALL Pearson: 0.9355821075919616\n",
      "Train  ALL Spearman: 0.915678314007338\n",
      "31.458804845809937\n",
      "Test Epoch:63 [(0%)]\t Loss: 0.1573  Pearson:0.9354 Spearman:0.9238\n",
      "Test : Loss:0.1543 \n",
      "pearson： 0.935474425930391 ALL Pearson: 0.9367452926384886\n",
      "spearman： 0.920175453020866 ALL Spearman: 0.9184285451866552\n",
      "time: 36.205480098724365\n",
      "0.03\n",
      "Train Epoch:64 [(0%)]\t Loss: 0.1540  Pearson:0.9438 Spearman:0.9216\n",
      "Train ALL Pearson: 0.933169087691227\n",
      "Train  ALL Spearman: 0.9131830071336369\n",
      "31.66108798980713\n",
      "Test Epoch:64 [(0%)]\t Loss: 0.1644  Pearson:0.9191 Spearman:0.8990\n",
      "Test : Loss:0.1527 \n",
      "pearson： 0.9355506170234971 ALL Pearson: 0.9373257404049393\n",
      "spearman： 0.9196505336581815 ALL Spearman: 0.9195723839081542\n",
      "time: 36.454550981521606\n",
      "0.03\n",
      "Train Epoch:65 [(0%)]\t Loss: 0.1502  Pearson:0.9397 Spearman:0.9223\n",
      "Train ALL Pearson: 0.9384199403563968\n",
      "Train  ALL Spearman: 0.9197659783785711\n",
      "31.523270845413208\n",
      "Test Epoch:65 [(0%)]\t Loss: 0.1816  Pearson:0.9309 Spearman:0.9130\n",
      "Test : Loss:0.1699 \n",
      "pearson： 0.9359057034622986 ALL Pearson: 0.937438533049648\n",
      "spearman： 0.918467929827693 ALL Spearman: 0.9203635733274377\n",
      "time: 36.18877458572388\n",
      "0.03\n",
      "Train Epoch:66 [(0%)]\t Loss: 0.1276  Pearson:0.9582 Spearman:0.9330\n",
      "Train ALL Pearson: 0.9324026269501214\n",
      "Train  ALL Spearman: 0.9127357550800671\n",
      "32.15703725814819\n",
      "Test Epoch:66 [(0%)]\t Loss: 0.2233  Pearson:0.9498 Spearman:0.9370\n",
      "Test : Loss:0.2398 \n",
      "pearson： 0.9417121539581246 ALL Pearson: 0.9372920236795784\n",
      "spearman： 0.9242940735534378 ALL Spearman: 0.9222537988251703\n",
      "time: 36.83953666687012\n",
      "0.03\n",
      "Train Epoch:67 [(0%)]\t Loss: 0.1737  Pearson:0.9291 Spearman:0.9108\n",
      "Train ALL Pearson: 0.9346044136544613\n",
      "Train  ALL Spearman: 0.9162831445171243\n",
      "31.136921167373657\n",
      "Test Epoch:67 [(0%)]\t Loss: 0.1464  Pearson:0.9342 Spearman:0.9070\n",
      "Test : Loss:0.1519 \n",
      "pearson： 0.9371558205338059 ALL Pearson: 0.9375407950067338\n",
      "spearman： 0.9192307951380212 ALL Spearman: 0.9208906884392801\n",
      "time: 35.944379806518555\n",
      "0.03\n",
      "Train Epoch:68 [(0%)]\t Loss: 0.1463  Pearson:0.9544 Spearman:0.9342\n",
      "Train ALL Pearson: 0.9363836038520796\n",
      "Train  ALL Spearman: 0.9181366076748033\n",
      "31.184356927871704\n",
      "Test Epoch:68 [(0%)]\t Loss: 0.2090  Pearson:0.9441 Spearman:0.9097\n",
      "Test : Loss:0.2141 \n",
      "pearson： 0.9354493899947252 ALL Pearson: 0.9372108662261343\n",
      "spearman： 0.9162962674121903 ALL Spearman: 0.921202923061051\n",
      "time: 35.87159085273743\n",
      "0.03\n",
      "Train Epoch:69 [(0%)]\t Loss: 0.1540  Pearson:0.9508 Spearman:0.9446\n",
      "Train ALL Pearson: 0.9355116932870731\n",
      "Train  ALL Spearman: 0.9168844569635485\n",
      "31.714265823364258\n",
      "Test Epoch:69 [(0%)]\t Loss: 0.2239  Pearson:0.9281 Spearman:0.9088\n",
      "Test : Loss:0.2160 \n",
      "pearson： 0.9363459288365754 ALL Pearson: 0.9377156777308526\n",
      "spearman： 0.9192151656058858 ALL Spearman: 0.9216564600751059\n",
      "time: 36.513585805892944\n",
      "0.03\n",
      "Train Epoch:70 [(0%)]\t Loss: 0.1486  Pearson:0.9560 Spearman:0.9265\n",
      "Train ALL Pearson: 0.9327622161284901\n",
      "Train  ALL Spearman: 0.9114856699126693\n",
      "31.797977447509766\n",
      "Test Epoch:70 [(0%)]\t Loss: 0.1785  Pearson:0.9391 Spearman:0.9202\n",
      "Test : Loss:0.1798 \n",
      "pearson： 0.9359272296539424 ALL Pearson: 0.9377017747955583\n",
      "spearman： 0.9209227499333271 ALL Spearman: 0.9203115949425262\n",
      "time: 36.375508546829224\n",
      "0.03\n",
      "Train Epoch:71 [(0%)]\t Loss: 0.1269  Pearson:0.9472 Spearman:0.9330\n",
      "Train ALL Pearson: 0.939823129285447\n",
      "Train  ALL Spearman: 0.9232236685390837\n",
      "31.71388339996338\n",
      "Test Epoch:71 [(0%)]\t Loss: 0.1594  Pearson:0.9296 Spearman:0.9017\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9348434686286521 ALL Pearson: 0.9386069186736731\n",
      "spearman： 0.9196955496093708 ALL Spearman: 0.920501027698441\n",
      "time: 36.51294445991516\n",
      "0.03\n",
      "Train Epoch:72 [(0%)]\t Loss: 0.1291  Pearson:0.9647 Spearman:0.9556\n",
      "Train ALL Pearson: 0.9409970242763657\n",
      "Train  ALL Spearman: 0.9243212325660394\n",
      "31.419474363327026\n",
      "Test Epoch:72 [(0%)]\t Loss: 0.1958  Pearson:0.9529 Spearman:0.9369\n",
      "Test : Loss:0.2035 \n",
      "pearson： 0.9390476115101223 ALL Pearson: 0.9390726581557812\n",
      "spearman： 0.9249061432894663 ALL Spearman: 0.92296248338328\n",
      "time: 36.032994747161865\n",
      "0.03\n",
      "Train Epoch:73 [(0%)]\t Loss: 0.1458  Pearson:0.9542 Spearman:0.9451\n",
      "Train ALL Pearson: 0.9386456676298693\n",
      "Train  ALL Spearman: 0.9191453544997997\n",
      "31.697757720947266\n",
      "Test Epoch:73 [(0%)]\t Loss: 0.1869  Pearson:0.9440 Spearman:0.9126\n",
      "Test : Loss:0.1875 \n",
      "pearson： 0.9415312549519801 ALL Pearson: 0.9399182927852348\n",
      "spearman： 0.9225629438788144 ALL Spearman: 0.9229146596280811\n",
      "time: 36.482253551483154\n",
      "0.03\n",
      "Train Epoch:74 [(0%)]\t Loss: 0.1359  Pearson:0.9538 Spearman:0.9590\n",
      "Train ALL Pearson: 0.9448529816406956\n",
      "Train  ALL Spearman: 0.9290323634212705\n",
      "31.69483971595764\n",
      "Test Epoch:74 [(0%)]\t Loss: 0.1447  Pearson:0.9486 Spearman:0.9284\n",
      "Test : Loss:0.1576 \n",
      "pearson： 0.938122408623935 ALL Pearson: 0.9382942354527031\n",
      "spearman： 0.9223144530944928 ALL Spearman: 0.9207250915166016\n",
      "time: 36.587271213531494\n",
      "0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:75 [(0%)]\t Loss: 0.1736  Pearson:0.9406 Spearman:0.9237\n",
      "Train ALL Pearson: 0.9345481437663272\n",
      "Train  ALL Spearman: 0.913479993489459\n",
      "31.949668645858765\n",
      "Test Epoch:75 [(0%)]\t Loss: 0.2017  Pearson:0.9364 Spearman:0.9095\n",
      "Test : Loss:0.2088 \n",
      "pearson： 0.9359755611352035 ALL Pearson: 0.9391417560913784\n",
      "spearman： 0.9154576618948499 ALL Spearman: 0.9226158230434331\n",
      "time: 36.609740018844604\n",
      "0.03\n",
      "Train Epoch:76 [(0%)]\t Loss: 0.1336  Pearson:0.9607 Spearman:0.9447\n",
      "Train ALL Pearson: 0.9414708991828704\n",
      "Train  ALL Spearman: 0.9249980232132019\n",
      "31.585854291915894\n",
      "Test Epoch:76 [(0%)]\t Loss: 0.1573  Pearson:0.9184 Spearman:0.9125\n",
      "Test : Loss:0.1544 \n",
      "pearson： 0.9348508329579599 ALL Pearson: 0.9395030704251109\n",
      "spearman： 0.919775549038063 ALL Spearman: 0.9214148656200284\n",
      "time: 36.29855918884277\n",
      "0.03\n",
      "Train Epoch:77 [(0%)]\t Loss: 0.1228  Pearson:0.9598 Spearman:0.9448\n",
      "Train ALL Pearson: 0.940651742783544\n",
      "Train  ALL Spearman: 0.9225136151132671\n",
      "31.83238983154297\n",
      "Test Epoch:77 [(0%)]\t Loss: 0.1867  Pearson:0.9221 Spearman:0.8981\n",
      "Test : Loss:0.1862 \n",
      "pearson： 0.9369241303890952 ALL Pearson: 0.9392092756035265\n",
      "spearman： 0.9206445124072862 ALL Spearman: 0.9229141191303329\n",
      "time: 36.492894887924194\n",
      "0.03\n",
      "Train Epoch:78 [(0%)]\t Loss: 0.1359  Pearson:0.9483 Spearman:0.9319\n",
      "Train ALL Pearson: 0.9450258272052877\n",
      "Train  ALL Spearman: 0.9280769980821096\n",
      "31.603278398513794\n",
      "Test Epoch:78 [(0%)]\t Loss: 0.1458  Pearson:0.9412 Spearman:0.9119\n",
      "Test : Loss:0.1477 \n",
      "pearson： 0.943135876814279 ALL Pearson: 0.9397133424608366\n",
      "spearman： 0.9223403897520299 ALL Spearman: 0.9221572649168869\n",
      "time: 36.238791942596436\n",
      "0.03\n",
      "Train Epoch:79 [(0%)]\t Loss: 0.1227  Pearson:0.9540 Spearman:0.9390\n",
      "Train ALL Pearson: 0.9456080121378904\n",
      "Train  ALL Spearman: 0.9295584619122091\n",
      "31.952045917510986\n",
      "Test Epoch:79 [(0%)]\t Loss: 0.1763  Pearson:0.9313 Spearman:0.9224\n",
      "Test : Loss:0.1761 \n",
      "pearson： 0.9383371383397705 ALL Pearson: 0.939558964858865\n",
      "spearman： 0.923893316642462 ALL Spearman: 0.92175659403797\n",
      "time: 36.89935755729675\n",
      "0.03\n",
      "Train Epoch:80 [(0%)]\t Loss: 0.1300  Pearson:0.9479 Spearman:0.9342\n",
      "Train ALL Pearson: 0.9421559251088619\n",
      "Train  ALL Spearman: 0.9241443551802738\n",
      "31.63186812400818\n",
      "Test Epoch:80 [(0%)]\t Loss: 0.1874  Pearson:0.9345 Spearman:0.9191\n",
      "Test : Loss:0.1871 \n",
      "pearson： 0.9373642129508307 ALL Pearson: 0.939505728756956\n",
      "spearman： 0.9239693201240218 ALL Spearman: 0.9234816607494045\n",
      "time: 36.25738501548767\n",
      "0.03\n",
      "Train Epoch:81 [(0%)]\t Loss: 0.1408  Pearson:0.9525 Spearman:0.9372\n",
      "Train ALL Pearson: 0.944439795374373\n",
      "Train  ALL Spearman: 0.9256478103610902\n",
      "31.3583505153656\n",
      "Test Epoch:81 [(0%)]\t Loss: 0.1532  Pearson:0.9423 Spearman:0.9267\n",
      "Test : Loss:0.1604 \n",
      "pearson： 0.939975654695435 ALL Pearson: 0.9381076501116584\n",
      "spearman： 0.9231092993061704 ALL Spearman: 0.9194944316806357\n",
      "time: 35.84890937805176\n",
      "0.03\n",
      "Train Epoch:82 [(0%)]\t Loss: 0.1779  Pearson:0.9526 Spearman:0.9442\n",
      "Train ALL Pearson: 0.9416174439991701\n",
      "Train  ALL Spearman: 0.9242521373332647\n",
      "31.547425508499146\n",
      "Test Epoch:82 [(0%)]\t Loss: 0.1818  Pearson:0.9495 Spearman:0.9224\n",
      "Test : Loss:0.1874 \n",
      "pearson： 0.9414654888598516 ALL Pearson: 0.9388041304668514\n",
      "spearman： 0.9253770776284743 ALL Spearman: 0.9225246820406138\n",
      "time: 36.45185422897339\n",
      "0.03\n",
      "Train Epoch:83 [(0%)]\t Loss: 0.1443  Pearson:0.9548 Spearman:0.9363\n",
      "Train ALL Pearson: 0.941090700949013\n",
      "Train  ALL Spearman: 0.9235318432309256\n",
      "31.817599773406982\n",
      "Test Epoch:83 [(0%)]\t Loss: 0.1792  Pearson:0.9283 Spearman:0.9135\n",
      "Test : Loss:0.1643 \n",
      "pearson： 0.9388641181257839 ALL Pearson: 0.9393758480508905\n",
      "spearman： 0.9232153129752994 ALL Spearman: 0.9217032362316432\n",
      "time: 36.64405179023743\n",
      "0.03\n",
      "Train Epoch:84 [(0%)]\t Loss: 0.1178  Pearson:0.9644 Spearman:0.9420\n",
      "Train ALL Pearson: 0.9504640697730787\n",
      "Train  ALL Spearman: 0.93531684154493\n",
      "32.24923062324524\n",
      "Test Epoch:84 [(0%)]\t Loss: 0.2296  Pearson:0.9359 Spearman:0.9016\n",
      "Test : Loss:0.2197 \n",
      "pearson： 0.93723923021041 ALL Pearson: 0.9398477793849132\n",
      "spearman： 0.9190076487367715 ALL Spearman: 0.923069610138536\n",
      "time: 37.14704084396362\n",
      "0.03\n",
      "Train Epoch:85 [(0%)]\t Loss: 0.1525  Pearson:0.9671 Spearman:0.9511\n",
      "Train ALL Pearson: 0.9496323279246645\n",
      "Train  ALL Spearman: 0.933435404033348\n",
      "31.399553775787354\n",
      "Test Epoch:85 [(0%)]\t Loss: 0.1569  Pearson:0.9364 Spearman:0.9234\n",
      "Test : Loss:0.1452 \n",
      "pearson： 0.9406846771061084 ALL Pearson: 0.9402688905391686\n",
      "spearman： 0.9247101820491893 ALL Spearman: 0.9228458370754721\n",
      "time: 36.24999809265137\n",
      "0.03\n",
      "Train Epoch:86 [(0%)]\t Loss: 0.1413  Pearson:0.9539 Spearman:0.9331\n",
      "Train ALL Pearson: 0.9428486463640392\n",
      "Train  ALL Spearman: 0.9242483484912517\n",
      "31.869898319244385\n",
      "Test Epoch:86 [(0%)]\t Loss: 0.1376  Pearson:0.9568 Spearman:0.9314\n",
      "Test : Loss:0.1508 \n",
      "pearson： 0.9426325745403229 ALL Pearson: 0.9390194916427486\n",
      "spearman： 0.9188943423883156 ALL Spearman: 0.9211291839294976\n",
      "time: 36.58038878440857\n",
      "0.03\n",
      "Train Epoch:87 [(0%)]\t Loss: 0.1480  Pearson:0.9476 Spearman:0.9371\n",
      "Train ALL Pearson: 0.9411818251434585\n",
      "Train  ALL Spearman: 0.9206735065711243\n",
      "31.507447957992554\n",
      "Test Epoch:87 [(0%)]\t Loss: 0.1687  Pearson:0.9299 Spearman:0.9019\n",
      "Test : Loss:0.1555 \n",
      "pearson： 0.9391727506999118 ALL Pearson: 0.9388423434537051\n",
      "spearman： 0.9174326610581837 ALL Spearman: 0.9209932891393506\n",
      "time: 36.31844091415405\n",
      "0.03\n",
      "Train Epoch:88 [(0%)]\t Loss: 0.1503  Pearson:0.9675 Spearman:0.9539\n",
      "Train ALL Pearson: 0.9488248842971447\n",
      "Train  ALL Spearman: 0.9337233530703615\n",
      "32.091941356658936\n",
      "Test Epoch:88 [(0%)]\t Loss: 0.1454  Pearson:0.9488 Spearman:0.9423\n",
      "Test : Loss:0.1466 \n",
      "pearson： 0.9444092401244504 ALL Pearson: 0.9401577802864688\n",
      "spearman： 0.9298273018233373 ALL Spearman: 0.9228270146264103\n",
      "time: 37.057350158691406\n",
      "0.03\n",
      "Train Epoch:89 [(0%)]\t Loss: 0.1062  Pearson:0.9770 Spearman:0.9616\n",
      "Train ALL Pearson: 0.9450083216049844\n",
      "Train  ALL Spearman: 0.9268336205687455\n",
      "31.30567741394043\n",
      "Test Epoch:89 [(0%)]\t Loss: 0.1583  Pearson:0.9405 Spearman:0.9151\n",
      "Test : Loss:0.1609 \n",
      "pearson： 0.940534238889325 ALL Pearson: 0.9385592272675164\n",
      "spearman： 0.9229800069064861 ALL Spearman: 0.9211619999744285\n",
      "time: 35.94618844985962\n",
      "0.03\n",
      "Train Epoch:90 [(0%)]\t Loss: 0.1827  Pearson:0.9578 Spearman:0.9521\n",
      "Train ALL Pearson: 0.9453205680525725\n",
      "Train  ALL Spearman: 0.9291398159182098\n",
      "31.862757444381714\n",
      "Test Epoch:90 [(0%)]\t Loss: 0.1446  Pearson:0.9321 Spearman:0.9100\n",
      "Test : Loss:0.1478 \n",
      "pearson： 0.9378081307087615 ALL Pearson: 0.9402490020428332\n",
      "spearman： 0.9226757315037987 ALL Spearman: 0.9235151005565366\n",
      "time: 36.47827744483948\n",
      "0.03\n",
      "Train Epoch:91 [(0%)]\t Loss: 0.1057  Pearson:0.9696 Spearman:0.9572\n",
      "Train ALL Pearson: 0.9477005221657079\n",
      "Train  ALL Spearman: 0.9298743315355625\n",
      "31.335957527160645\n",
      "Test Epoch:91 [(0%)]\t Loss: 0.1899  Pearson:0.9379 Spearman:0.9231\n",
      "Test : Loss:0.1817 \n",
      "pearson： 0.9400063832671548 ALL Pearson: 0.9395843770907546\n",
      "spearman： 0.9251344774837544 ALL Spearman: 0.9230154195556307\n",
      "time: 35.968472957611084\n",
      "0.03\n",
      "Train Epoch:92 [(0%)]\t Loss: 0.1118  Pearson:0.9730 Spearman:0.9548\n",
      "Train ALL Pearson: 0.9533899650602724\n",
      "Train  ALL Spearman: 0.9397310060072448\n",
      "31.842470407485962\n",
      "Test Epoch:92 [(0%)]\t Loss: 0.1592  Pearson:0.9390 Spearman:0.9328\n",
      "Test : Loss:0.1825 \n",
      "pearson： 0.9397222301045312 ALL Pearson: 0.9389130285196547\n",
      "spearman： 0.9256068510988867 ALL Spearman: 0.9233406987275304\n",
      "time: 36.59446406364441\n",
      "0.03\n",
      "Train Epoch:93 [(0%)]\t Loss: 0.1172  Pearson:0.9619 Spearman:0.9571\n",
      "Train ALL Pearson: 0.9506189292736137\n",
      "Train  ALL Spearman: 0.9350741832346701\n",
      "31.791807174682617\n",
      "Test Epoch:93 [(0%)]\t Loss: 0.2129  Pearson:0.9315 Spearman:0.9207\n",
      "Test : Loss:0.2166 \n",
      "pearson： 0.9378202417210878 ALL Pearson: 0.9380881867156208\n",
      "spearman： 0.9209985951866826 ALL Spearman: 0.9237170820435452\n",
      "time: 36.52129077911377\n",
      "0.03\n",
      "Train Epoch:94 [(0%)]\t Loss: 0.1561  Pearson:0.9524 Spearman:0.9518\n",
      "Train ALL Pearson: 0.9445579782688154\n",
      "Train  ALL Spearman: 0.9268312055686767\n",
      "31.489798545837402\n",
      "Test Epoch:94 [(0%)]\t Loss: 0.2164  Pearson:0.9232 Spearman:0.9047\n",
      "Test : Loss:0.2076 \n",
      "pearson： 0.9340783659412548 ALL Pearson: 0.939347188730323\n",
      "spearman： 0.915840525915733 ALL Spearman: 0.9230542195201625\n",
      "time: 36.24873232841492\n",
      "0.03\n",
      "Train Epoch:95 [(0%)]\t Loss: 0.1513  Pearson:0.9595 Spearman:0.9445\n",
      "Train ALL Pearson: 0.9495856016675316\n",
      "Train  ALL Spearman: 0.933201224026287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.247596502304077\n",
      "Test Epoch:95 [(0%)]\t Loss: 0.1398  Pearson:0.9464 Spearman:0.9230\n",
      "Test : Loss:0.1493 \n",
      "pearson： 0.9407567887327007 ALL Pearson: 0.9386240016614861\n",
      "spearman： 0.9241825998724305 ALL Spearman: 0.920973946360374\n",
      "time: 36.062052726745605\n",
      "0.03\n",
      "Train Epoch:96 [(0%)]\t Loss: 0.1260  Pearson:0.9540 Spearman:0.9320\n",
      "Train ALL Pearson: 0.9458090446036247\n",
      "Train  ALL Spearman: 0.9289569378589096\n",
      "31.480178594589233\n",
      "Test Epoch:96 [(0%)]\t Loss: 0.1485  Pearson:0.9369 Spearman:0.9055\n",
      "Test : Loss:0.1612 \n",
      "pearson： 0.9382832141745446 ALL Pearson: 0.9369410543493075\n",
      "spearman： 0.9168678272323443 ALL Spearman: 0.9186868693451474\n",
      "time: 36.24665021896362\n",
      "0.03\n",
      "Train Epoch:97 [(0%)]\t Loss: 0.1622  Pearson:0.9608 Spearman:0.9492\n",
      "Train ALL Pearson: 0.9506584020569906\n",
      "Train  ALL Spearman: 0.9363485376622291\n",
      "31.589441537857056\n",
      "Test Epoch:97 [(0%)]\t Loss: 0.1975  Pearson:0.9406 Spearman:0.9270\n",
      "Test : Loss:0.2012 \n",
      "pearson： 0.940003370929223 ALL Pearson: 0.9401820999160588\n",
      "spearman： 0.9242736194208168 ALL Spearman: 0.9242709576888939\n",
      "time: 36.28193640708923\n",
      "0.03\n",
      "Train Epoch:98 [(0%)]\t Loss: 0.1346  Pearson:0.9741 Spearman:0.9616\n",
      "Train ALL Pearson: 0.948682795680626\n",
      "Train  ALL Spearman: 0.9307376132183309\n",
      "31.535173892974854\n",
      "Test Epoch:98 [(0%)]\t Loss: 0.1833  Pearson:0.9581 Spearman:0.9315\n",
      "Test : Loss:0.1950 \n",
      "pearson： 0.9450333537331214 ALL Pearson: 0.9409605437761276\n",
      "spearman： 0.9257908521756139 ALL Spearman: 0.9244423336105614\n",
      "time: 36.02773356437683\n",
      "0.03\n",
      "Train Epoch:99 [(0%)]\t Loss: 0.1462  Pearson:0.9583 Spearman:0.9568\n",
      "Train ALL Pearson: 0.9511378102030986\n",
      "Train  ALL Spearman: 0.9360606821161989\n",
      "31.85225224494934\n",
      "Test Epoch:99 [(0%)]\t Loss: 0.1416  Pearson:0.9191 Spearman:0.9088\n",
      "Test : Loss:0.1497 \n",
      "pearson： 0.9342567510031422 ALL Pearson: 0.9390547627168296\n",
      "spearman： 0.9198726856405478 ALL Spearman: 0.9228674690860987\n",
      "time: 36.672513008117676\n",
      "0.03\n",
      "Train Epoch:100 [(0%)]\t Loss: 0.1333  Pearson:0.9643 Spearman:0.9582\n",
      "Train ALL Pearson: 0.9522579795192962\n",
      "Train  ALL Spearman: 0.9388341494970537\n",
      "31.668856382369995\n",
      "Test Epoch:100 [(0%)]\t Loss: 0.1394  Pearson:0.9538 Spearman:0.9413\n",
      "Test : Loss:0.1457 \n",
      "pearson： 0.9422202353487376 ALL Pearson: 0.9395539416873896\n",
      "spearman： 0.9259554279140886 ALL Spearman: 0.9226672576011862\n",
      "time: 36.70224213600159\n",
      "0.03\n",
      "Train Epoch:101 [(0%)]\t Loss: 0.1162  Pearson:0.9634 Spearman:0.9533\n",
      "Train ALL Pearson: 0.9523413572914384\n",
      "Train  ALL Spearman: 0.9383084699239034\n",
      "31.58883237838745\n",
      "Test Epoch:101 [(0%)]\t Loss: 0.2143  Pearson:0.9490 Spearman:0.9271\n",
      "Test : Loss:0.2175 \n",
      "pearson： 0.9414730894824384 ALL Pearson: 0.9386133003544066\n",
      "spearman： 0.9221671416177152 ALL Spearman: 0.9216313181005771\n",
      "time: 36.40628790855408\n",
      "0.03\n",
      "Train Epoch:102 [(0%)]\t Loss: 0.1533  Pearson:0.9587 Spearman:0.9552\n",
      "Train ALL Pearson: 0.945014812356709\n",
      "Train  ALL Spearman: 0.9281102415901615\n",
      "31.69454073905945\n",
      "Test Epoch:102 [(0%)]\t Loss: 0.1864  Pearson:0.9332 Spearman:0.9212\n",
      "Test : Loss:0.1857 \n",
      "pearson： 0.935763596310126 ALL Pearson: 0.9387920689942862\n",
      "spearman： 0.9211238836077523 ALL Spearman: 0.92299940996833\n",
      "time: 36.63495755195618\n",
      "0.03\n",
      "Train Epoch:103 [(0%)]\t Loss: 0.1350  Pearson:0.9648 Spearman:0.9525\n",
      "Train ALL Pearson: 0.9515138190002647\n",
      "Train  ALL Spearman: 0.9356044412666514\n",
      "31.545278787612915\n",
      "Test Epoch:103 [(0%)]\t Loss: 0.1581  Pearson:0.9251 Spearman:0.9033\n",
      "Test : Loss:0.1478 \n",
      "pearson： 0.93865732427746 ALL Pearson: 0.9395865639515905\n",
      "spearman： 0.9182668722556308 ALL Spearman: 0.9229801547994844\n",
      "time: 36.186338901519775\n",
      "0.03\n",
      "Train Epoch:104 [(0%)]\t Loss: 0.1141  Pearson:0.9651 Spearman:0.9518\n",
      "Train ALL Pearson: 0.9476152471038325\n",
      "Train  ALL Spearman: 0.9320672110058451\n",
      "31.63343071937561\n",
      "Test Epoch:104 [(0%)]\t Loss: 0.1477  Pearson:0.9330 Spearman:0.9231\n",
      "Test : Loss:0.1503 \n",
      "pearson： 0.9350036733149003 ALL Pearson: 0.9382620740934311\n",
      "spearman： 0.9190768528789071 ALL Spearman: 0.9217435821517419\n",
      "time: 36.3450722694397\n",
      "0.03\n",
      "Train Epoch:105 [(0%)]\t Loss: 0.1323  Pearson:0.9646 Spearman:0.9587\n",
      "Train ALL Pearson: 0.9557266761984937\n",
      "Train  ALL Spearman: 0.9413286396654431\n",
      "31.37549328804016\n",
      "Test Epoch:105 [(0%)]\t Loss: 0.1540  Pearson:0.9344 Spearman:0.8960\n",
      "Test : Loss:0.1472 \n",
      "pearson： 0.9375272615875333 ALL Pearson: 0.9397721940503672\n",
      "spearman： 0.9190115024572084 ALL Spearman: 0.922987512389253\n",
      "time: 36.04099726676941\n",
      "0.03\n",
      "Train Epoch:106 [(0%)]\t Loss: 0.1076  Pearson:0.9688 Spearman:0.9636\n",
      "Train ALL Pearson: 0.9507859458189477\n",
      "Train  ALL Spearman: 0.9339010830500781\n",
      "31.29457378387451\n",
      "Test Epoch:106 [(0%)]\t Loss: 0.1470  Pearson:0.9305 Spearman:0.9269\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.9368198503822238 ALL Pearson: 0.939407287740909\n",
      "spearman： 0.9214489951798872 ALL Spearman: 0.9220388501311765\n",
      "time: 36.07304096221924\n",
      "0.03\n",
      "Train Epoch:107 [(0%)]\t Loss: 0.1143  Pearson:0.9709 Spearman:0.9627\n",
      "Train ALL Pearson: 0.9536863544352413\n",
      "Train  ALL Spearman: 0.9384067091269607\n",
      "31.496504068374634\n",
      "Test Epoch:107 [(0%)]\t Loss: 0.1477  Pearson:0.9334 Spearman:0.8916\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.9357803124982959 ALL Pearson: 0.938005654996379\n",
      "spearman： 0.9142030493771255 ALL Spearman: 0.9197967459319338\n",
      "time: 36.209991216659546\n",
      "0.03\n",
      "Train Epoch:108 [(0%)]\t Loss: 0.1234  Pearson:0.9587 Spearman:0.9569\n",
      "Train ALL Pearson: 0.9485911527596057\n",
      "Train  ALL Spearman: 0.9331273450407808\n",
      "31.5541250705719\n",
      "Test Epoch:108 [(0%)]\t Loss: 0.1534  Pearson:0.9396 Spearman:0.9226\n",
      "Test : Loss:0.1596 \n",
      "pearson： 0.938757466267494 ALL Pearson: 0.9375328760761767\n",
      "spearman： 0.9196481478409677 ALL Spearman: 0.9193432125847565\n",
      "time: 36.1912145614624\n",
      "0.03\n",
      "Train Epoch:109 [(0%)]\t Loss: 0.1500  Pearson:0.9623 Spearman:0.9587\n",
      "Train ALL Pearson: 0.9571173608173553\n",
      "Train  ALL Spearman: 0.9429368035925977\n",
      "32.19042658805847\n",
      "Test Epoch:109 [(0%)]\t Loss: 0.1975  Pearson:0.9405 Spearman:0.8886\n",
      "Test : Loss:0.1824 \n",
      "pearson： 0.9368785168058728 ALL Pearson: 0.9397300112151311\n",
      "spearman： 0.9135372630475213 ALL Spearman: 0.9231529494643109\n",
      "time: 36.96389842033386\n",
      "0.03\n",
      "Train Epoch:110 [(0%)]\t Loss: 0.1220  Pearson:0.9463 Spearman:0.9490\n",
      "Train ALL Pearson: 0.952539698235065\n",
      "Train  ALL Spearman: 0.9391324280489826\n",
      "31.565690517425537\n",
      "Test Epoch:110 [(0%)]\t Loss: 0.1446  Pearson:0.9404 Spearman:0.9183\n",
      "Test : Loss:0.1506 \n",
      "pearson： 0.9392726887039579 ALL Pearson: 0.9397471264470874\n",
      "spearman： 0.9206028909082065 ALL Spearman: 0.9226005160298675\n",
      "time: 36.510103940963745\n",
      "0.03\n",
      "Train Epoch:111 [(0%)]\t Loss: 0.1016  Pearson:0.9769 Spearman:0.9649\n",
      "Train ALL Pearson: 0.9543950884884839\n",
      "Train  ALL Spearman: 0.9393446076759617\n",
      "31.63544797897339\n",
      "Test Epoch:111 [(0%)]\t Loss: 0.1708  Pearson:0.9361 Spearman:0.8902\n",
      "Test : Loss:0.1612 \n",
      "pearson： 0.9360700647272087 ALL Pearson: 0.9380015481728474\n",
      "spearman： 0.9166210719405089 ALL Spearman: 0.9205304022918263\n",
      "time: 36.247968912124634\n",
      "0.03\n",
      "Train Epoch:112 [(0%)]\t Loss: 0.1549  Pearson:0.9686 Spearman:0.9570\n",
      "Train ALL Pearson: 0.9525113518757794\n",
      "Train  ALL Spearman: 0.9369890534784958\n",
      "31.759918212890625\n",
      "Test Epoch:112 [(0%)]\t Loss: 0.1790  Pearson:0.9424 Spearman:0.9383\n",
      "Test : Loss:0.1945 \n",
      "pearson： 0.940081873264662 ALL Pearson: 0.939312063574367\n",
      "spearman： 0.9271634888180782 ALL Spearman: 0.9227610753961942\n",
      "time: 36.62986779212952\n",
      "0.03\n",
      "Train Epoch:113 [(0%)]\t Loss: 0.1177  Pearson:0.9766 Spearman:0.9564\n",
      "Train ALL Pearson: 0.9569897718970884\n",
      "Train  ALL Spearman: 0.943382576209281\n",
      "31.29141640663147\n",
      "Test Epoch:113 [(0%)]\t Loss: 0.2161  Pearson:0.9309 Spearman:0.9152\n",
      "Test : Loss:0.1949 \n",
      "pearson： 0.9375614030106066 ALL Pearson: 0.9385923981052999\n",
      "spearman： 0.9228884281874673 ALL Spearman: 0.9222806902382322\n",
      "time: 35.837058782577515\n",
      "0.03\n",
      "Train Epoch:114 [(0%)]\t Loss: 0.1390  Pearson:0.9668 Spearman:0.9558\n",
      "Train ALL Pearson: 0.9551064661817457\n",
      "Train  ALL Spearman: 0.9396604748921116\n",
      "31.33114981651306\n",
      "Test Epoch:114 [(0%)]\t Loss: 0.1801  Pearson:0.9426 Spearman:0.9237\n",
      "Test : Loss:0.1791 \n",
      "pearson： 0.9388319685986741 ALL Pearson: 0.9399988254088011\n",
      "spearman： 0.921230701502412 ALL Spearman: 0.9236374297630983\n",
      "time: 36.20758628845215\n",
      "0.03\n",
      "Train Epoch:115 [(0%)]\t Loss: 0.1112  Pearson:0.9771 Spearman:0.9743\n",
      "Train ALL Pearson: 0.9511085330035827\n",
      "Train  ALL Spearman: 0.9347821489880251\n",
      "31.577059984207153\n",
      "Test Epoch:115 [(0%)]\t Loss: 0.1854  Pearson:0.9334 Spearman:0.9079\n",
      "Test : Loss:0.1960 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson： 0.9388918509802782 ALL Pearson: 0.9385285474408988\n",
      "spearman： 0.9200255597284449 ALL Spearman: 0.922609947972253\n",
      "time: 36.21357178688049\n",
      "0.03\n",
      "Train Epoch:116 [(0%)]\t Loss: 0.1296  Pearson:0.9704 Spearman:0.9582\n",
      "Train ALL Pearson: 0.9540552114581707\n",
      "Train  ALL Spearman: 0.9412035342721897\n",
      "31.491530895233154\n",
      "Test Epoch:116 [(0%)]\t Loss: 0.1627  Pearson:0.9237 Spearman:0.9165\n",
      "Test : Loss:0.1549 \n",
      "pearson： 0.9319498844642539 ALL Pearson: 0.9373170508871991\n",
      "spearman： 0.9150002386073531 ALL Spearman: 0.919601249289867\n",
      "time: 36.20202016830444\n",
      "0.03\n",
      "Train Epoch:117 [(0%)]\t Loss: 0.1233  Pearson:0.9728 Spearman:0.9458\n",
      "Train ALL Pearson: 0.9525920422194558\n",
      "Train  ALL Spearman: 0.938376211859802\n",
      "31.5565402507782\n",
      "Test Epoch:117 [(0%)]\t Loss: 0.1820  Pearson:0.9446 Spearman:0.9237\n",
      "Test : Loss:0.1817 \n",
      "pearson： 0.9406242427405346 ALL Pearson: 0.9390026156085479\n",
      "spearman： 0.9235991683269625 ALL Spearman: 0.9224902449553777\n",
      "time: 36.120075941085815\n",
      "0.03\n",
      "Train Epoch:118 [(0%)]\t Loss: 0.1240  Pearson:0.9705 Spearman:0.9529\n",
      "Train ALL Pearson: 0.956151023343953\n",
      "Train  ALL Spearman: 0.9404334610598342\n",
      "31.955726861953735\n",
      "Test Epoch:118 [(0%)]\t Loss: 0.1672  Pearson:0.9318 Spearman:0.8914\n",
      "Test : Loss:0.1534 \n",
      "pearson： 0.9379202730029822 ALL Pearson: 0.9382654936417307\n",
      "spearman： 0.918073019582557 ALL Spearman: 0.921578000629949\n",
      "time: 36.91013813018799\n",
      "0.03\n",
      "Train Epoch:119 [(0%)]\t Loss: 0.1369  Pearson:0.9700 Spearman:0.9640\n",
      "Train ALL Pearson: 0.956419814159672\n",
      "Train  ALL Spearman: 0.9435202160284311\n",
      "31.471478939056396\n",
      "Test Epoch:119 [(0%)]\t Loss: 0.1635  Pearson:0.9403 Spearman:0.9023\n",
      "Test : Loss:0.1526 \n",
      "pearson： 0.9396990828164644 ALL Pearson: 0.9384875211502369\n",
      "spearman： 0.9189926435125403 ALL Spearman: 0.921826378046267\n",
      "time: 36.37990593910217\n",
      "0.009\n",
      "Train Epoch:120 [(0%)]\t Loss: 0.1227  Pearson:0.9646 Spearman:0.9606\n",
      "Train ALL Pearson: 0.9633245661917452\n",
      "Train  ALL Spearman: 0.9515134053290869\n",
      "31.955228328704834\n",
      "Test Epoch:120 [(0%)]\t Loss: 0.1572  Pearson:0.9418 Spearman:0.9207\n",
      "Test : Loss:0.1516 \n",
      "pearson： 0.9415849280032216 ALL Pearson: 0.9410339554643937\n",
      "spearman： 0.9259448686627838 ALL Spearman: 0.9247375872615756\n",
      "time: 36.58774256706238\n",
      "0.009\n",
      "Train Epoch:121 [(0%)]\t Loss: 0.0998  Pearson:0.9727 Spearman:0.9544\n",
      "Train ALL Pearson: 0.9659594947652166\n",
      "Train  ALL Spearman: 0.9549927121932169\n",
      "32.2918701171875\n",
      "Test Epoch:121 [(0%)]\t Loss: 0.1600  Pearson:0.9328 Spearman:0.9214\n",
      "Test : Loss:0.1522 \n",
      "pearson： 0.9404561087142074 ALL Pearson: 0.9409005031468706\n",
      "spearman： 0.9254652950743805 ALL Spearman: 0.9246562918533064\n",
      "time: 36.96637201309204\n",
      "0.009\n",
      "Train Epoch:122 [(0%)]\t Loss: 0.1007  Pearson:0.9683 Spearman:0.9624\n",
      "Train ALL Pearson: 0.9664083692541986\n",
      "Train  ALL Spearman: 0.955819999449621\n",
      "31.13057780265808\n",
      "Test Epoch:122 [(0%)]\t Loss: 0.1764  Pearson:0.9308 Spearman:0.9164\n",
      "Test : Loss:0.1539 \n",
      "pearson： 0.9346915839081937 ALL Pearson: 0.9410409796454391\n",
      "spearman： 0.919678297829105 ALL Spearman: 0.9246669012576948\n",
      "time: 36.03500580787659\n",
      "0.009\n",
      "Train Epoch:123 [(0%)]\t Loss: 0.1014  Pearson:0.9685 Spearman:0.9643\n",
      "Train ALL Pearson: 0.9664518502605896\n",
      "Train  ALL Spearman: 0.9552838033779787\n",
      "31.808534383773804\n",
      "Test Epoch:123 [(0%)]\t Loss: 0.1519  Pearson:0.9368 Spearman:0.9214\n",
      "Test : Loss:0.1510 \n",
      "pearson： 0.9409324043061038 ALL Pearson: 0.9412643341646149\n",
      "spearman： 0.926276721567682 ALL Spearman: 0.925130320506696\n",
      "time: 36.5969979763031\n",
      "0.009\n",
      "Train Epoch:124 [(0%)]\t Loss: 0.1115  Pearson:0.9629 Spearman:0.9586\n",
      "Train ALL Pearson: 0.9668897511557419\n",
      "Train  ALL Spearman: 0.9559000841926348\n",
      "31.858766078948975\n",
      "Test Epoch:124 [(0%)]\t Loss: 0.1532  Pearson:0.9505 Spearman:0.9392\n",
      "Test : Loss:0.1493 \n",
      "pearson： 0.9431359622838239 ALL Pearson: 0.9411871590051044\n",
      "spearman： 0.9274339765708987 ALL Spearman: 0.925358941520252\n",
      "time: 36.606242418289185\n",
      "0.009\n",
      "Train Epoch:125 [(0%)]\t Loss: 0.1074  Pearson:0.9675 Spearman:0.9509\n",
      "Train ALL Pearson: 0.9675875214866073\n",
      "Train  ALL Spearman: 0.9564338123544616\n",
      "31.50404691696167\n",
      "Test Epoch:125 [(0%)]\t Loss: 0.1407  Pearson:0.9432 Spearman:0.9170\n",
      "Test : Loss:0.1533 \n",
      "pearson： 0.9415786978185793 ALL Pearson: 0.9412429735301046\n",
      "spearman： 0.924425161362036 ALL Spearman: 0.9253540719068909\n",
      "time: 35.970614433288574\n",
      "0.009\n",
      "Train Epoch:126 [(0%)]\t Loss: 0.1097  Pearson:0.9637 Spearman:0.9545\n",
      "Train ALL Pearson: 0.9669205818809019\n",
      "Train  ALL Spearman: 0.9565164586777247\n",
      "31.748221158981323\n",
      "Test Epoch:126 [(0%)]\t Loss: 0.1487  Pearson:0.9450 Spearman:0.9213\n",
      "Test : Loss:0.1496 \n",
      "pearson： 0.9418575474394967 ALL Pearson: 0.941197318641677\n",
      "spearman： 0.9233951992522562 ALL Spearman: 0.9252456727459688\n",
      "time: 36.61720943450928\n",
      "0.009\n",
      "Train Epoch:127 [(0%)]\t Loss: 0.1029  Pearson:0.9718 Spearman:0.9589\n",
      "Train ALL Pearson: 0.9668582563749433\n",
      "Train  ALL Spearman: 0.955298965284578\n",
      "31.755820751190186\n",
      "Test Epoch:127 [(0%)]\t Loss: 0.1433  Pearson:0.9388 Spearman:0.9274\n",
      "Test : Loss:0.1514 \n",
      "pearson： 0.9424943919852368 ALL Pearson: 0.9415383479143136\n",
      "spearman： 0.9262152832026089 ALL Spearman: 0.9256397422010756\n",
      "time: 36.425785541534424\n",
      "0.009\n",
      "Train Epoch:128 [(0%)]\t Loss: 0.0965  Pearson:0.9755 Spearman:0.9621\n",
      "Train ALL Pearson: 0.9682366514221736\n",
      "Train  ALL Spearman: 0.9581985591527985\n",
      "32.04450249671936\n",
      "Test Epoch:128 [(0%)]\t Loss: 0.1609  Pearson:0.9164 Spearman:0.9017\n",
      "Test : Loss:0.1501 \n",
      "pearson： 0.9375543246598089 ALL Pearson: 0.9415217541149224\n",
      "spearman： 0.9205304999317803 ALL Spearman: 0.925165273183322\n",
      "time: 36.794979095458984\n",
      "0.009\n",
      "Train Epoch:129 [(0%)]\t Loss: 0.0992  Pearson:0.9680 Spearman:0.9489\n",
      "Train ALL Pearson: 0.9674292577814286\n",
      "Train  ALL Spearman: 0.9569676013973583\n",
      "31.667384147644043\n",
      "Test Epoch:129 [(0%)]\t Loss: 0.1592  Pearson:0.9362 Spearman:0.9147\n",
      "Test : Loss:0.1566 \n",
      "pearson： 0.9393269110654354 ALL Pearson: 0.9407774816123718\n",
      "spearman： 0.919472624101466 ALL Spearman: 0.9247938236230532\n",
      "time: 36.453609466552734\n",
      "0.009\n",
      "Train Epoch:130 [(0%)]\t Loss: 0.1064  Pearson:0.9611 Spearman:0.9559\n",
      "Train ALL Pearson: 0.9677127353902941\n",
      "Train  ALL Spearman: 0.9571687681196148\n",
      "31.66238236427307\n",
      "Test Epoch:130 [(0%)]\t Loss: 0.1437  Pearson:0.9514 Spearman:0.9198\n",
      "Test : Loss:0.1524 \n",
      "pearson： 0.9424112651879972 ALL Pearson: 0.9410507717289197\n",
      "spearman： 0.9270934631257567 ALL Spearman: 0.9250092600117586\n",
      "time: 36.28891658782959\n",
      "0.009\n",
      "Train Epoch:131 [(0%)]\t Loss: 0.1069  Pearson:0.9618 Spearman:0.9465\n",
      "Train ALL Pearson: 0.9681061934147851\n",
      "Train  ALL Spearman: 0.9573000580559603\n",
      "31.856170892715454\n",
      "Test Epoch:131 [(0%)]\t Loss: 0.1517  Pearson:0.9394 Spearman:0.9186\n",
      "Test : Loss:0.1514 \n",
      "pearson： 0.9424696847572914 ALL Pearson: 0.9412361609499016\n",
      "spearman： 0.9235879759144254 ALL Spearman: 0.9252226818179484\n",
      "time: 36.652159690856934\n",
      "0.009\n",
      "Train Epoch:132 [(0%)]\t Loss: 0.1009  Pearson:0.9698 Spearman:0.9585\n",
      "Train ALL Pearson: 0.9687063544353327\n",
      "Train  ALL Spearman: 0.9585493259259908\n",
      "31.396695852279663\n",
      "Test Epoch:132 [(0%)]\t Loss: 0.1536  Pearson:0.9404 Spearman:0.9107\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.9397070827215426 ALL Pearson: 0.9410226269868677\n",
      "spearman： 0.9218109649459886 ALL Spearman: 0.9247481218616325\n",
      "time: 36.228835582733154\n",
      "0.009\n",
      "Train Epoch:133 [(0%)]\t Loss: 0.1065  Pearson:0.9655 Spearman:0.9562\n",
      "Train ALL Pearson: 0.9689835877263554\n",
      "Train  ALL Spearman: 0.958904381426149\n",
      "31.54671597480774\n",
      "Test Epoch:133 [(0%)]\t Loss: 0.1537  Pearson:0.9485 Spearman:0.9293\n",
      "Test : Loss:0.1532 \n",
      "pearson： 0.9433644995796938 ALL Pearson: 0.9415425021374897\n",
      "spearman： 0.9248057868538413 ALL Spearman: 0.9256172671753121\n",
      "time: 36.524654388427734\n",
      "0.009\n",
      "Train Epoch:134 [(0%)]\t Loss: 0.0986  Pearson:0.9711 Spearman:0.9568\n",
      "Train ALL Pearson: 0.9687215735831185\n",
      "Train  ALL Spearman: 0.9577295023299606\n",
      "31.422895669937134\n",
      "Test Epoch:134 [(0%)]\t Loss: 0.1513  Pearson:0.9408 Spearman:0.9325\n",
      "Test : Loss:0.1504 \n",
      "pearson： 0.9414650160531238 ALL Pearson: 0.9413744795048145\n",
      "spearman： 0.9272073073964275 ALL Spearman: 0.9253548012214602\n",
      "time: 36.294333696365356\n",
      "0.009\n",
      "Train Epoch:135 [(0%)]\t Loss: 0.1131  Pearson:0.9650 Spearman:0.9501\n",
      "Train ALL Pearson: 0.9690574658667069\n",
      "Train  ALL Spearman: 0.9586964120886416\n",
      "31.84655261039734\n",
      "Test Epoch:135 [(0%)]\t Loss: 0.1573  Pearson:0.9461 Spearman:0.9259\n",
      "Test : Loss:0.1471 \n",
      "pearson： 0.9419840096952554 ALL Pearson: 0.942065560928402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.926761243092804 ALL Spearman: 0.926230238557667\n",
      "time: 36.5140540599823\n",
      "0.009\n",
      "Train Epoch:136 [(0%)]\t Loss: 0.1005  Pearson:0.9715 Spearman:0.9627\n",
      "Train ALL Pearson: 0.969572830467996\n",
      "Train  ALL Spearman: 0.9590545257192206\n",
      "31.692320585250854\n",
      "Test Epoch:136 [(0%)]\t Loss: 0.1339  Pearson:0.9492 Spearman:0.9331\n",
      "Test : Loss:0.1477 \n",
      "pearson： 0.9443906706789893 ALL Pearson: 0.9417178666964288\n",
      "spearman： 0.931234588592183 ALL Spearman: 0.92586092165337\n",
      "time: 36.38481569290161\n",
      "0.009\n",
      "Train Epoch:137 [(0%)]\t Loss: 0.1006  Pearson:0.9769 Spearman:0.9638\n",
      "Train ALL Pearson: 0.96951377256711\n",
      "Train  ALL Spearman: 0.9594204347239906\n",
      "31.858219861984253\n",
      "Test Epoch:137 [(0%)]\t Loss: 0.1575  Pearson:0.9307 Spearman:0.9180\n",
      "Test : Loss:0.1500 \n",
      "pearson： 0.9387862634582343 ALL Pearson: 0.9419271940858592\n",
      "spearman： 0.9255975269380035 ALL Spearman: 0.9261135357799876\n",
      "time: 36.689794301986694\n",
      "0.009\n",
      "Train Epoch:138 [(0%)]\t Loss: 0.1117  Pearson:0.9636 Spearman:0.9565\n",
      "Train ALL Pearson: 0.968954257644507\n",
      "Train  ALL Spearman: 0.9583609038389282\n",
      "31.906392097473145\n",
      "Test Epoch:138 [(0%)]\t Loss: 0.1650  Pearson:0.9417 Spearman:0.9186\n",
      "Test : Loss:0.1527 \n",
      "pearson： 0.9411953750939961 ALL Pearson: 0.9416185032551372\n",
      "spearman： 0.921916195057884 ALL Spearman: 0.9256705329717037\n",
      "time: 36.703474044799805\n",
      "0.009\n",
      "Train Epoch:139 [(0%)]\t Loss: 0.1038  Pearson:0.9664 Spearman:0.9531\n",
      "Train ALL Pearson: 0.9695875876290116\n",
      "Train  ALL Spearman: 0.9593674962259067\n",
      "31.522847890853882\n",
      "Test Epoch:139 [(0%)]\t Loss: 0.1523  Pearson:0.9539 Spearman:0.9389\n",
      "Test : Loss:0.1517 \n",
      "pearson： 0.9404482829530066 ALL Pearson: 0.9414320930148746\n",
      "spearman： 0.9260835871442061 ALL Spearman: 0.9258904275501116\n",
      "time: 36.18735194206238\n",
      "0.009\n",
      "Train Epoch:140 [(0%)]\t Loss: 0.0990  Pearson:0.9705 Spearman:0.9670\n",
      "Train ALL Pearson: 0.9698325064289489\n",
      "Train  ALL Spearman: 0.9598090220335022\n",
      "31.493034839630127\n",
      "Test Epoch:140 [(0%)]\t Loss: 0.1577  Pearson:0.9413 Spearman:0.9182\n",
      "Test : Loss:0.1549 \n",
      "pearson： 0.9426848942537194 ALL Pearson: 0.9412890875466622\n",
      "spearman： 0.925326490380277 ALL Spearman: 0.925464905086375\n",
      "time: 36.418456077575684\n",
      "0.009\n",
      "Train Epoch:141 [(0%)]\t Loss: 0.0869  Pearson:0.9714 Spearman:0.9599\n",
      "Train ALL Pearson: 0.9703989582378864\n",
      "Train  ALL Spearman: 0.9602313870442595\n",
      "31.657576322555542\n",
      "Test Epoch:141 [(0%)]\t Loss: 0.1467  Pearson:0.9431 Spearman:0.9254\n",
      "Test : Loss:0.1500 \n",
      "pearson： 0.9428210021764222 ALL Pearson: 0.941583295741194\n",
      "spearman： 0.925844198965838 ALL Spearman: 0.9260248758149149\n",
      "time: 36.5390100479126\n",
      "0.009\n",
      "Train Epoch:142 [(0%)]\t Loss: 0.0854  Pearson:0.9757 Spearman:0.9647\n",
      "Train ALL Pearson: 0.9705620435822025\n",
      "Train  ALL Spearman: 0.9603284573903731\n",
      "31.357466220855713\n",
      "Test Epoch:142 [(0%)]\t Loss: 0.1562  Pearson:0.9368 Spearman:0.9187\n",
      "Test : Loss:0.1553 \n",
      "pearson： 0.9424853204604831 ALL Pearson: 0.9414480575228109\n",
      "spearman： 0.9261537452447457 ALL Spearman: 0.9262823257925888\n",
      "time: 36.16784119606018\n",
      "0.009\n",
      "Train Epoch:143 [(0%)]\t Loss: 0.0918  Pearson:0.9719 Spearman:0.9498\n",
      "Train ALL Pearson: 0.9704742882408393\n",
      "Train  ALL Spearman: 0.9605243738447293\n",
      "31.600293159484863\n",
      "Test Epoch:143 [(0%)]\t Loss: 0.1559  Pearson:0.9344 Spearman:0.9065\n",
      "Test : Loss:0.1472 \n",
      "pearson： 0.9393510638351578 ALL Pearson: 0.9420021592806833\n",
      "spearman： 0.9228119836649992 ALL Spearman: 0.9263672851718886\n",
      "time: 36.44222068786621\n",
      "0.009\n",
      "Train Epoch:144 [(0%)]\t Loss: 0.1057  Pearson:0.9685 Spearman:0.9470\n",
      "Train ALL Pearson: 0.9706716945536635\n",
      "Train  ALL Spearman: 0.960572716894237\n",
      "31.57593870162964\n",
      "Test Epoch:144 [(0%)]\t Loss: 0.1691  Pearson:0.9265 Spearman:0.9112\n",
      "Test : Loss:0.1553 \n",
      "pearson： 0.9374612093241822 ALL Pearson: 0.9415386431634111\n",
      "spearman： 0.9233279461610291 ALL Spearman: 0.9260105177892541\n",
      "time: 36.492361545562744\n",
      "0.009\n",
      "Train Epoch:145 [(0%)]\t Loss: 0.0940  Pearson:0.9787 Spearman:0.9598\n",
      "Train ALL Pearson: 0.9700802266364081\n",
      "Train  ALL Spearman: 0.959134475488616\n",
      "32.23621654510498\n",
      "Test Epoch:145 [(0%)]\t Loss: 0.1551  Pearson:0.9576 Spearman:0.9476\n",
      "Test : Loss:0.1522 \n",
      "pearson： 0.9439698556789434 ALL Pearson: 0.9417466254971503\n",
      "spearman： 0.9245906097524922 ALL Spearman: 0.9263394652784253\n",
      "time: 36.908716917037964\n",
      "0.009\n",
      "Train Epoch:146 [(0%)]\t Loss: 0.0878  Pearson:0.9792 Spearman:0.9692\n",
      "Train ALL Pearson: 0.9710851928241887\n",
      "Train  ALL Spearman: 0.9612880900646462\n",
      "31.799798250198364\n",
      "Test Epoch:146 [(0%)]\t Loss: 0.1457  Pearson:0.9343 Spearman:0.9170\n",
      "Test : Loss:0.1520 \n",
      "pearson： 0.941193760328601 ALL Pearson: 0.9419188928881932\n",
      "spearman： 0.924934353466861 ALL Spearman: 0.9262776930595144\n",
      "time: 36.394325256347656\n",
      "0.009\n",
      "Train Epoch:147 [(0%)]\t Loss: 0.0891  Pearson:0.9788 Spearman:0.9687\n",
      "Train ALL Pearson: 0.9707176842132579\n",
      "Train  ALL Spearman: 0.9608657272377656\n",
      "31.821694374084473\n",
      "Test Epoch:147 [(0%)]\t Loss: 0.1520  Pearson:0.9372 Spearman:0.9192\n",
      "Test : Loss:0.1522 \n",
      "pearson： 0.9411349220538249 ALL Pearson: 0.9417689905369275\n",
      "spearman： 0.9258022740277206 ALL Spearman: 0.9261407784799169\n",
      "time: 36.513349533081055\n",
      "0.009\n",
      "Train Epoch:148 [(0%)]\t Loss: 0.0829  Pearson:0.9785 Spearman:0.9703\n",
      "Train ALL Pearson: 0.9720074217239769\n",
      "Train  ALL Spearman: 0.9629125287868296\n",
      "31.857348442077637\n",
      "Test Epoch:148 [(0%)]\t Loss: 0.1592  Pearson:0.9381 Spearman:0.9270\n",
      "Test : Loss:0.1525 \n",
      "pearson： 0.9408841451075509 ALL Pearson: 0.9412845737381597\n",
      "spearman： 0.9278965948458254 ALL Spearman: 0.9258963000273046\n",
      "time: 36.57283663749695\n",
      "0.009\n",
      "Train Epoch:149 [(0%)]\t Loss: 0.1012  Pearson:0.9779 Spearman:0.9667\n",
      "Train ALL Pearson: 0.9713864801990677\n",
      "Train  ALL Spearman: 0.9621146817457995\n",
      "31.67754030227661\n",
      "Test Epoch:149 [(0%)]\t Loss: 0.1408  Pearson:0.9479 Spearman:0.9373\n",
      "Test : Loss:0.1475 \n",
      "pearson： 0.9430595230800932 ALL Pearson: 0.9412727301576036\n",
      "spearman： 0.9276533255095053 ALL Spearman: 0.9256694343751956\n",
      "time: 36.396610260009766\n",
      "0.009\n",
      "Train Epoch:150 [(0%)]\t Loss: 0.0932  Pearson:0.9738 Spearman:0.9627\n",
      "Train ALL Pearson: 0.9718788318472973\n",
      "Train  ALL Spearman: 0.9623718681123608\n",
      "31.779810428619385\n",
      "Test Epoch:150 [(0%)]\t Loss: 0.1493  Pearson:0.9378 Spearman:0.9240\n",
      "Test : Loss:0.1504 \n",
      "pearson： 0.9408577906648259 ALL Pearson: 0.9415490413559835\n",
      "spearman： 0.9235684402389409 ALL Spearman: 0.9257728557224574\n",
      "time: 36.560293436050415\n",
      "0.009\n",
      "Train Epoch:151 [(0%)]\t Loss: 0.0883  Pearson:0.9734 Spearman:0.9673\n",
      "Train ALL Pearson: 0.9720829163987654\n",
      "Train  ALL Spearman: 0.9618264796884951\n",
      "31.664183616638184\n",
      "Test Epoch:151 [(0%)]\t Loss: 0.1497  Pearson:0.9431 Spearman:0.9195\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.9407095611141291 ALL Pearson: 0.9415807001675188\n",
      "spearman： 0.9239208932530443 ALL Spearman: 0.9260569800610748\n",
      "time: 36.41166114807129\n",
      "0.009\n",
      "Train Epoch:152 [(0%)]\t Loss: 0.1033  Pearson:0.9601 Spearman:0.9464\n",
      "Train ALL Pearson: 0.9718336283364566\n",
      "Train  ALL Spearman: 0.9625465414189143\n",
      "31.637705087661743\n",
      "Test Epoch:152 [(0%)]\t Loss: 0.1295  Pearson:0.9554 Spearman:0.9381\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9454505598685089 ALL Pearson: 0.9413965894315484\n",
      "spearman： 0.9268981751750268 ALL Spearman: 0.925406728268237\n",
      "time: 36.32232141494751\n",
      "0.009\n",
      "Train Epoch:153 [(0%)]\t Loss: 0.0909  Pearson:0.9764 Spearman:0.9671\n",
      "Train ALL Pearson: 0.9718118828367022\n",
      "Train  ALL Spearman: 0.9625428629361201\n",
      "31.66556477546692\n",
      "Test Epoch:153 [(0%)]\t Loss: 0.1538  Pearson:0.9365 Spearman:0.9338\n",
      "Test : Loss:0.1511 \n",
      "pearson： 0.9397108684308869 ALL Pearson: 0.9413961435841824\n",
      "spearman： 0.9255580334166329 ALL Spearman: 0.9258700492449203\n",
      "time: 36.2087516784668\n",
      "0.009\n",
      "Train Epoch:154 [(0%)]\t Loss: 0.0998  Pearson:0.9717 Spearman:0.9587\n",
      "Train ALL Pearson: 0.9724861847807242\n",
      "Train  ALL Spearman: 0.9628063064188355\n",
      "31.91815686225891\n",
      "Test Epoch:154 [(0%)]\t Loss: 0.1647  Pearson:0.9284 Spearman:0.9156\n",
      "Test : Loss:0.1530 \n",
      "pearson： 0.9400626681648273 ALL Pearson: 0.9412844916295631\n",
      "spearman： 0.9249887736253555 ALL Spearman: 0.9260314996624869\n",
      "time: 36.91055631637573\n",
      "0.009\n",
      "Train Epoch:155 [(0%)]\t Loss: 0.0899  Pearson:0.9742 Spearman:0.9653\n",
      "Train ALL Pearson: 0.9722830557612646\n",
      "Train  ALL Spearman: 0.9625330409975084\n",
      "31.683393478393555\n",
      "Test Epoch:155 [(0%)]\t Loss: 0.1532  Pearson:0.9347 Spearman:0.9258\n",
      "Test : Loss:0.1459 \n",
      "pearson： 0.9372002996736483 ALL Pearson: 0.9410961926088056\n",
      "spearman： 0.921501852611409 ALL Spearman: 0.9253224531553159\n",
      "time: 36.476216316223145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009\n",
      "Train Epoch:156 [(0%)]\t Loss: 0.1041  Pearson:0.9692 Spearman:0.9577\n",
      "Train ALL Pearson: 0.9725877208088686\n",
      "Train  ALL Spearman: 0.9633538129739272\n",
      "32.027029037475586\n",
      "Test Epoch:156 [(0%)]\t Loss: 0.1454  Pearson:0.9608 Spearman:0.9417\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9439896263609295 ALL Pearson: 0.9408194541276627\n",
      "spearman： 0.9258841225067843 ALL Spearman: 0.9252478835064334\n",
      "time: 36.65954256057739\n",
      "0.0026999999999999997\n",
      "Train Epoch:157 [(0%)]\t Loss: 0.0905  Pearson:0.9816 Spearman:0.9748\n",
      "Train ALL Pearson: 0.9705074973611595\n",
      "Train  ALL Spearman: 0.9601075948257781\n",
      "31.441629886627197\n",
      "Test Epoch:157 [(0%)]\t Loss: 0.1506  Pearson:0.9379 Spearman:0.9357\n",
      "Test : Loss:0.1506 \n",
      "pearson： 0.941432497054604 ALL Pearson: 0.9418874678028366\n",
      "spearman： 0.9285890753793656 ALL Spearman: 0.9264511495945262\n",
      "time: 36.08914136886597\n",
      "0.0026999999999999997\n",
      "Train Epoch:158 [(0%)]\t Loss: 0.0915  Pearson:0.9799 Spearman:0.9707\n",
      "Train ALL Pearson: 0.9692313014819339\n",
      "Train  ALL Spearman: 0.958248306578428\n",
      "31.252241611480713\n",
      "Test Epoch:158 [(0%)]\t Loss: 0.1574  Pearson:0.9382 Spearman:0.9173\n",
      "Test : Loss:0.1490 \n",
      "pearson： 0.9386358033398924 ALL Pearson: 0.941913998706903\n",
      "spearman： 0.9235789105343215 ALL Spearman: 0.9264096090057278\n",
      "time: 35.851765871047974\n",
      "0.0026999999999999997\n",
      "Train Epoch:159 [(0%)]\t Loss: 0.1036  Pearson:0.9668 Spearman:0.9619\n",
      "Train ALL Pearson: 0.9699837829617509\n",
      "Train  ALL Spearman: 0.960256102845154\n",
      "31.494091510772705\n",
      "Test Epoch:159 [(0%)]\t Loss: 0.1548  Pearson:0.9364 Spearman:0.9139\n",
      "Test : Loss:0.1512 \n",
      "pearson： 0.9355448606651016 ALL Pearson: 0.9419519480166625\n",
      "spearman： 0.9186880410424224 ALL Spearman: 0.926140950823161\n",
      "time: 36.120038747787476\n",
      "0.0026999999999999997\n",
      "Train Epoch:160 [(0%)]\t Loss: 0.1001  Pearson:0.9741 Spearman:0.9638\n",
      "Train ALL Pearson: 0.9706585551755907\n",
      "Train  ALL Spearman: 0.9611352771025039\n",
      "31.98153519630432\n",
      "Test Epoch:160 [(0%)]\t Loss: 0.1545  Pearson:0.9479 Spearman:0.9282\n",
      "Test : Loss:0.1524 \n",
      "pearson： 0.9420528082014896 ALL Pearson: 0.941589955136877\n",
      "spearman： 0.9222974533326724 ALL Spearman: 0.9257021993654424\n",
      "time: 36.717485666275024\n",
      "0.0026999999999999997\n",
      "Train Epoch:161 [(0%)]\t Loss: 0.0937  Pearson:0.9711 Spearman:0.9637\n",
      "Train ALL Pearson: 0.9695191213243071\n",
      "Train  ALL Spearman: 0.9596943674672992\n",
      "31.67443084716797\n",
      "Test Epoch:161 [(0%)]\t Loss: 0.1612  Pearson:0.9310 Spearman:0.9152\n",
      "Test : Loss:0.1512 \n",
      "pearson： 0.9367043640202045 ALL Pearson: 0.9416598131576522\n",
      "spearman： 0.923643485179324 ALL Spearman: 0.9259404033968257\n",
      "time: 36.35693073272705\n",
      "0.0026999999999999997\n",
      "Train Epoch:162 [(0%)]\t Loss: 0.1016  Pearson:0.9717 Spearman:0.9582\n",
      "Train ALL Pearson: 0.9696557903924642\n",
      "Train  ALL Spearman: 0.9593571437949101\n",
      "31.485243797302246\n",
      "Test Epoch:162 [(0%)]\t Loss: 0.1355  Pearson:0.9521 Spearman:0.9172\n",
      "Test : Loss:0.1524 \n",
      "pearson： 0.9429910203769893 ALL Pearson: 0.941830562981284\n",
      "spearman： 0.9229984817435898 ALL Spearman: 0.9261789726762805\n",
      "time: 36.12675499916077\n",
      "0.0026999999999999997\n",
      "Train Epoch:163 [(0%)]\t Loss: 0.0982  Pearson:0.9754 Spearman:0.9699\n",
      "Train ALL Pearson: 0.9701292455971579\n",
      "Train  ALL Spearman: 0.9599394365461357\n",
      "31.57241415977478\n",
      "Test Epoch:163 [(0%)]\t Loss: 0.1585  Pearson:0.9520 Spearman:0.9434\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.9430065660314039 ALL Pearson: 0.9419357817354134\n",
      "spearman： 0.929580544594602 ALL Spearman: 0.9261577634566408\n",
      "time: 36.37687397003174\n",
      "0.0026999999999999997\n",
      "Train Epoch:164 [(0%)]\t Loss: 0.1014  Pearson:0.9712 Spearman:0.9636\n",
      "Train ALL Pearson: 0.970282677055136\n",
      "Train  ALL Spearman: 0.9602384815758004\n",
      "31.86356806755066\n",
      "Test Epoch:164 [(0%)]\t Loss: 0.1476  Pearson:0.9221 Spearman:0.8833\n",
      "Test : Loss:0.1503 \n",
      "pearson： 0.9362404836157338 ALL Pearson: 0.9418780994218743\n",
      "spearman： 0.9116039518486594 ALL Spearman: 0.9260738066286893\n",
      "time: 36.46009373664856\n",
      "0.0026999999999999997\n",
      "Train Epoch:165 [(0%)]\t Loss: 0.0868  Pearson:0.9731 Spearman:0.9480\n",
      "Train ALL Pearson: 0.9706398461093951\n",
      "Train  ALL Spearman: 0.9603496766528797\n",
      "31.815986156463623\n",
      "Test Epoch:165 [(0%)]\t Loss: 0.1482  Pearson:0.9443 Spearman:0.9282\n",
      "Test : Loss:0.1531 \n",
      "pearson： 0.9438711428341094 ALL Pearson: 0.9419382488949078\n",
      "spearman： 0.9279646189896841 ALL Spearman: 0.9260286368769494\n",
      "time: 36.400134325027466\n",
      "0.0026999999999999997\n",
      "Train Epoch:166 [(0%)]\t Loss: 0.0942  Pearson:0.9788 Spearman:0.9670\n",
      "Train ALL Pearson: 0.9708889423578896\n",
      "Train  ALL Spearman: 0.9610305445242586\n",
      "31.46691346168518\n",
      "Test Epoch:166 [(0%)]\t Loss: 0.1648  Pearson:0.9385 Spearman:0.9254\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9407188439865848 ALL Pearson: 0.9416417531990313\n",
      "spearman： 0.923613125705906 ALL Spearman: 0.9257855005829355\n",
      "time: 36.37234115600586\n",
      "0.0026999999999999997\n",
      "Train Epoch:167 [(0%)]\t Loss: 0.0919  Pearson:0.9716 Spearman:0.9596\n",
      "Train ALL Pearson: 0.9702539873521339\n",
      "Train  ALL Spearman: 0.9604893585978136\n",
      "31.94061851501465\n",
      "Test Epoch:167 [(0%)]\t Loss: 0.1510  Pearson:0.9454 Spearman:0.9256\n",
      "Test : Loss:0.1477 \n",
      "pearson： 0.9432236058331608 ALL Pearson: 0.9415687622583236\n",
      "spearman： 0.9246158762351016 ALL Spearman: 0.9257490756277535\n",
      "time: 36.63611197471619\n",
      "0.0026999999999999997\n",
      "Train Epoch:168 [(0%)]\t Loss: 0.0901  Pearson:0.9807 Spearman:0.9678\n",
      "Train ALL Pearson: 0.9709525201268161\n",
      "Train  ALL Spearman: 0.9609145011459155\n",
      "31.695393800735474\n",
      "Test Epoch:168 [(0%)]\t Loss: 0.1626  Pearson:0.9534 Spearman:0.9196\n",
      "Test : Loss:0.1523 \n",
      "pearson： 0.9448094810126577 ALL Pearson: 0.9414054016586054\n",
      "spearman： 0.9277932568610917 ALL Spearman: 0.9257667701725067\n",
      "time: 36.34790229797363\n",
      "0.0026999999999999997\n",
      "Train Epoch:169 [(0%)]\t Loss: 0.1033  Pearson:0.9690 Spearman:0.9535\n",
      "Train ALL Pearson: 0.9705853955506513\n",
      "Train  ALL Spearman: 0.9604193430026291\n",
      "31.798501014709473\n",
      "Test Epoch:169 [(0%)]\t Loss: 0.1618  Pearson:0.9316 Spearman:0.9080\n",
      "Test : Loss:0.1517 \n",
      "pearson： 0.9402618819105926 ALL Pearson: 0.9417483832115829\n",
      "spearman： 0.9222295645586623 ALL Spearman: 0.925845862493534\n",
      "time: 36.58296751976013\n",
      "0.0026999999999999997\n",
      "Train Epoch:170 [(0%)]\t Loss: 0.0923  Pearson:0.9700 Spearman:0.9624\n",
      "Train ALL Pearson: 0.9713290605151735\n",
      "Train  ALL Spearman: 0.96187702649189\n",
      "32.12520098686218\n",
      "Test Epoch:170 [(0%)]\t Loss: 0.1459  Pearson:0.9493 Spearman:0.9273\n",
      "Test : Loss:0.1506 \n",
      "pearson： 0.9428219636312641 ALL Pearson: 0.9414297323318792\n",
      "spearman： 0.925535612027402 ALL Spearman: 0.9255725928456838\n",
      "time: 36.70773148536682\n",
      "0.0026999999999999997\n",
      "Train Epoch:171 [(0%)]\t Loss: 0.0865  Pearson:0.9760 Spearman:0.9698\n",
      "Train ALL Pearson: 0.9705851793523524\n",
      "Train  ALL Spearman: 0.9606344541639626\n",
      "31.67459464073181\n",
      "Test Epoch:171 [(0%)]\t Loss: 0.1542  Pearson:0.9334 Spearman:0.9260\n",
      "Test : Loss:0.1491 \n",
      "pearson： 0.9396160126760816 ALL Pearson: 0.9414487142239033\n",
      "spearman： 0.9264624333540475 ALL Spearman: 0.9254480528506115\n",
      "time: 36.61101245880127\n",
      "0.0026999999999999997\n",
      "Train Epoch:172 [(0%)]\t Loss: 0.0907  Pearson:0.9756 Spearman:0.9644\n",
      "Train ALL Pearson: 0.9711127974506598\n",
      "Train  ALL Spearman: 0.9617470500156853\n",
      "32.129135608673096\n",
      "Test Epoch:172 [(0%)]\t Loss: 0.1730  Pearson:0.9290 Spearman:0.9278\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9406872309504274 ALL Pearson: 0.9413975686606636\n",
      "spearman： 0.9268187906045567 ALL Spearman: 0.9255636801604484\n",
      "time: 36.762648820877075\n",
      "0.0026999999999999997\n",
      "Train Epoch:173 [(0%)]\t Loss: 0.1104  Pearson:0.9714 Spearman:0.9513\n",
      "Train ALL Pearson: 0.9710664194566008\n",
      "Train  ALL Spearman: 0.9611471901424764\n",
      "31.79118251800537\n",
      "Test Epoch:173 [(0%)]\t Loss: 0.1558  Pearson:0.9243 Spearman:0.9155\n",
      "Test : Loss:0.1495 \n",
      "pearson： 0.93662225125981 ALL Pearson: 0.9415411674899645\n",
      "spearman： 0.9207150963744651 ALL Spearman: 0.9256382035792634\n",
      "time: 36.486905097961426\n",
      "0.0026999999999999997\n",
      "Train Epoch:174 [(0%)]\t Loss: 0.0970  Pearson:0.9712 Spearman:0.9630\n",
      "Train ALL Pearson: 0.9712202853934941\n",
      "Train  ALL Spearman: 0.9610610870833272\n",
      "32.34453344345093\n",
      "Test Epoch:174 [(0%)]\t Loss: 0.1409  Pearson:0.9496 Spearman:0.9440\n",
      "Test : Loss:0.1507 \n",
      "pearson： 0.9431309961125458 ALL Pearson: 0.941480283125483\n",
      "spearman： 0.9304309679213735 ALL Spearman: 0.9255901344817085\n",
      "time: 36.99704146385193\n",
      "0.0026999999999999997\n",
      "Train Epoch:175 [(0%)]\t Loss: 0.1021  Pearson:0.9717 Spearman:0.9565\n",
      "Train ALL Pearson: 0.971292559268348\n",
      "Train  ALL Spearman: 0.962050656053234\n",
      "31.85367226600647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:175 [(0%)]\t Loss: 0.1514  Pearson:0.9427 Spearman:0.9284\n",
      "Test : Loss:0.1523 \n",
      "pearson： 0.9419725445818589 ALL Pearson: 0.9416150257860051\n",
      "spearman： 0.9268367478782651 ALL Spearman: 0.9257280083435012\n",
      "time: 36.74909973144531\n",
      "0.0026999999999999997\n",
      "Train Epoch:176 [(0%)]\t Loss: 0.0960  Pearson:0.9734 Spearman:0.9643\n",
      "Train ALL Pearson: 0.9717114124186403\n",
      "Train  ALL Spearman: 0.9617129132259378\n",
      "31.408470153808594\n",
      "Test Epoch:176 [(0%)]\t Loss: 0.1601  Pearson:0.9245 Spearman:0.9077\n",
      "Test : Loss:0.1511 \n",
      "pearson： 0.9376702642037237 ALL Pearson: 0.9415636614608576\n",
      "spearman： 0.9227156564625978 ALL Spearman: 0.9257108495567825\n",
      "time: 36.12895655632019\n",
      "0.0026999999999999997\n",
      "Train Epoch:177 [(0%)]\t Loss: 0.0848  Pearson:0.9783 Spearman:0.9689\n",
      "Train ALL Pearson: 0.971745353463752\n",
      "Train  ALL Spearman: 0.9624500238009974\n",
      "31.65467667579651\n",
      "Test Epoch:177 [(0%)]\t Loss: 0.1519  Pearson:0.9395 Spearman:0.9147\n",
      "Test : Loss:0.1535 \n",
      "pearson： 0.9432188158445793 ALL Pearson: 0.9415864378427199\n",
      "spearman： 0.9257578334085049 ALL Spearman: 0.9258212079029576\n",
      "time: 36.24083232879639\n",
      "0.0008099999999999998\n",
      "Train Epoch:178 [(0%)]\t Loss: 0.0941  Pearson:0.9746 Spearman:0.9637\n",
      "Train ALL Pearson: 0.9701136904890187\n",
      "Train  ALL Spearman: 0.9596331453540097\n",
      "31.741463661193848\n",
      "Test Epoch:178 [(0%)]\t Loss: 0.1605  Pearson:0.9334 Spearman:0.9287\n",
      "Test : Loss:0.1495 \n",
      "pearson： 0.9386711851514591 ALL Pearson: 0.942072743252742\n",
      "spearman： 0.9252354573845412 ALL Spearman: 0.9263128562419439\n",
      "time: 36.723228454589844\n",
      "0.0008099999999999998\n",
      "Train Epoch:179 [(0%)]\t Loss: 0.0992  Pearson:0.9687 Spearman:0.9603\n",
      "Train ALL Pearson: 0.9696199733495644\n",
      "Train  ALL Spearman: 0.9591118039683165\n",
      "31.721938610076904\n",
      "Test Epoch:179 [(0%)]\t Loss: 0.1512  Pearson:0.9521 Spearman:0.9353\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9475666811737704 ALL Pearson: 0.9421038447558557\n",
      "spearman： 0.930171382439859 ALL Spearman: 0.9262865713033956\n",
      "time: 36.37444591522217\n",
      "0.0008099999999999998\n",
      "Train Epoch:180 [(0%)]\t Loss: 0.1023  Pearson:0.9746 Spearman:0.9555\n",
      "Train ALL Pearson: 0.9696600429944964\n",
      "Train  ALL Spearman: 0.9597288409113985\n",
      "31.647446155548096\n",
      "Test Epoch:180 [(0%)]\t Loss: 0.1457  Pearson:0.9466 Spearman:0.8971\n",
      "Test : Loss:0.1513 \n",
      "pearson： 0.9423412549297132 ALL Pearson: 0.9420033116828345\n",
      "spearman： 0.921075598148553 ALL Spearman: 0.9262934129905231\n",
      "time: 36.35893630981445\n",
      "0.0008099999999999998\n",
      "Train Epoch:181 [(0%)]\t Loss: 0.1055  Pearson:0.9640 Spearman:0.9463\n",
      "Train ALL Pearson: 0.9706359395225037\n",
      "Train  ALL Spearman: 0.9603709197393908\n",
      "31.79177474975586\n",
      "Test Epoch:181 [(0%)]\t Loss: 0.1482  Pearson:0.9451 Spearman:0.9380\n",
      "Test : Loss:0.1516 \n",
      "pearson： 0.9437250314494211 ALL Pearson: 0.9420011348998029\n",
      "spearman： 0.9280403738198284 ALL Spearman: 0.9262141427716878\n",
      "time: 36.522257566452026\n",
      "0.0008099999999999998\n",
      "Train Epoch:182 [(0%)]\t Loss: 0.0998  Pearson:0.9735 Spearman:0.9644\n",
      "Train ALL Pearson: 0.9704934665439495\n",
      "Train  ALL Spearman: 0.9608736196182379\n",
      "31.258636474609375\n",
      "Test Epoch:182 [(0%)]\t Loss: 0.1612  Pearson:0.9320 Spearman:0.8981\n",
      "Test : Loss:0.1527 \n",
      "pearson： 0.941518303705217 ALL Pearson: 0.9420328395818445\n",
      "spearman： 0.9253954609998122 ALL Spearman: 0.9262910771895358\n",
      "time: 35.92168855667114\n",
      "0.0008099999999999998\n",
      "Train Epoch:183 [(0%)]\t Loss: 0.0923  Pearson:0.9743 Spearman:0.9649\n",
      "Train ALL Pearson: 0.9703730848539601\n",
      "Train  ALL Spearman: 0.9600388368804077\n",
      "31.813279390335083\n",
      "Test Epoch:183 [(0%)]\t Loss: 0.1478  Pearson:0.9229 Spearman:0.8990\n",
      "Test : Loss:0.1504 \n",
      "pearson： 0.9398180121633172 ALL Pearson: 0.9420204673375702\n",
      "spearman： 0.9211342732464721 ALL Spearman: 0.9262720152933783\n",
      "time: 36.809807777404785\n",
      "0.0008099999999999998\n",
      "Train Epoch:184 [(0%)]\t Loss: 0.1107  Pearson:0.9647 Spearman:0.9599\n",
      "Train ALL Pearson: 0.9706674548517736\n",
      "Train  ALL Spearman: 0.9607167530794352\n",
      "31.303631067276\n",
      "Test Epoch:184 [(0%)]\t Loss: 0.1444  Pearson:0.9432 Spearman:0.9170\n",
      "Test : Loss:0.1511 \n",
      "pearson： 0.9394948972482534 ALL Pearson: 0.9420179345009545\n",
      "spearman： 0.9206615178784393 ALL Spearman: 0.9262530127735945\n",
      "time: 36.06710410118103\n",
      "0.0008099999999999998\n",
      "Train Epoch:185 [(0%)]\t Loss: 0.1046  Pearson:0.9637 Spearman:0.9529\n",
      "Train ALL Pearson: 0.9698754907327853\n",
      "Train  ALL Spearman: 0.9595139209638754\n",
      "31.98479390144348\n",
      "Test Epoch:185 [(0%)]\t Loss: 0.1524  Pearson:0.9359 Spearman:0.9173\n",
      "Test : Loss:0.1513 \n",
      "pearson： 0.9412045092419267 ALL Pearson: 0.9420221826794584\n",
      "spearman： 0.9254156563234023 ALL Spearman: 0.9262402667343389\n",
      "time: 36.822242975234985\n",
      "0.0008099999999999998\n",
      "Train Epoch:186 [(0%)]\t Loss: 0.0872  Pearson:0.9779 Spearman:0.9649\n",
      "Train ALL Pearson: 0.9701979036692526\n",
      "Train  ALL Spearman: 0.96015876410764\n",
      "31.192856788635254\n",
      "Test Epoch:186 [(0%)]\t Loss: 0.1520  Pearson:0.9558 Spearman:0.9367\n",
      "Test : Loss:0.1516 \n",
      "pearson： 0.9477257321279082 ALL Pearson: 0.9419927534444753\n",
      "spearman： 0.929395232626742 ALL Spearman: 0.9262387882222709\n",
      "time: 36.115278482437134\n",
      "0.0008099999999999998\n",
      "Train Epoch:187 [(0%)]\t Loss: 0.1014  Pearson:0.9701 Spearman:0.9584\n",
      "Train ALL Pearson: 0.9707494564067531\n",
      "Train  ALL Spearman: 0.9604317950296364\n",
      "32.02752614021301\n",
      "Test Epoch:187 [(0%)]\t Loss: 0.1429  Pearson:0.9424 Spearman:0.9276\n",
      "Test : Loss:0.1512 \n",
      "pearson： 0.9400923694884881 ALL Pearson: 0.9420394757566719\n",
      "spearman： 0.9261965434121356 ALL Spearman: 0.9262248489810717\n",
      "time: 36.772005796432495\n",
      "0.0008099999999999998\n",
      "Train Epoch:188 [(0%)]\t Loss: 0.1026  Pearson:0.9667 Spearman:0.9599\n",
      "Train ALL Pearson: 0.9711286914883724\n",
      "Train  ALL Spearman: 0.9610818064659602\n",
      "31.590450048446655\n",
      "Test Epoch:188 [(0%)]\t Loss: 0.1542  Pearson:0.9168 Spearman:0.9125\n",
      "Test : Loss:0.1510 \n",
      "pearson： 0.9394588191851052 ALL Pearson: 0.9420426155702869\n",
      "spearman： 0.9252643228679599 ALL Spearman: 0.9262791195019071\n",
      "time: 36.283488035202026\n",
      "0.0008099999999999998\n",
      "Train Epoch:189 [(0%)]\t Loss: 0.0875  Pearson:0.9724 Spearman:0.9613\n",
      "Train ALL Pearson: 0.970516937839833\n",
      "Train  ALL Spearman: 0.9608504606979699\n",
      "31.58278775215149\n",
      "Test Epoch:189 [(0%)]\t Loss: 0.1576  Pearson:0.9371 Spearman:0.9066\n",
      "Test : Loss:0.1501 \n",
      "pearson： 0.9419239387348095 ALL Pearson: 0.9418842398912152\n",
      "spearman： 0.9232828982175182 ALL Spearman: 0.9261694079929284\n",
      "time: 36.41823720932007\n",
      "0.0008099999999999998\n",
      "Train Epoch:190 [(0%)]\t Loss: 0.1067  Pearson:0.9585 Spearman:0.9421\n",
      "Train ALL Pearson: 0.9699719041340465\n",
      "Train  ALL Spearman: 0.9593860927103123\n",
      "31.726633310317993\n",
      "Test Epoch:190 [(0%)]\t Loss: 0.1507  Pearson:0.9437 Spearman:0.9343\n",
      "Test : Loss:0.1498 \n",
      "pearson： 0.9437172781483203 ALL Pearson: 0.9419082856167604\n",
      "spearman： 0.9285627335231983 ALL Spearman: 0.9262226224530348\n",
      "time: 36.42612552642822\n",
      "0.0008099999999999998\n",
      "Train Epoch:191 [(0%)]\t Loss: 0.1051  Pearson:0.9670 Spearman:0.9424\n",
      "Train ALL Pearson: 0.9705921003785126\n",
      "Train  ALL Spearman: 0.9607079493026704\n",
      "31.442299842834473\n",
      "Test Epoch:191 [(0%)]\t Loss: 0.1434  Pearson:0.9529 Spearman:0.9387\n",
      "Test : Loss:0.1506 \n",
      "pearson： 0.9449916111348527 ALL Pearson: 0.9419413748776477\n",
      "spearman： 0.9303914656575037 ALL Spearman: 0.9262558193746168\n",
      "time: 36.10380482673645\n",
      "0.0008099999999999998\n",
      "Train Epoch:192 [(0%)]\t Loss: 0.0856  Pearson:0.9749 Spearman:0.9591\n",
      "Train ALL Pearson: 0.9703750538264158\n",
      "Train  ALL Spearman: 0.9603918098090133\n",
      "31.85269045829773\n",
      "Test Epoch:192 [(0%)]\t Loss: 0.1560  Pearson:0.9253 Spearman:0.9094\n",
      "Test : Loss:0.1502 \n",
      "pearson： 0.9396205465501837 ALL Pearson: 0.9419029136025636\n",
      "spearman： 0.9230919383842074 ALL Spearman: 0.9262406238882104\n",
      "time: 36.655149936676025\n",
      "0.0008099999999999998\n",
      "Train Epoch:193 [(0%)]\t Loss: 0.1054  Pearson:0.9712 Spearman:0.9542\n",
      "Train ALL Pearson: 0.9706582895142133\n",
      "Train  ALL Spearman: 0.960558453596824\n",
      "31.545233964920044\n",
      "Test Epoch:193 [(0%)]\t Loss: 0.1462  Pearson:0.9512 Spearman:0.9349\n",
      "Test : Loss:0.1511 \n",
      "pearson： 0.9444064287144435 ALL Pearson: 0.9419080108853727\n",
      "spearman： 0.9278004585041771 ALL Spearman: 0.9262210885711096\n",
      "time: 36.09377479553223\n",
      "0.0008099999999999998\n",
      "Train Epoch:194 [(0%)]\t Loss: 0.1077  Pearson:0.9640 Spearman:0.9485\n",
      "Train ALL Pearson: 0.9711120749013415\n",
      "Train  ALL Spearman: 0.9615513605373136\n",
      "31.80635905265808\n",
      "Test Epoch:194 [(0%)]\t Loss: 0.1433  Pearson:0.9396 Spearman:0.9108\n",
      "Test : Loss:0.1507 \n",
      "pearson： 0.941911122203366 ALL Pearson: 0.941944486949534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman： 0.9257903859671678 ALL Spearman: 0.9262290834912446\n",
      "time: 36.35747718811035\n",
      "0.0008099999999999998\n",
      "Train Epoch:195 [(0%)]\t Loss: 0.0978  Pearson:0.9773 Spearman:0.9645\n",
      "Train ALL Pearson: 0.9707606119866732\n",
      "Train  ALL Spearman: 0.9611786272955424\n",
      "31.930830478668213\n",
      "Test Epoch:195 [(0%)]\t Loss: 0.1527  Pearson:0.9406 Spearman:0.9353\n",
      "Test : Loss:0.1516 \n",
      "pearson： 0.9431784982935736 ALL Pearson: 0.9419145286421504\n",
      "spearman： 0.9276503849797568 ALL Spearman: 0.9262575523950204\n",
      "time: 36.81026482582092\n",
      "0.0008099999999999998\n",
      "Train Epoch:196 [(0%)]\t Loss: 0.0903  Pearson:0.9801 Spearman:0.9648\n",
      "Train ALL Pearson: 0.9703066772484547\n",
      "Train  ALL Spearman: 0.9608555988254567\n",
      "31.695650339126587\n",
      "Test Epoch:196 [(0%)]\t Loss: 0.1627  Pearson:0.9274 Spearman:0.9087\n",
      "Test : Loss:0.1511 \n",
      "pearson： 0.9389865226198041 ALL Pearson: 0.9419141622799407\n",
      "spearman： 0.9247278646992434 ALL Spearman: 0.9262636389909574\n",
      "time: 36.35515642166138\n",
      "0.0008099999999999998\n",
      "Train Epoch:197 [(0%)]\t Loss: 0.0965  Pearson:0.9742 Spearman:0.9656\n",
      "Train ALL Pearson: 0.9703782955658773\n",
      "Train  ALL Spearman: 0.9603921643561351\n",
      "31.42469048500061\n",
      "Test Epoch:197 [(0%)]\t Loss: 0.1579  Pearson:0.9248 Spearman:0.9124\n",
      "Test : Loss:0.1517 \n",
      "pearson： 0.9399556753302064 ALL Pearson: 0.9418484800256101\n",
      "spearman： 0.9250179703887366 ALL Spearman: 0.9262209624305225\n",
      "time: 36.00521945953369\n",
      "0.0008099999999999998\n",
      "Train Epoch:198 [(0%)]\t Loss: 0.1009  Pearson:0.9687 Spearman:0.9599\n",
      "Train ALL Pearson: 0.9704317272529583\n",
      "Train  ALL Spearman: 0.9606526358015661\n",
      "31.887996435165405\n",
      "Test Epoch:198 [(0%)]\t Loss: 0.1409  Pearson:0.9373 Spearman:0.9202\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.9437513421780359 ALL Pearson: 0.9418196154638931\n",
      "spearman： 0.927804231087358 ALL Spearman: 0.926203757101513\n",
      "time: 36.630393981933594\n",
      "0.0008099999999999998\n",
      "Train Epoch:199 [(0%)]\t Loss: 0.1016  Pearson:0.9716 Spearman:0.9591\n",
      "Train ALL Pearson: 0.970079596244097\n",
      "Train  ALL Spearman: 0.9599535208648652\n",
      "32.23949337005615\n",
      "Test Epoch:199 [(0%)]\t Loss: 0.1481  Pearson:0.9401 Spearman:0.9276\n",
      "Test : Loss:0.1513 \n",
      "pearson： 0.9401245009573688 ALL Pearson: 0.9418592369949089\n",
      "spearman： 0.9299618254746591 ALL Spearman: 0.9261988251241715\n",
      "time: 37.03095722198486\n",
      "0.0008099999999999998\n",
      "Train Epoch:200 [(0%)]\t Loss: 0.1060  Pearson:0.9689 Spearman:0.9586\n",
      "Train ALL Pearson: 0.9703070822114264\n",
      "Train  ALL Spearman: 0.9601911041549691\n",
      "32.208083391189575\n",
      "Test Epoch:200 [(0%)]\t Loss: 0.1507  Pearson:0.9449 Spearman:0.9371\n",
      "Test : Loss:0.1504 \n",
      "pearson： 0.9431028580777884 ALL Pearson: 0.9418393632618531\n",
      "spearman： 0.9301105748381405 ALL Spearman: 0.9261502459639344\n",
      "time: 36.99319338798523\n",
      "0.00024299999999999994\n",
      "Train Epoch:201 [(0%)]\t Loss: 0.0965  Pearson:0.9757 Spearman:0.9657\n",
      "Train ALL Pearson: 0.9694041228574461\n",
      "Train  ALL Spearman: 0.9585957835875188\n",
      "31.614309787750244\n",
      "Test Epoch:201 [(0%)]\t Loss: 0.1484  Pearson:0.9378 Spearman:0.9238\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9404895667015167 ALL Pearson: 0.9420983484263548\n",
      "spearman： 0.924519710799252 ALL Spearman: 0.926306420871877\n",
      "time: 36.5491418838501\n",
      "0.00024299999999999994\n",
      "Train Epoch:202 [(0%)]\t Loss: 0.0940  Pearson:0.9796 Spearman:0.9758\n",
      "Train ALL Pearson: 0.970146515521945\n",
      "Train  ALL Spearman: 0.9596991810756292\n",
      "31.608519792556763\n",
      "Test Epoch:202 [(0%)]\t Loss: 0.1574  Pearson:0.9433 Spearman:0.9183\n",
      "Test : Loss:0.1502 \n",
      "pearson： 0.9425788615556306 ALL Pearson: 0.9420810455732659\n",
      "spearman： 0.9264668948387126 ALL Spearman: 0.9263055103321399\n",
      "time: 36.25402903556824\n",
      "0.00024299999999999994\n",
      "Train Epoch:203 [(0%)]\t Loss: 0.1055  Pearson:0.9638 Spearman:0.9421\n",
      "Train ALL Pearson: 0.9696938794467314\n",
      "Train  ALL Spearman: 0.9591790391124999\n",
      "31.83031964302063\n",
      "Test Epoch:203 [(0%)]\t Loss: 0.1473  Pearson:0.9381 Spearman:0.9270\n",
      "Test : Loss:0.1511 \n",
      "pearson： 0.9387315736480318 ALL Pearson: 0.9420855338217874\n",
      "spearman： 0.9261162835746215 ALL Spearman: 0.9263290982822656\n",
      "time: 36.633779764175415\n",
      "0.00024299999999999994\n",
      "Train Epoch:204 [(0%)]\t Loss: 0.1060  Pearson:0.9683 Spearman:0.9474\n",
      "Train ALL Pearson: 0.9693126445568894\n",
      "Train  ALL Spearman: 0.9591369595441049\n",
      "31.94030785560608\n",
      "Test Epoch:204 [(0%)]\t Loss: 0.1551  Pearson:0.9312 Spearman:0.9097\n",
      "Test : Loss:0.1508 \n",
      "pearson： 0.9403042630785319 ALL Pearson: 0.9420461138229096\n",
      "spearman： 0.9256659347187033 ALL Spearman: 0.9262597737894287\n",
      "time: 36.783326864242554\n",
      "0.00024299999999999994\n",
      "Train Epoch:205 [(0%)]\t Loss: 0.0981  Pearson:0.9706 Spearman:0.9622\n",
      "Train ALL Pearson: 0.9706486688077011\n",
      "Train  ALL Spearman: 0.9603500581890424\n",
      "31.957901000976562\n",
      "Test Epoch:205 [(0%)]\t Loss: 0.1411  Pearson:0.9542 Spearman:0.9270\n",
      "Test : Loss:0.1502 \n",
      "pearson： 0.941545757821886 ALL Pearson: 0.9420245664717445\n",
      "spearman： 0.9231046793546301 ALL Spearman: 0.9262642884490447\n",
      "time: 36.737752199172974\n",
      "0.00024299999999999994\n",
      "Train Epoch:206 [(0%)]\t Loss: 0.0906  Pearson:0.9715 Spearman:0.9600\n",
      "Train ALL Pearson: 0.9695923264987769\n",
      "Train  ALL Spearman: 0.959123043269988\n",
      "32.433185338974\n",
      "Test Epoch:206 [(0%)]\t Loss: 0.1505  Pearson:0.9300 Spearman:0.9130\n",
      "Test : Loss:0.1505 \n",
      "pearson： 0.9420478102596447 ALL Pearson: 0.9420562983970907\n",
      "spearman： 0.9229827626695386 ALL Spearman: 0.9262538811172362\n",
      "time: 37.14167523384094\n",
      "0.00024299999999999994\n",
      "Train Epoch:207 [(0%)]\t Loss: 0.1034  Pearson:0.9693 Spearman:0.9512\n",
      "Train ALL Pearson: 0.9704002115668101\n",
      "Train  ALL Spearman: 0.9597913537528187\n",
      "31.995290517807007\n",
      "Test Epoch:207 [(0%)]\t Loss: 0.1342  Pearson:0.9443 Spearman:0.9213\n",
      "Test : Loss:0.1504 \n",
      "pearson： 0.941359331749405 ALL Pearson: 0.9420139034863071\n",
      "spearman： 0.9254618401488082 ALL Spearman: 0.9262170150368588\n",
      "time: 36.600812911987305\n",
      "0.00024299999999999994\n",
      "Train Epoch:208 [(0%)]\t Loss: 0.0864  Pearson:0.9811 Spearman:0.9604\n",
      "Train ALL Pearson: 0.9706057483469146\n",
      "Train  ALL Spearman: 0.9611292212181897\n",
      "31.625813722610474\n",
      "Test Epoch:208 [(0%)]\t Loss: 0.1481  Pearson:0.9494 Spearman:0.9296\n",
      "Test : Loss:0.1503 \n",
      "pearson： 0.9455483560017416 ALL Pearson: 0.9419984110767846\n",
      "spearman： 0.9279814156868403 ALL Spearman: 0.9262254496156114\n",
      "time: 36.560235023498535\n",
      "0.00024299999999999994\n",
      "Train Epoch:209 [(0%)]\t Loss: 0.1126  Pearson:0.9653 Spearman:0.9557\n",
      "Train ALL Pearson: 0.9698961743197597\n",
      "Train  ALL Spearman: 0.9596813597948013\n",
      "31.739379167556763\n",
      "Test Epoch:209 [(0%)]\t Loss: 0.1503  Pearson:0.9351 Spearman:0.9152\n",
      "Test : Loss:0.1507 \n",
      "pearson： 0.9403011912305181 ALL Pearson: 0.9420090251243565\n",
      "spearman： 0.9265900609885734 ALL Spearman: 0.9262393976843641\n",
      "time: 36.428876638412476\n",
      "0.00024299999999999994\n",
      "Train Epoch:210 [(0%)]\t Loss: 0.1093  Pearson:0.9639 Spearman:0.9537\n",
      "Train ALL Pearson: 0.9698644564014198\n",
      "Train  ALL Spearman: 0.9592867689100518\n",
      "31.896209478378296\n",
      "Test Epoch:210 [(0%)]\t Loss: 0.1484  Pearson:0.9296 Spearman:0.9040\n",
      "Test : Loss:0.1502 \n",
      "pearson： 0.9409609590198407 ALL Pearson: 0.9420159323919637\n",
      "spearman： 0.9236474301796029 ALL Spearman: 0.9262385139668792\n",
      "time: 36.64868426322937\n",
      "0.00024299999999999994\n",
      "Train Epoch:211 [(0%)]\t Loss: 0.1068  Pearson:0.9713 Spearman:0.9580\n",
      "Train ALL Pearson: 0.9699814295664103\n",
      "Train  ALL Spearman: 0.9602193886038554\n",
      "31.865584135055542\n",
      "Test Epoch:211 [(0%)]\t Loss: 0.1521  Pearson:0.9324 Spearman:0.9291\n",
      "Test : Loss:0.1503 \n",
      "pearson： 0.9404774341614782 ALL Pearson: 0.9420293151192032\n",
      "spearman： 0.927304997748055 ALL Spearman: 0.9262596117134417\n",
      "time: 36.537705421447754\n",
      "0.00024299999999999994\n",
      "Train Epoch:212 [(0%)]\t Loss: 0.1108  Pearson:0.9632 Spearman:0.9399\n",
      "Train ALL Pearson: 0.9697426169107296\n",
      "Train  ALL Spearman: 0.9591243925792707\n",
      "32.25221085548401\n",
      "Test Epoch:212 [(0%)]\t Loss: 0.1488  Pearson:0.9535 Spearman:0.9418\n",
      "Test : Loss:0.1506 \n",
      "pearson： 0.9463825252404274 ALL Pearson: 0.9420041737162201\n",
      "spearman： 0.9324133066806181 ALL Spearman: 0.9261978314274228\n",
      "time: 37.232614278793335\n",
      "0.00024299999999999994\n",
      "Train Epoch:213 [(0%)]\t Loss: 0.1016  Pearson:0.9686 Spearman:0.9513\n",
      "Train ALL Pearson: 0.9701632420333461\n",
      "Train  ALL Spearman: 0.9601571564421955\n",
      "31.6410653591156\n",
      "Test Epoch:213 [(0%)]\t Loss: 0.1373  Pearson:0.9558 Spearman:0.9517\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.9466947875887491 ALL Pearson: 0.942002434457773\n",
      "spearman： 0.9328090594569404 ALL Spearman: 0.9262036162934157\n",
      "time: 36.19760251045227\n",
      "0.00024299999999999994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:214 [(0%)]\t Loss: 0.1037  Pearson:0.9583 Spearman:0.9473\n",
      "Train ALL Pearson: 0.9704775931008368\n",
      "Train  ALL Spearman: 0.9605439013612304\n",
      "31.723787307739258\n",
      "Test Epoch:214 [(0%)]\t Loss: 0.1584  Pearson:0.9302 Spearman:0.9041\n",
      "Test : Loss:0.1510 \n",
      "pearson： 0.9389967989795647 ALL Pearson: 0.9419716622147571\n",
      "spearman： 0.9189791207033516 ALL Spearman: 0.9261810048598083\n",
      "time: 36.392290353775024\n",
      "0.00024299999999999994\n",
      "Train Epoch:215 [(0%)]\t Loss: 0.0916  Pearson:0.9750 Spearman:0.9619\n",
      "Train ALL Pearson: 0.9701346227259087\n",
      "Train  ALL Spearman: 0.9599219318253427\n",
      "31.666250705718994\n",
      "Test Epoch:215 [(0%)]\t Loss: 0.1600  Pearson:0.9411 Spearman:0.9216\n",
      "Test : Loss:0.1521 \n",
      "pearson： 0.9421741552618534 ALL Pearson: 0.9419693723308216\n",
      "spearman： 0.9275169159419021 ALL Spearman: 0.9261596518985692\n",
      "time: 36.426724910736084\n",
      "0.00024299999999999994\n",
      "Train Epoch:216 [(0%)]\t Loss: 0.1011  Pearson:0.9747 Spearman:0.9607\n",
      "Train ALL Pearson: 0.9704257114713308\n",
      "Train  ALL Spearman: 0.9604196397477045\n",
      "32.38386130332947\n",
      "Test Epoch:216 [(0%)]\t Loss: 0.1572  Pearson:0.9398 Spearman:0.9243\n",
      "Test : Loss:0.1503 \n",
      "pearson： 0.9417142775521262 ALL Pearson: 0.9419828885945728\n",
      "spearman： 0.9263718504079996 ALL Spearman: 0.926173773777317\n",
      "time: 37.051865100860596\n",
      "0.00024299999999999994\n",
      "Train Epoch:217 [(0%)]\t Loss: 0.0899  Pearson:0.9780 Spearman:0.9649\n",
      "Train ALL Pearson: 0.9700875395076366\n",
      "Train  ALL Spearman: 0.9599723768253712\n",
      "32.248228311538696\n",
      "Test Epoch:217 [(0%)]\t Loss: 0.1476  Pearson:0.9524 Spearman:0.9495\n",
      "Test : Loss:0.1514 \n",
      "pearson： 0.9429042829471392 ALL Pearson: 0.9419709102159223\n",
      "spearman： 0.9286472549708612 ALL Spearman: 0.9261790753488514\n",
      "time: 37.08327293395996\n",
      "0.00024299999999999994\n",
      "Train Epoch:218 [(0%)]\t Loss: 0.1040  Pearson:0.9695 Spearman:0.9510\n",
      "Train ALL Pearson: 0.970357244125848\n",
      "Train  ALL Spearman: 0.9598929363497056\n",
      "31.791752576828003\n",
      "Test Epoch:218 [(0%)]\t Loss: 0.1524  Pearson:0.9463 Spearman:0.9437\n",
      "Test : Loss:0.1495 \n",
      "pearson： 0.9403222191236268 ALL Pearson: 0.941987214814946\n",
      "spearman： 0.927008728301389 ALL Spearman: 0.9262245717651304\n",
      "time: 36.556766748428345\n",
      "0.00024299999999999994\n",
      "Train Epoch:219 [(0%)]\t Loss: 0.0935  Pearson:0.9750 Spearman:0.9644\n",
      "Train ALL Pearson: 0.9700178786884098\n",
      "Train  ALL Spearman: 0.9596644022484494\n",
      "32.6490433216095\n",
      "Test Epoch:219 [(0%)]\t Loss: 0.1455  Pearson:0.9526 Spearman:0.9457\n",
      "Test : Loss:0.1501 \n",
      "pearson： 0.9440723779402537 ALL Pearson: 0.9419859441697452\n",
      "spearman： 0.930909813876822 ALL Spearman: 0.9262149322774766\n",
      "time: 37.37699007987976\n",
      "0.00024299999999999994\n",
      "Train Epoch:220 [(0%)]\t Loss: 0.1075  Pearson:0.9679 Spearman:0.9505\n",
      "Train ALL Pearson: 0.9697432518578993\n",
      "Train  ALL Spearman: 0.9586658110287056\n",
      "31.686856508255005\n",
      "Test Epoch:220 [(0%)]\t Loss: 0.1466  Pearson:0.9493 Spearman:0.9280\n",
      "Test : Loss:0.1502 \n",
      "pearson： 0.9417229851272286 ALL Pearson: 0.9420017785013847\n",
      "spearman： 0.9234188988570476 ALL Spearman: 0.9262263656016185\n",
      "time: 36.556294441223145\n",
      "0.00024299999999999994\n",
      "Train Epoch:221 [(0%)]\t Loss: 0.0913  Pearson:0.9786 Spearman:0.9693\n",
      "Train ALL Pearson: 0.9697267091418219\n",
      "Train  ALL Spearman: 0.9591634214654001\n",
      "32.011783838272095\n",
      "Test Epoch:221 [(0%)]\t Loss: 0.1400  Pearson:0.9474 Spearman:0.9126\n",
      "Test : Loss:0.1503 \n",
      "pearson： 0.9403085059825437 ALL Pearson: 0.9420095155887303\n",
      "spearman： 0.9229379181083072 ALL Spearman: 0.926216329357816\n",
      "time: 36.55832552909851\n",
      "7.289999999999998e-05\n",
      "Train Epoch:222 [(0%)]\t Loss: 0.0961  Pearson:0.9714 Spearman:0.9696\n",
      "Train ALL Pearson: 0.9700798709996059\n",
      "Train  ALL Spearman: 0.9600065891693388\n",
      "31.51340913772583\n",
      "Test Epoch:222 [(0%)]\t Loss: 0.1465  Pearson:0.9453 Spearman:0.9200\n",
      "Test : Loss:0.1516 \n",
      "pearson： 0.9425605791519999 ALL Pearson: 0.9420994716756248\n",
      "spearman： 0.9239097107039966 ALL Spearman: 0.9263095047158805\n",
      "time: 36.3308641910553\n",
      "7.289999999999998e-05\n",
      "Train Epoch:223 [(0%)]\t Loss: 0.0919  Pearson:0.9769 Spearman:0.9562\n",
      "Train ALL Pearson: 0.9701608714596737\n",
      "Train  ALL Spearman: 0.960034771500855\n",
      "32.0438015460968\n",
      "Test Epoch:223 [(0%)]\t Loss: 0.1390  Pearson:0.9555 Spearman:0.9327\n",
      "Test : Loss:0.1499 \n",
      "pearson： 0.945427896766222 ALL Pearson: 0.9420903350179786\n",
      "spearman： 0.9277984150848364 ALL Spearman: 0.9263150907830725\n",
      "time: 36.6768901348114\n",
      "7.289999999999998e-05\n",
      "Train Epoch:224 [(0%)]\t Loss: 0.0968  Pearson:0.9697 Spearman:0.9576\n",
      "Train ALL Pearson: 0.9701734378648995\n",
      "Train  ALL Spearman: 0.9601882461906129\n",
      "32.12834024429321\n",
      "Test Epoch:224 [(0%)]\t Loss: 0.1489  Pearson:0.9445 Spearman:0.9301\n",
      "Test : Loss:0.1504 \n",
      "pearson： 0.9416035519432072 ALL Pearson: 0.9420794345680491\n",
      "spearman： 0.9280640000243445 ALL Spearman: 0.9263113124865033\n",
      "time: 36.88501787185669\n",
      "7.289999999999998e-05\n",
      "Train Epoch:225 [(0%)]\t Loss: 0.1005  Pearson:0.9706 Spearman:0.9589\n",
      "Train ALL Pearson: 0.9705221886784245\n",
      "Train  ALL Spearman: 0.9602272002454378\n",
      "31.85060477256775\n",
      "Test Epoch:225 [(0%)]\t Loss: 0.1470  Pearson:0.9392 Spearman:0.9011\n",
      "Test : Loss:0.1506 \n",
      "pearson： 0.9417176012906427 ALL Pearson: 0.9420780437449893\n",
      "spearman： 0.918837557815895 ALL Spearman: 0.9263172036919447\n",
      "time: 36.538100719451904\n",
      "7.289999999999998e-05\n",
      "Train Epoch:226 [(0%)]\t Loss: 0.1050  Pearson:0.9708 Spearman:0.9613\n",
      "Train ALL Pearson: 0.9704750690253785\n",
      "Train  ALL Spearman: 0.9602043773074983\n",
      "32.01536226272583\n",
      "Test Epoch:226 [(0%)]\t Loss: 0.1706  Pearson:0.9353 Spearman:0.9136\n",
      "Test : Loss:0.1505 \n",
      "pearson： 0.939863803119657 ALL Pearson: 0.9420829833276991\n",
      "spearman： 0.9246065616470154 ALL Spearman: 0.9263307828728178\n",
      "time: 36.72485160827637\n",
      "7.289999999999998e-05\n",
      "Train Epoch:227 [(0%)]\t Loss: 0.1067  Pearson:0.9647 Spearman:0.9473\n",
      "Train ALL Pearson: 0.9701563876989404\n",
      "Train  ALL Spearman: 0.9603157118358387\n",
      "32.00455284118652\n",
      "Test Epoch:227 [(0%)]\t Loss: 0.1443  Pearson:0.9477 Spearman:0.9154\n",
      "Test : Loss:0.1507 \n",
      "pearson： 0.9444450395914582 ALL Pearson: 0.9420810983792397\n",
      "spearman： 0.9226037454339373 ALL Spearman: 0.9263166991295965\n",
      "time: 36.73203682899475\n",
      "7.289999999999998e-05\n",
      "Train Epoch:228 [(0%)]\t Loss: 0.1097  Pearson:0.9542 Spearman:0.9475\n",
      "Train ALL Pearson: 0.969802446445246\n",
      "Train  ALL Spearman: 0.959617798139521\n",
      "31.466075897216797\n",
      "Test Epoch:228 [(0%)]\t Loss: 0.1443  Pearson:0.9423 Spearman:0.9232\n",
      "Test : Loss:0.1503 \n",
      "pearson： 0.9422784068840138 ALL Pearson: 0.9420739308891873\n",
      "spearman： 0.9292278567623423 ALL Spearman: 0.9263044282906265\n",
      "time: 36.39549446105957\n",
      "7.289999999999998e-05\n",
      "Train Epoch:229 [(0%)]\t Loss: 0.0896  Pearson:0.9714 Spearman:0.9690\n",
      "Train ALL Pearson: 0.9705471820096269\n",
      "Train  ALL Spearman: 0.960345221497291\n",
      "32.797743797302246\n",
      "Test Epoch:229 [(0%)]\t Loss: 0.1606  Pearson:0.9356 Spearman:0.9156\n",
      "Test : Loss:0.1519 \n",
      "pearson： 0.9395414241210315 ALL Pearson: 0.9420629604655751\n",
      "spearman： 0.9238091923057884 ALL Spearman: 0.9262951320227097\n",
      "time: 37.55322551727295\n",
      "7.289999999999998e-05\n",
      "Train Epoch:230 [(0%)]\t Loss: 0.0987  Pearson:0.9668 Spearman:0.9501\n",
      "Train ALL Pearson: 0.9695808339551896\n",
      "Train  ALL Spearman: 0.9581829135600718\n",
      "31.778932571411133\n",
      "Test Epoch:230 [(0%)]\t Loss: 0.1530  Pearson:0.9372 Spearman:0.9056\n",
      "Test : Loss:0.1505 \n",
      "pearson： 0.9428807567849319 ALL Pearson: 0.9420608455688663\n",
      "spearman： 0.9254093757376619 ALL Spearman: 0.9262995909457875\n",
      "time: 36.57839345932007\n",
      "7.289999999999998e-05\n",
      "Train Epoch:231 [(0%)]\t Loss: 0.0851  Pearson:0.9773 Spearman:0.9604\n",
      "Train ALL Pearson: 0.9706311383255448\n",
      "Train  ALL Spearman: 0.9602199739350921\n",
      "31.53945231437683\n",
      "Test Epoch:231 [(0%)]\t Loss: 0.1377  Pearson:0.9457 Spearman:0.9172\n",
      "Test : Loss:0.1523 \n",
      "pearson： 0.9447994319663411 ALL Pearson: 0.9420488785105342\n",
      "spearman： 0.9246922505365099 ALL Spearman: 0.9262965496375632\n",
      "time: 36.36490511894226\n",
      "7.289999999999998e-05\n",
      "Train Epoch:232 [(0%)]\t Loss: 0.1016  Pearson:0.9594 Spearman:0.9482\n",
      "Train ALL Pearson: 0.9701500994789034\n",
      "Train  ALL Spearman: 0.959936793201276\n",
      "32.04228687286377\n",
      "Test Epoch:232 [(0%)]\t Loss: 0.1402  Pearson:0.9367 Spearman:0.9172\n",
      "Test : Loss:0.1514 \n",
      "pearson： 0.9411033541187094 ALL Pearson: 0.9420665369918585\n",
      "spearman： 0.9223051490083365 ALL Spearman: 0.9263011149000895\n",
      "time: 36.833837032318115\n",
      "7.289999999999998e-05\n",
      "Train Epoch:233 [(0%)]\t Loss: 0.1111  Pearson:0.9695 Spearman:0.9603\n",
      "Train ALL Pearson: 0.9701792814525035\n",
      "Train  ALL Spearman: 0.9601004611833379\n",
      "31.491697072982788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:233 [(0%)]\t Loss: 0.1373  Pearson:0.9567 Spearman:0.9319\n",
      "Test : Loss:0.1506 \n",
      "pearson： 0.9452526957111425 ALL Pearson: 0.94206791177747\n",
      "spearman： 0.9248939789202886 ALL Spearman: 0.9262909913846017\n",
      "time: 36.33014440536499\n",
      "7.289999999999998e-05\n",
      "Train Epoch:234 [(0%)]\t Loss: 0.0968  Pearson:0.9748 Spearman:0.9651\n",
      "Train ALL Pearson: 0.9697272808718106\n",
      "Train  ALL Spearman: 0.9596273406951178\n",
      "31.929489374160767\n",
      "Test Epoch:234 [(0%)]\t Loss: 0.1579  Pearson:0.9167 Spearman:0.8736\n",
      "Test : Loss:0.1516 \n",
      "pearson： 0.936315792274278 ALL Pearson: 0.9420705991691588\n",
      "spearman： 0.9144168765566364 ALL Spearman: 0.9263090353555565\n",
      "time: 36.57802700996399\n",
      "7.289999999999998e-05\n",
      "Train Epoch:235 [(0%)]\t Loss: 0.0996  Pearson:0.9737 Spearman:0.9601\n",
      "Train ALL Pearson: 0.9704205345334821\n",
      "Train  ALL Spearman: 0.9607967666222649\n",
      "32.08279752731323\n",
      "Test Epoch:235 [(0%)]\t Loss: 0.1508  Pearson:0.9382 Spearman:0.9079\n",
      "Test : Loss:0.1510 \n",
      "pearson： 0.940391984855069 ALL Pearson: 0.9420731153121751\n",
      "spearman： 0.9195010996003078 ALL Spearman: 0.9262937070200465\n",
      "time: 36.929624795913696\n",
      "7.289999999999998e-05\n",
      "Train Epoch:236 [(0%)]\t Loss: 0.1082  Pearson:0.9609 Spearman:0.9519\n",
      "Train ALL Pearson: 0.969936547374905\n",
      "Train  ALL Spearman: 0.9597305641348249\n",
      "31.561190843582153\n",
      "Test Epoch:236 [(0%)]\t Loss: 0.1526  Pearson:0.9471 Spearman:0.9300\n",
      "Test : Loss:0.1510 \n",
      "pearson： 0.9403347997877284 ALL Pearson: 0.9420784357837795\n",
      "spearman： 0.9214052372473176 ALL Spearman: 0.9262902074061854\n",
      "time: 36.283217906951904\n",
      "7.289999999999998e-05\n",
      "Train Epoch:237 [(0%)]\t Loss: 0.0960  Pearson:0.9670 Spearman:0.9564\n",
      "Train ALL Pearson: 0.9696028289276644\n",
      "Train  ALL Spearman: 0.9592710436526558\n",
      "33.389296531677246\n",
      "Test Epoch:237 [(0%)]\t Loss: 0.1445  Pearson:0.9400 Spearman:0.9308\n",
      "Test : Loss:0.1508 \n",
      "pearson： 0.9424039809906126 ALL Pearson: 0.9420799272053764\n",
      "spearman： 0.9287517262408097 ALL Spearman: 0.9263055477613138\n",
      "time: 38.257932901382446\n",
      "7.289999999999998e-05\n",
      "Train Epoch:238 [(0%)]\t Loss: 0.1024  Pearson:0.9695 Spearman:0.9542\n",
      "Train ALL Pearson: 0.9701627316164648\n",
      "Train  ALL Spearman: 0.9598743983489617\n",
      "31.760949850082397\n",
      "Test Epoch:238 [(0%)]\t Loss: 0.1460  Pearson:0.9453 Spearman:0.9238\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9449197191682103 ALL Pearson: 0.9420737223135666\n",
      "spearman： 0.9280523124258457 ALL Spearman: 0.9262984461466224\n",
      "time: 36.50342845916748\n",
      "7.289999999999998e-05\n",
      "Train Epoch:239 [(0%)]\t Loss: 0.0993  Pearson:0.9693 Spearman:0.9679\n",
      "Train ALL Pearson: 0.9695872962256284\n",
      "Train  ALL Spearman: 0.9592746822174529\n",
      "31.360501050949097\n",
      "Test Epoch:239 [(0%)]\t Loss: 0.1547  Pearson:0.9271 Spearman:0.9107\n",
      "Test : Loss:0.1516 \n",
      "pearson： 0.9407990943955874 ALL Pearson: 0.9420611368362469\n",
      "spearman： 0.9247069079373156 ALL Spearman: 0.9263033238271142\n",
      "time: 35.94303059577942\n",
      "7.289999999999998e-05\n",
      "Train Epoch:240 [(0%)]\t Loss: 0.0902  Pearson:0.9777 Spearman:0.9661\n",
      "Train ALL Pearson: 0.9703349901539833\n",
      "Train  ALL Spearman: 0.9600434911796164\n",
      "31.671448707580566\n",
      "Test Epoch:240 [(0%)]\t Loss: 0.1602  Pearson:0.9292 Spearman:0.9113\n",
      "Test : Loss:0.1509 \n",
      "pearson： 0.9425896250658365 ALL Pearson: 0.9420555543573532\n",
      "spearman： 0.9252209149422551 ALL Spearman: 0.9262953065660804\n",
      "time: 36.41892600059509\n",
      "7.289999999999998e-05\n",
      "Train Epoch:241 [(0%)]\t Loss: 0.0918  Pearson:0.9768 Spearman:0.9648\n",
      "Train ALL Pearson: 0.969648117998737\n",
      "Train  ALL Spearman: 0.959281404943012\n",
      "32.02356815338135\n",
      "Test Epoch:241 [(0%)]\t Loss: 0.1367  Pearson:0.9516 Spearman:0.9322\n",
      "Test : Loss:0.1508 \n",
      "pearson： 0.9425420764656387 ALL Pearson: 0.9420580496659817\n",
      "spearman： 0.9258043030164078 ALL Spearman: 0.9262886687843734\n",
      "time: 37.170815229415894\n",
      "7.289999999999998e-05\n",
      "Train Epoch:242 [(0%)]\t Loss: 0.1051  Pearson:0.9655 Spearman:0.9515\n",
      "Train ALL Pearson: 0.9694637319768674\n",
      "Train  ALL Spearman: 0.9593450366591197\n",
      "32.36231303215027\n",
      "Test Epoch:242 [(0%)]\t Loss: 0.1378  Pearson:0.9354 Spearman:0.9090\n",
      "Test : Loss:0.1512 \n",
      "pearson： 0.9371659431031568 ALL Pearson: 0.9420445835025528\n",
      "spearman： 0.9202706027250325 ALL Spearman: 0.9262863292894786\n",
      "time: 37.20334315299988\n",
      "Test Epoch:-1 [(0%)]\t Loss: 0.1548  Pearson:0.9506 Spearman:0.9335\n",
      "Test : Loss:0.1507 \n",
      "pearson： 0.9432190787401467 ALL Pearson: 0.942103844755856\n",
      "spearman： 0.9299753669691477 ALL Spearman: 0.9262865713033956\n",
      "PLCC: [0.9390283689521908, 0.9380595102191237, 0.9333994268588421, 0.9374250181967769, 0.942103844755856] SRCC: [0.9231706745873007, 0.9249109784651682, 0.9185469034467663, 0.9250218347437751, 0.9262865713033956]\n",
      "Split: 4 Median PLCC: 0.9380595102191237 SRCC: 0.9249109784651682\n",
      "Test Epoch:-1 [(0%)]\t Loss: 3.0683  Pearson:0.0312 Spearman:0.0122\n",
      "Test : Loss:3.1016 \n",
      "pearson： 0.028624230534143067 ALL Pearson: 0.03790680516829844\n",
      "spearman： 0.020885644993343753 ALL Spearman: 0.030372971384820452\n",
      "0.01\n",
      "Train Epoch:0 [(0%)]\t Loss: 3.1335  Pearson:0.0363 Spearman:0.0090\n",
      "Train ALL Pearson: -0.012468020652039093\n",
      "Train  ALL Spearman: -0.007562554969786425\n",
      "20.451844930648804\n",
      "Test Epoch:0 [(0%)]\t Loss: 1.2603  Pearson:-0.0978 Spearman:-0.0778\n",
      "Test : Loss:1.2044 \n",
      "pearson： -0.08219561225174365 ALL Pearson: -0.09195289142296652\n",
      "spearman： -0.08411093495742807 ALL Spearman: -0.09556079092235469\n",
      "time: 25.125573873519897\n",
      "0.01\n",
      "Train Epoch:1 [(0%)]\t Loss: 1.0176  Pearson:-0.1927 Spearman:-0.1977\n",
      "Train ALL Pearson: 0.12732471042113438\n",
      "Train  ALL Spearman: 0.11370886422312126\n",
      "20.84535241127014\n",
      "Test Epoch:1 [(0%)]\t Loss: 0.6422  Pearson:0.3919 Spearman:0.3580\n",
      "Test : Loss:0.6181 \n",
      "pearson： 0.4568753831400205 ALL Pearson: 0.4717769028927806\n",
      "spearman： 0.42155491129773887 ALL Spearman: 0.4357944948202632\n",
      "time: 25.837381839752197\n",
      "0.01\n",
      "Train Epoch:2 [(0%)]\t Loss: 0.5641  Pearson:0.1301 Spearman:0.1585\n",
      "Train ALL Pearson: 0.3961866503333454\n",
      "Train  ALL Spearman: 0.3655216459176133\n",
      "20.61260986328125\n",
      "Test Epoch:2 [(0%)]\t Loss: 0.5969  Pearson:0.6509 Spearman:0.6168\n",
      "Test : Loss:0.5610 \n",
      "pearson： 0.6267668448041704 ALL Pearson: 0.6235477150800562\n",
      "spearman： 0.590789619086328 ALL Spearman: 0.5839426633295646\n",
      "time: 25.51880192756653\n",
      "0.01\n",
      "Train Epoch:3 [(0%)]\t Loss: 0.4546  Pearson:0.4744 Spearman:0.4578\n",
      "Train ALL Pearson: 0.5018972678211061\n",
      "Train  ALL Spearman: 0.4756083750241672\n",
      "20.4778892993927\n",
      "Test Epoch:3 [(0%)]\t Loss: 0.5189  Pearson:0.6685 Spearman:0.6377\n",
      "Test : Loss:0.5473 \n",
      "pearson： 0.6808030045148924 ALL Pearson: 0.682337019719894\n",
      "spearman： 0.6546500002867904 ALL Spearman: 0.6466625069288467\n",
      "time: 25.46045470237732\n",
      "0.01\n",
      "Train Epoch:4 [(0%)]\t Loss: 0.4018  Pearson:0.4979 Spearman:0.4397\n",
      "Train ALL Pearson: 0.543735220118387\n",
      "Train  ALL Spearman: 0.5107684321824564\n",
      "20.53206205368042\n",
      "Test Epoch:4 [(0%)]\t Loss: 0.5521  Pearson:0.7010 Spearman:0.6822\n",
      "Test : Loss:0.5426 \n",
      "pearson： 0.7046450734068681 ALL Pearson: 0.7082002901586486\n",
      "spearman： 0.6734415070994321 ALL Spearman: 0.6729171411706667\n",
      "time: 25.337641954421997\n",
      "0.01\n",
      "Train Epoch:5 [(0%)]\t Loss: 0.3957  Pearson:0.5372 Spearman:0.4886\n",
      "Train ALL Pearson: 0.5764057554026013\n",
      "Train  ALL Spearman: 0.5402631614725372\n",
      "20.57237720489502\n",
      "Test Epoch:5 [(0%)]\t Loss: 0.5358  Pearson:0.7148 Spearman:0.7078\n",
      "Test : Loss:0.5319 \n",
      "pearson： 0.7169228629033604 ALL Pearson: 0.725675655954709\n",
      "spearman： 0.6929893794208604 ALL Spearman: 0.6924792086812996\n",
      "time: 25.56294274330139\n",
      "0.01\n",
      "Train Epoch:6 [(0%)]\t Loss: 0.3858  Pearson:0.5399 Spearman:0.4864\n",
      "Train ALL Pearson: 0.6076906331301851\n",
      "Train  ALL Spearman: 0.5761169971166942\n",
      "20.00234365463257\n",
      "Test Epoch:6 [(0%)]\t Loss: 0.5142  Pearson:0.7228 Spearman:0.6799\n",
      "Test : Loss:0.5066 \n",
      "pearson： 0.7247881615090367 ALL Pearson: 0.7344582406894692\n",
      "spearman： 0.6981353064857101 ALL Spearman: 0.7025580434041078\n",
      "time: 24.927446365356445\n",
      "0.01\n",
      "Train Epoch:7 [(0%)]\t Loss: 0.3843  Pearson:0.6409 Spearman:0.5929\n",
      "Train ALL Pearson: 0.6101747990926352\n",
      "Train  ALL Spearman: 0.5804536403527567\n",
      "20.84226417541504\n",
      "Test Epoch:7 [(0%)]\t Loss: 0.5232  Pearson:0.7725 Spearman:0.7121\n",
      "Test : Loss:0.5315 \n",
      "pearson： 0.746855010692033 ALL Pearson: 0.7458900959425234\n",
      "spearman： 0.7084369310412953 ALL Spearman: 0.7164023114021121\n",
      "time: 25.786298274993896\n",
      "0.01\n",
      "Train Epoch:8 [(0%)]\t Loss: 0.3726  Pearson:0.6462 Spearman:0.5577\n",
      "Train ALL Pearson: 0.6203511619530658\n",
      "Train  ALL Spearman: 0.582152169304349\n",
      "20.004497051239014\n",
      "Test Epoch:8 [(0%)]\t Loss: 0.5334  Pearson:0.7778 Spearman:0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : Loss:0.5403 \n",
      "pearson： 0.7606685495236918 ALL Pearson: 0.7509816016027069\n",
      "spearman： 0.7331941531142707 ALL Spearman: 0.7226778099249528\n",
      "time: 24.68512988090515\n",
      "0.01\n",
      "Train Epoch:9 [(0%)]\t Loss: 0.3609  Pearson:0.6025 Spearman:0.5573\n",
      "Train ALL Pearson: 0.6328002032249636\n",
      "Train  ALL Spearman: 0.5997582916627179\n",
      "20.22194528579712\n",
      "Test Epoch:9 [(0%)]\t Loss: 0.5120  Pearson:0.7454 Spearman:0.7232\n",
      "Test : Loss:0.5106 \n",
      "pearson： 0.7539188645756247 ALL Pearson: 0.752756317165682\n",
      "spearman： 0.736523200518283 ALL Spearman: 0.7243499649532499\n",
      "time: 24.981443881988525\n",
      "0.01\n",
      "Train Epoch:10 [(0%)]\t Loss: 0.3626  Pearson:0.5328 Spearman:0.4715\n",
      "Train ALL Pearson: 0.6450086234639993\n",
      "Train  ALL Spearman: 0.6157717980930024\n",
      "20.33559012413025\n",
      "Test Epoch:10 [(0%)]\t Loss: 0.5375  Pearson:0.7326 Spearman:0.6340\n",
      "Test : Loss:0.5265 \n",
      "pearson： 0.7513807954404128 ALL Pearson: 0.7562362202556399\n",
      "spearman： 0.7190997414292517 ALL Spearman: 0.7288007615023582\n",
      "time: 25.250014305114746\n",
      "0.001\n",
      "Train Epoch:11 [(0%)]\t Loss: 0.3406  Pearson:0.6368 Spearman:0.6355\n",
      "Train ALL Pearson: 0.6544217995525383\n",
      "Train  ALL Spearman: 0.6227718642935479\n",
      "31.310828685760498\n",
      "Test Epoch:11 [(0%)]\t Loss: 0.4698  Pearson:0.7782 Spearman:0.7504\n",
      "Test : Loss:0.5101 \n",
      "pearson： 0.7622200809767784 ALL Pearson: 0.7654949051282613\n",
      "spearman： 0.7317827909846593 ALL Spearman: 0.7406154496937606\n",
      "time: 36.086297035217285\n",
      "0.001\n",
      "Train Epoch:12 [(0%)]\t Loss: 0.3568  Pearson:0.6198 Spearman:0.6099\n",
      "Train ALL Pearson: 0.6702517952968533\n",
      "Train  ALL Spearman: 0.6439144518544253\n",
      "31.46231460571289\n",
      "Test Epoch:12 [(0%)]\t Loss: 0.4768  Pearson:0.8240 Spearman:0.8152\n",
      "Test : Loss:0.5165 \n",
      "pearson： 0.7811297333226782 ALL Pearson: 0.7717309811172277\n",
      "spearman： 0.7645025124922317 ALL Spearman: 0.748854979010781\n",
      "time: 36.20379447937012\n",
      "0.001\n",
      "Train Epoch:13 [(0%)]\t Loss: 0.3275  Pearson:0.6756 Spearman:0.6028\n",
      "Train ALL Pearson: 0.672938078872716\n",
      "Train  ALL Spearman: 0.649177603032001\n",
      "31.640530586242676\n",
      "Test Epoch:13 [(0%)]\t Loss: 0.5118  Pearson:0.7660 Spearman:0.7665\n",
      "Test : Loss:0.5055 \n",
      "pearson： 0.7789385506477653 ALL Pearson: 0.7782891579157131\n",
      "spearman： 0.7625278130127715 ALL Spearman: 0.7566641987565862\n",
      "time: 36.38361072540283\n",
      "0.001\n",
      "Train Epoch:14 [(0%)]\t Loss: 0.3235  Pearson:0.6312 Spearman:0.5961\n",
      "Train ALL Pearson: 0.6888729033940859\n",
      "Train  ALL Spearman: 0.6618475230474471\n",
      "32.10499405860901\n",
      "Test Epoch:14 [(0%)]\t Loss: 0.4960  Pearson:0.7896 Spearman:0.7842\n",
      "Test : Loss:0.4918 \n",
      "pearson： 0.782014871562572 ALL Pearson: 0.7841123550959342\n",
      "spearman： 0.7633002473804686 ALL Spearman: 0.7631364433380343\n",
      "time: 37.06301307678223\n",
      "0.001\n",
      "Train Epoch:15 [(0%)]\t Loss: 0.3352  Pearson:0.6413 Spearman:0.6074\n",
      "Train ALL Pearson: 0.6926452384538272\n",
      "Train  ALL Spearman: 0.6698571535641952\n",
      "32.45735216140747\n",
      "Test Epoch:15 [(0%)]\t Loss: 0.4788  Pearson:0.8300 Spearman:0.8077\n",
      "Test : Loss:0.5083 \n",
      "pearson： 0.7975608974523087 ALL Pearson: 0.788096395612165\n",
      "spearman： 0.7761119406033526 ALL Spearman: 0.7687298341273556\n",
      "time: 37.04788064956665\n",
      "0.001\n",
      "Train Epoch:16 [(0%)]\t Loss: 0.3238  Pearson:0.7403 Spearman:0.6872\n",
      "Train ALL Pearson: 0.7027136314801304\n",
      "Train  ALL Spearman: 0.6734656962378105\n",
      "32.80064558982849\n",
      "Test Epoch:16 [(0%)]\t Loss: 0.5577  Pearson:0.7497 Spearman:0.7298\n",
      "Test : Loss:0.4977 \n",
      "pearson： 0.7767315845985283 ALL Pearson: 0.7924049666681448\n",
      "spearman： 0.7630930445638654 ALL Spearman: 0.7738875803221044\n",
      "time: 37.618152379989624\n",
      "0.001\n",
      "Train Epoch:17 [(0%)]\t Loss: 0.3382  Pearson:0.6433 Spearman:0.6403\n",
      "Train ALL Pearson: 0.708830322165837\n",
      "Train  ALL Spearman: 0.6834886262166479\n",
      "31.79246973991394\n",
      "Test Epoch:17 [(0%)]\t Loss: 0.4837  Pearson:0.8334 Spearman:0.8214\n",
      "Test : Loss:0.4983 \n",
      "pearson： 0.8027442639569412 ALL Pearson: 0.7967108662922664\n",
      "spearman： 0.7820704845989828 ALL Spearman: 0.7786689048135648\n",
      "time: 36.62392044067383\n",
      "0.001\n",
      "Train Epoch:18 [(0%)]\t Loss: 0.2857  Pearson:0.7559 Spearman:0.7298\n",
      "Train ALL Pearson: 0.7191627978800215\n",
      "Train  ALL Spearman: 0.6923713426038529\n",
      "32.16995620727539\n",
      "Test Epoch:18 [(0%)]\t Loss: 0.4896  Pearson:0.7596 Spearman:0.7362\n",
      "Test : Loss:0.4880 \n",
      "pearson： 0.8010529768014218 ALL Pearson: 0.8002010879898742\n",
      "spearman： 0.776999158402695 ALL Spearman: 0.7818987787445956\n",
      "time: 37.01940059661865\n",
      "0.001\n",
      "Train Epoch:19 [(0%)]\t Loss: 0.2837  Pearson:0.7365 Spearman:0.6657\n",
      "Train ALL Pearson: 0.7154357145464103\n",
      "Train  ALL Spearman: 0.6886363861001065\n",
      "32.84058094024658\n",
      "Test Epoch:19 [(0%)]\t Loss: 0.4697  Pearson:0.8178 Spearman:0.7978\n",
      "Test : Loss:0.4799 \n",
      "pearson： 0.8090717261714445 ALL Pearson: 0.8044157211342552\n",
      "spearman： 0.7914690090815487 ALL Spearman: 0.7855469356211829\n",
      "time: 37.61885452270508\n",
      "0.001\n",
      "Train Epoch:20 [(0%)]\t Loss: 0.3259  Pearson:0.6709 Spearman:0.6468\n"
     ]
    }
   ],
   "source": [
    "run koniq244_deit3fc_with_768patch2_10times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
